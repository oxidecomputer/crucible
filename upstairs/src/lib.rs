#![feature(asm)]
use bytes::Bytes;
use std::clone::Clone;
use std::collections::{HashMap, VecDeque};
use std::fmt;
use std::fmt::{Debug, Formatter};
use std::net::SocketAddrV4;
use std::sync::mpsc as std_mpsc;
use std::sync::{Arc, Mutex, MutexGuard};
use std::time::Duration;

use crucible_common::*;
use crucible_protocol::*;

use anyhow::{anyhow, bail, Result};
use futures::{SinkExt, StreamExt};
use ringbuffer::{AllocRingBuffer, RingBufferExt, RingBufferWrite};
use structopt::StructOpt;
use usdt::register_probes;

use tokio::net::tcp::WriteHalf;
use tokio::net::{TcpSocket, TcpStream};
use tokio::sync::{mpsc, watch, Notify};
use tokio::time::{sleep_until, Instant};
use tokio_util::codec::{FramedRead, FramedWrite};

use aes::cipher::generic_array::GenericArray;
use aes::{Aes128, NewBlockCipher};
use base64;
use xts_mode::{get_tweak_default, Xts128};

// Include the Rust implementation generated by the build script.
include!(concat!(env!("OUT_DIR"), "/crutrace.rs"));

#[derive(Debug, StructOpt)]
#[structopt(about = "volume-side storage component")]
pub struct Opt {
    #[structopt(short, long, default_value = "127.0.0.1:9000")]
    target: Vec<SocketAddrV4>,

    #[structopt(short, long)]
    key: Option<String>,
}

impl Opt {
    fn key_bytes(&self) -> Option<Vec<u8>> {
        match &self.key {
            Some(key) => match base64::decode(key) {
                Ok(bytes) => Some(bytes),
                Err(_) => None,
            },
            None => None,
        }
    }
}

pub fn opts() -> Result<Opt> {
    let opt: Opt = Opt::from_args();
    println!("raw options: {:?}", opt);

    if opt.target.is_empty() {
        bail!("must specify at least one --target");
    }

    if let Some(key) = &opt.key {
        // For xts, key size must be 32 bytes
        let key_bytes = base64::decode(key)?;

        if key_bytes.len() != 32 {
            bail!("Key must be 32 bytes!");
        }
    }

    Ok(opt)
}

impl Opt {
    /*
     * Use:
     *
     *     let opt = Opt::from_string("-- -t 192.168.1.1:3801 -t 192.168.1.2:3801".to_string()).unwrap();
     *
     */
    pub fn from_string(args: String) -> Result<Opt> {
        let opt: Opt = Opt::from_iter(args.split(' '));

        if opt.target.is_empty() {
            bail!("must specify at least one --target");
        }

        Ok(opt)
    }
}

#[cfg(test)]
mod tests {
    use crate::Opt;
    use std::net::{Ipv4Addr, SocketAddrV4};

    #[test]
    fn test_opt_from_string() {
        let opt = Opt::from_string(
            "-- -t 192.168.1.1:3801 -t 192.168.1.2:3801".to_string(),
        )
        .unwrap();
        assert_eq!(opt.target.is_empty(), false);
        assert_eq!(opt.target.len(), 2);

        assert_eq!(
            opt.target[0],
            SocketAddrV4::new(Ipv4Addr::new(192, 168, 1, 1), 3801)
        );
        assert_eq!(
            opt.target[1],
            SocketAddrV4::new(Ipv4Addr::new(192, 168, 1, 2), 3801)
        );
    }
}

pub fn deadline_secs(secs: u64) -> Instant {
    Instant::now()
        .checked_add(Duration::from_secs(secs))
        .unwrap()
}

async fn proc_frame(
    u: &Arc<Upstairs>,
    m: &Message,
    client_id: u8,
    ds_done_tx: mpsc::Sender<u64>,
) -> Result<()> {
    match m {
        Message::Imok => Ok(()),
        Message::WriteAck(ds_id) => {
            Ok(io_completed(u, *ds_id, client_id, None, ds_done_tx).await?)
        }
        Message::FlushAck(ds_id) => {
            Ok(io_completed(u, *ds_id, client_id, None, ds_done_tx).await?)
        }
        Message::ReadResponse(ds_id, data) => Ok(io_completed(
            u,
            *ds_id,
            client_id,
            Some(data.clone()),
            ds_done_tx,
        )
        .await?),
        x => bail!("unexpected frame {:?}", x),
    }
}

/*
 * Convert a virtual block offset and length into a Vec of:
 *     Extent number (EID), Block offset, Length in bytes
 *
 * If the offset + length would fit into a single extent, then we only have
 * one tuple in the Vec.  If the offset + length does not fit into the
 * extent, then add a second tuple with EID + 1, 0, and whatever length
 * is remaining.
 *
 * We don't support a length greater than a single extent,
 * which means we only have to support spanning two extents at most.
 *
 */
fn extent_from_offset(
    ddef: MutexGuard<RegionDefinition>,
    offset: u64,
    len: usize,
) -> Result<Vec<(u64, u64, usize)>> {
    // TODO Make asserts return error
    assert!(len as u64 >= ddef.block_size());
    assert!(len as u64 % ddef.block_size() == 0);
    assert!(offset % ddef.block_size() == 0);

    let space_per_extent = ddef.block_size() * ddef.extent_size();
    /*
     * XXX We only support a single region (downstairs).  When we grow to
     * support a LBA size that is larger than a single region, then we will
     * need to write more code.
     */
    let eid: u64 = offset / space_per_extent;
    assert!((len as u64) <= space_per_extent);
    assert!((eid as u32) < ddef.extent_count());

    let block_in_extent: u64 =
        (offset - (eid * space_per_extent)) / ddef.block_size();

    let mut res = Vec::new();

    /*
     * Check to see if our length extends past the end of this region.
     * If it fits, then we can just add the tuple to our Vec.  If it
     * does not fit, then we need to push two things into our Vec and
     * determine the length that each extent needs.
     */
    let data_blocks = (len as u64) / ddef.block_size();
    if data_blocks + block_in_extent <= ddef.extent_size() {
        res.push((eid, block_in_extent, len));
    } else {
        assert!((eid as u32) + 1 < ddef.extent_count());
        let new_len =
            (ddef.extent_size() - block_in_extent) * ddef.block_size();
        res.push((eid, block_in_extent, new_len as usize));
        res.push((eid + 1, 0, len - (new_len as usize)));
    }

    Ok(res)
}

/*
 * Decide what to do with a downstairs that has just connected and has
 * sent us information about its extents.
 *
 * This will eventually need to message the main thread so it knows
 * when it can starting doing I/O.
 */
fn process_downstairs(
    target: &SocketAddrV4,
    u: &Arc<Upstairs>,
    bs: u64,
    es: u64,
    ec: u32,
    versions: Vec<u64>,
) -> Result<()> {
    println!(
        "{} Evaluate new downstairs : bs:{} es:{} ec:{} versions: {:?}",
        target, bs, es, ec, versions
    );

    let mut fi = u.flush_info.lock().unwrap();
    if fi.flush_numbers.is_empty() {
        /*
         * This is the first version list we have, so
         * we will make it the original and compare
         * whatever comes next.
         */
        fi.flush_numbers = versions;
        fi.next_flush = *fi.flush_numbers.iter().max().unwrap() + 1;
        println!(
            "Set inital Extent versions to {:?}\nNext flush: {}",
            fi.flush_numbers, fi.next_flush
        );
    } else if fi.flush_numbers.len() != versions.len() {
        /*
         * I don't think there is much we can do here, the expected number
         * of flush numbers does not match.  Possibly we have grown one but
         * not the rest of the downstairs?
         */
        panic!(
            "Expected downstairs version \
              len:{:?} does not match new \
              downstairs:{:?}",
            fi.flush_numbers.len(),
            versions.len()
        );
    } else {
        /*
         * We already have a list of versions to compare with.  Make that
         * comparision now against this new list
         */
        let ver_cmp = fi.flush_numbers.iter().eq(versions.iter());
        if !ver_cmp {
            println!(
                "{} MISMATCH expected: {:?} != new: {:?}",
                target, fi.flush_numbers, versions
            );
            // XXX Recovery process should start here
            println!("{} Ignoring this downstairs version info", target);
        }
    }

    /*
     * XXX Here we have another workaround.  We don't know
     * the region info until after we connect to each
     * downstairs, but we share the ARC Upstairs before we
     * know what to expect.  For now I'm using zero as an
     * indication that we don't yet know the valid values
     * and non-zero meaning we have at least one downstairs
     * to compare with.  We might want to consider breaking
     * out the static config info into something different
     * that is updated on initial downstairs setup from the
     * structures we use for work submission.
     *
     * 0 should never be a valid block size
     */
    let mut ddef = u.ddef.lock().unwrap();
    if ddef.block_size() == 0 {
        ddef.set_block_size(bs);
        ddef.set_extent_size(es);
        ddef.set_extent_count(ec);
        println!("Global using: bs:{} es:{} ec:{}", bs, es, ec);
    }

    if ddef.block_size() != bs
        || ddef.extent_size() != es
        || ddef.extent_count() != ec
    {
        // XXX Figure out if we can hande this error.  Possibly not.
        panic!("New downstaris region info mismatch");
    }
    Ok(())
}

/*
 * This function is called when the upstairs task is notified that
 * a downstairs operation has completed.  We add the read buffer to the
 * IOop struct for later processing if required.
 *
 */
async fn io_completed(
    up: &Arc<Upstairs>,
    ds_id: u64,
    client_id: u8,
    data: Option<bytes::Bytes>,
    ds_done_tx: mpsc::Sender<u64>,
) -> Result<()> {
    let mut gw_work_done = false;

    /*
     * We can't call .send with the lock held, so we check to see
     * if we do need to notify the up_ds_listen task that all work
     * is finished for a ds_id.
     */
    {
        let mut work = up.ds_work.lock().unwrap();
        let counts = work.complete(ds_id, client_id, data)?;
        if counts.active == 0 {
            gw_work_done = true;
        }
    }
    if gw_work_done {
        ds_done_tx.send(ds_id).await?
    }

    Ok(())
}

/*
 * This function is called by a worker task after the main task has added
 * work to the hashmap and notified the worker tasks that new work is ready
 * to be serviced.  The worker task will walk the hashmap and build a list
 * of new work that it needs to do.  It will then iterate through those
 * work items and send them over the wire to this tasks waiting downstaris.
 */
async fn io_send(
    u: &Arc<Upstairs>,
    fw: &mut FramedWrite<WriteHalf<'_>, CrucibleEncoder>,
    client_id: u8,
) -> Result<()> {
    /*
     * Build ourselves a list of all the jobs on the work hashmap that
     * have the job state for our client id in the IOState::New
     */
    let mut new_work = u.ds_work.lock().unwrap().new_work(client_id);

    /*
     * Now we have a list of all the job IDs that are new for our client id.
     * Walk this list and process each job, marking it InProgress as we
     * do the work.  We do this in two loops because we can't hold the
     * lock for the hashmap while we do work, and if we release the lock
     * to do work, we would have to start over and look at all jobs in the
     * map to see if they are new.
     *
     * This also allows us to sort the job ids and do them in order they
     * were put into the hashmap, though I don't think that is required.
     */
    new_work.sort_unstable();

    for new_id in new_work.iter() {
        /*
         * We can't hold the hashmap mutex into the await send
         * below, so make a scope to get our next piece of work
         * from the hashmap and release the lock when we leave
         * this scope.
         */
        let job = u.ds_work.lock().unwrap().in_progress(*new_id, client_id);

        match job {
            IOop::Write {
                dependencies,
                eid,
                block_offset,
                data,
            } => {
                println!(
                    "[{}] Write ds_id:{} eid:{:?} bo:{:?}",
                    client_id, *new_id, eid, block_offset
                );
                fw.send(Message::Write(
                    *new_id,
                    eid,
                    dependencies.clone(),
                    block_offset,
                    data.clone(),
                ))
                .await?
            }
            IOop::Flush {
                dependencies,
                flush_number,
            } => {
                println!(
                    "Flush ds_id:{} dep:{:?} flush_number:{:?}",
                    *new_id,
                    dependencies.len(),
                    flush_number
                );
                fw.send(Message::Flush(
                    *new_id,
                    dependencies.clone(),
                    flush_number,
                ))
                .await?
            }
            IOop::Read {
                eid,
                block_offset,
                blocks,
            } => {
                println!(
                    "Read  ds_id:{} eid:{:?} bo:{:?} blocks:{}",
                    *new_id, eid, block_offset, blocks,
                );
                fw.send(Message::ReadRequest(
                    *new_id,
                    eid,
                    block_offset,
                    blocks,
                ))
                .await?
            }
        }
    }
    Ok(())
}

/*
 * Once we have a connection to a downstairs, this task takes over and
 * handles both the initial negotiation and then watches the input for
 * changes, indicating that new work in on the work hashmap.  We will
 * walk the hashmap on the input signal and get any new work for this
 * specific downstairs and mark that job as in progress.
 */
async fn proc(
    target: &SocketAddrV4,
    input: &mut watch::Receiver<u64>,
    output: &mpsc::Sender<Condition>,
    up: &Arc<Upstairs>,
    mut sock: TcpStream,
    connected: &mut bool,
    client_id: u8,
    ds_done_tx: mpsc::Sender<u64>,
) -> Result<()> {
    let (r, w) = sock.split();
    let mut fr = FramedRead::new(r, CrucibleDecoder::new());
    let mut fw = FramedWrite::new(w, CrucibleEncoder::new());

    /*
     * As the "client", we must begin the negotiation.
     */
    fw.send(Message::HereIAm(1)).await?;

    /*
     * Don't wait more than 5 seconds to hear from the other side.
     * XXX Timeouts, timeouts: always wrong!  Some too short and some too long.
     */
    let mut deadline = deadline_secs(50);
    let mut negotiated = false;

    /*
     * To keep things alive, initiate a ping any time we have been idle for a
     * second.
     */
    let mut pingat = deadline_secs(10);
    let mut needping = false;

    loop {
        /*
         * XXX Just a thought here, could we send so much input that the
         * select would always have input.changed() and starve out the
         * fr.next() select?  Does this select ever work that way?
         */
        // println!("[{}] at the top of the loop", client_id);
        tokio::select! {
            _ = sleep_until(deadline) => {
                if !negotiated {
                    bail!("did not negotiate a protocol");
                } else {
                    bail!("inactivity timeout");
                }
            }
            _ = sleep_until(pingat), if needping => {
                fw.send(Message::Ruok).await?;
                needping = false;
            }
            _ = input.changed() => {
                /*
                let iv = *input.borrow();
                println!("[{}] Input changed with {}", client_id, iv);
                 */
                io_send(up, &mut fw, client_id).await?;
            }
            f = fr.next() => {
                /*
                 * Negotiate protocol before we get into specifics.
                 */
                match f.transpose()? {
                    None => {
                        return Ok(())
                    }
                    Some(Message::YesItsMe(version)) => {
                        if negotiated {
                            bail!("negotiated already!");
                        }
                        /*
                         * XXX Valid version to compare with should come
                         * from main task
                         */
                        if version != 1 {
                            bail!("expected version 1, got {}", version);
                        }
                        negotiated = true;
                        needping = true;
                        deadline = deadline_secs(50);

                        /*
                         * Ask for the current version of all extents.
                         */
                        fw.send(Message::ExtentVersionsPlease).await?;
                    }
                    Some(Message::ExtentVersions(bs, es, ec, versions)) => {
                        if !negotiated {
                            bail!("expected YesItsMe first");
                        }
                        process_downstairs(target, up, bs, es, ec, versions)?;

                        /*
                         *  XXX I moved this here for now so we can use this
                         * signal to move forward with I/O.  Eventually this
                         * connected being true state should be sent from the
                         * initial YesItsMe case, and we send something else
                         * through a different watcher that tells the main
                         * task the list of versions, or something like that.
                         */

                        /*
                         * If we get here, we are ready to receive IO
                         */
                        *connected = true;
                        output.send(Condition {
                            target: *target,
                            connected: true,
                        }).await
                        .unwrap();
                    }
                    Some(m) => {
                        if !negotiated {
                            bail!("expected YesItsMe first");
                        }
                        let ds_done_tx = ds_done_tx.clone();
                        proc_frame(up, &m, client_id, ds_done_tx).await?;
                        deadline = deadline_secs(50);
                        pingat = deadline_secs(10);
                        needping = true;
                    }
                }
            }
        }
    }
}

/*
 * This task is responsible for the connection to and traffic between
 * a specific downstairs instance.  In here we handle taking work off of
 * the global work list and performing that work for a specific downstairs.
 */
async fn looper(
    target: SocketAddrV4,
    mut input: watch::Receiver<u64>,
    output: mpsc::Sender<Condition>,
    up: &Arc<Upstairs>,
    client_id: u8,
    ds_done_tx: mpsc::Sender<u64>,
) {
    let mut firstgo = true;
    let mut connected = false;

    'outer: loop {
        if firstgo {
            firstgo = false;
        } else {
            tokio::time::sleep(Duration::from_secs(1)).await;
        }

        /*
         * Make connection to this downstairs.
         */
        let sock = TcpSocket::new_v4().expect("v4 socket");

        /*
         * Set a connect timeout, and connect to the target:
         */
        println!("{0}[{1}] connecting to {0}", target, client_id);
        let deadline = tokio::time::sleep_until(deadline_secs(10));
        tokio::pin!(deadline);
        let tcp = sock.connect(target.into());
        tokio::pin!(tcp);

        let tcp: TcpStream = loop {
            tokio::select! {
                _ = &mut deadline => {
                    println!("connect timeout");
                    continue 'outer;
                }
                tcp = &mut tcp => {
                    match tcp {
                        Ok(tcp) => {
                            println!("{0}[{1}] ok, connected to {0}",
                                target,
                                client_id);
                            break tcp;
                        }
                        Err(e) => {
                            println!("{0} connect to {0} failure: {1:?}",
                                target, e);
                            continue 'outer;
                        }
                    }
                }
            }
        };

        /*
         * Once we have a connected downstairs, the proc task takes over and
         * handles negiotation and work processing.
         */
        if let Err(e) = proc(
            &target,
            &mut input,
            &output,
            up,
            tcp,
            &mut connected,
            client_id,
            ds_done_tx.clone(),
        )
        .await
        {
            eprintln!("ERROR: {}: proc: {:?}", target, e);
        }

        if connected {
            output
                .send(Condition {
                    target,
                    connected: false,
                })
                .await
                .unwrap();
            connected = false;
        }
    }
}

/*
 * The structure that tracks downstairs work in progress
 */
#[derive(Debug)]
pub struct Work {
    active: HashMap<u64, DownstairsIO>,
    next_id: u64,
    completed: AllocRingBuffer<u64>,
}

#[derive(Debug, Default)]
pub struct WorkCounts {
    active: u64,
    done: u64,
}

impl Work {
    /**
     * Assign a new downstairs ID.
     */
    fn next_id(&mut self) -> u64 {
        let id = self.next_id;
        self.next_id += 1;
        id
    }

    /**
     * Mark this request as in progress for this client, and return a copy
     * of the details of the request.
     */
    fn in_progress(&mut self, ds_id: u64, client_id: u8) -> IOop {
        let job = self.active.get_mut(&ds_id).unwrap();
        let oldstate = job.state.insert(client_id, IOState::InProgress);
        assert_eq!(oldstate, Some(IOState::New));
        job.work.clone()
    }

    /**
     * Return a list of downstairs request IDs that represent unissued
     * requests for this client.
     */
    fn new_work(&self, client_id: u8) -> Vec<u64> {
        self.active
            .values()
            .filter_map(|job| {
                if let Some(IOState::New) = job.state.get(&client_id) {
                    Some(job.ds_id)
                } else {
                    None
                }
            })
            .collect()
    }

    /**
     * Walk the active hashmap and Return a Vec of downstairs request IDs
     * where all requests have been completed.
     */
    fn completed_work(&mut self) -> Vec<u64> {
        let mut completed = Vec::new();
        let mut kvec = self.active.keys().cloned().collect::<Vec<u64>>();
        kvec.sort_unstable();
        for k in kvec.iter() {
            if self.state_count(*k).unwrap().active == 0 {
                completed.push(*k)
            }
        }
        completed
    }

    /**
     * Enqueue a new downstairs request.
     */
    fn enqueue(&mut self, io: DownstairsIO) {
        self.active.insert(io.ds_id, io);
    }

    /**
     * Collect the state of the jobs from each client.
     */
    fn state_count(&mut self, ds_id: u64) -> Result<WorkCounts> {
        /* XXX Should this support invalid ds_ids? */
        let job = self
            .active
            .get_mut(&ds_id)
            .ok_or_else(|| anyhow!("reqid {} is not active", ds_id))?;

        let mut wc: WorkCounts = Default::default();
        for state in job.state.values() {
            match state {
                IOState::New | IOState::InProgress => wc.active += 1,
                IOState::Done | IOState::Skipped | IOState::Error => {
                    wc.done += 1;
                }
            }
        }
        Ok(wc)
    }

    /**
     * Mark this downstairs request as complete for this client.  Returns
     * counts clients for which this request is still active or has been
     * completed already.
     */
    fn complete(
        &mut self,
        ds_id: u64,
        client_id: u8,
        data: Option<bytes::Bytes>,
    ) -> Result<WorkCounts> {
        let job = self
            .active
            .get_mut(&ds_id)
            .ok_or_else(|| anyhow!("reqid {} is not active", ds_id))?;
        let oldstate = job.state.insert(client_id, IOState::Done);
        assert_ne!(oldstate, Some(IOState::Done));

        /*
         * If the data field has a buffer in it, then we attach that buffer
         * (clone really) to the job.data field.  When a read completes it
         * will have a buffer and we keep that buffer around so it can be
         * transferred back to the guest when all IOs that made up that read
         * have returned data.
         */
        if let Some(data) = data {
            if job.data.is_none() {
                println!("Save data for ds_id:{}", ds_id);
                job.data = Some(data);
            }
        } // XXX else assert this is not a read

        /*
         * Return the state count for the I/O on this ds_id
         */
        self.state_count(ds_id)
    }

    /**
     * This request is now complete on all peers.  Remove it from the active set
     * and mark it in the completed ring buffer.
     *
     * If there is data in job.data, then we need to transfer that data to the
     * upstairs guest job that started this.
     */
    fn retire(&mut self, ds_id: u64) -> DownstairsIO {
        assert!(!self.completed.contains(&ds_id));
        let old = self.active.remove(&ds_id).unwrap();
        self.completed.push(ds_id);
        old
    }
}

/// Implement XTS encryption
/// See: https://en.wikipedia.org/wiki/Disk_encryption_theory#XEX-based_tweaked-codebook_mode_with_ciphertext_stealing_(XTS)
pub struct UpstairsEncryptionContext {
    xts: Xts128<Aes128>,
    key: Vec<u8>,
    block_size: usize,
    offsets: HashMap<u64, u128>,
}

impl Debug for UpstairsEncryptionContext {
    fn fmt(&self, f: &mut Formatter<'_>) -> Result<(), fmt::Error> {
        f.debug_struct("UpstairsEncryptionContext")
            .field("block_size", &self.block_size)
            .finish()
    }
}

impl Clone for UpstairsEncryptionContext {
    fn clone(&self) -> Self {
        let mut context =
            UpstairsEncryptionContext::new(self.key.clone(), self.block_size);
        for (key, val) in &self.offsets {
            context.set_offset_for_ds_id(*key, *val);
        }
        context
    }

    fn clone_from(&mut self, source: &Self) {
        *self = UpstairsEncryptionContext::new(
            source.key.clone(),
            source.block_size,
        );
        for (key, val) in &source.offsets {
            self.set_offset_for_ds_id(*key, *val);
        }
    }
}

impl UpstairsEncryptionContext {
    pub fn new(key: Vec<u8>, block_size: usize) -> UpstairsEncryptionContext {
        assert!(key.len() == 32);

        let cipher_1 = Aes128::new(GenericArray::from_slice(&key[..16]));
        let cipher_2 = Aes128::new(GenericArray::from_slice(&key[16..]));

        let xts = Xts128::<Aes128>::new(cipher_1, cipher_2);

        UpstairsEncryptionContext {
            xts,
            key,
            block_size,
            offsets: HashMap::default(),
        }
    }

    pub fn key(&self) -> &Vec<u8> {
        &self.key
    }

    pub fn block_size(&self) -> usize {
        self.block_size
    }

    // TODO: checksums?

    fn set_offset_for_ds_id(&mut self, ds_id: u64, block_offset: u128) {
        if self.offsets.contains_key(&ds_id) {
            // TODO: if exists already, bad news!
            panic!("ds_id reused?!");
        }
        self.offsets.insert(ds_id, block_offset);
    }

    pub fn encrypt_in_place(
        &mut self,
        data: &mut [u8],
        ds_id: u64,
        sector_index: u128,
    ) {
        self.set_offset_for_ds_id(ds_id, sector_index);
        self.xts.encrypt_area(
            data,
            self.block_size,
            sector_index,
            get_tweak_default,
        );
    }

    pub fn decrypt_in_place(&self, data: &mut [u8], ds_id: u64) {
        let sector_index: u128 = *self.offsets.get(&ds_id).unwrap();
        self.xts.decrypt_area(
            data,
            self.block_size,
            sector_index,
            get_tweak_default,
        );
    }
}

// key material made with `openssl rand -base64 32`

#[test]
pub fn test_upstairs_encryption_clone_offsets_not_shared() {
    let key_bytes =
        base64::decode("qOUIYJ3pElgcFx1VOiPiATOtyZO0kRVBdK/GANmn13A=").unwrap();
    let mut context =
        UpstairsEncryptionContext::new(Vec::<u8>::from(key_bytes), 512);

    let clone = context.clone();

    assert_eq!(context.key(), clone.key());
    assert_eq!(context.block_size(), clone.block_size());

    assert_eq!(context.offsets, clone.offsets);

    let mut block = [0u8; 512];
    context.encrypt_in_place(&mut block[..], 0, 0);

    assert_ne!(context.offsets, clone.offsets);
}

#[test]
pub fn test_upstairs_encryption_context_ok() {
    use rand::{thread_rng, Rng};

    let key_bytes =
        base64::decode("ClENKTXD2bCyXSHnKXY7GGnk+NvQKbwpatjWP2fJzk0=").unwrap();
    let mut context =
        UpstairsEncryptionContext::new(Vec::<u8>::from(key_bytes), 512);

    let mut block = [0u8; 512];
    thread_rng().fill(&mut block[..]);

    let orig_block = block.clone();

    context.encrypt_in_place(&mut block[..], 0, 0);
    assert_ne!(block, orig_block);

    context.decrypt_in_place(&mut block[..], 0);
    assert_eq!(block, orig_block);
}

#[test]
#[should_panic(expected = "called `Option::unwrap()` on a `None` value")]
pub fn test_upstairs_encryption_context_bad_index() {
    use rand::{thread_rng, Rng};

    let key_bytes =
        base64::decode("EVrH+ABhMP0MLfxynCalDq1vWCCWCWFfsSsJoJeDCx8=").unwrap();
    let mut context =
        UpstairsEncryptionContext::new(Vec::<u8>::from(key_bytes), 512);

    let mut block = [0u8; 512];
    thread_rng().fill(&mut block[..]);

    let orig_block = block.clone();

    // The wrong block index shouldn't work.
    context.encrypt_in_place(&mut block[..], 0, 0);
    assert_ne!(block, orig_block);

    context.decrypt_in_place(&mut block[..], 1);
    assert_ne!(block, orig_block);
}

/*
 * XXX Track scheduled storage work in the central structure.  Have the
 * target management task check for work to do here by changing the value in
 * its watch::channel.  Have the main thread determine that an overflow of
 * work to do backing up in here means we need to do something like mark the
 * target as behind or institute some kind of back pressure, etc.
 */
#[derive(Debug)]
pub struct Upstairs {
    /*
     * The guest struct keeps track of jobs accepted from the Guest as they
     * progress through crucible.  A single job submitted can produce
     * multiple downstairs requests.
     */
    guest: Arc<Guest>,
    /*
     * This Work struct keeps track of IO operations going between upstairs
     * and downstairs.  New work for downstairs is generated inside the
     * upstairs on behalf of IO requests coming from the guest.
     */
    ds_work: Mutex<Work>,
    /*
     * The flush info Vec is only used when first connecting or re-connecting
     * to a downstairs.  It is populated with the versions the upstairs
     * considers the "correct".  If a downstairs disconnects and then
     * comes back, it has to match or be made to match what was decided
     * as the correct list.  This may involve having to refresh the versions
     * vec.
     *
     * The versions vec is not enough to solve a mismatch.  We really need
     * Generation number, flush number, and dirty bit for every extent
     * when resolving conflicts.
     *
     * On Startup we determine the highest flush number from all three
     * downstairs.  We add one to that and it becomes the next flush
     * number.  Flush numbers increment by one each time.
     */
    flush_info: Mutex<FlushInfo>,
    /*
     * The global description of the downstairs region we are using.
     * This allows us to verify each downstairs is the same, as well as
     * enables us to tranlate an LBA to an extent and block offset.
     */
    ddef: Mutex<RegionDefinition>,
    /*
     * The state of a downstairs connection, based on client ID
     * Ready here indicates it can receive IO.
     */
    downstairs: Mutex<Vec<DownstairsState>>,

    /*
     * Optional encryption context - supply a key in the Opt
     */
    encryption_context: Option<UpstairsEncryptionContext>,
}

impl Upstairs {
    pub fn new(opt: &Opt, guest: Arc<Guest>) -> Arc<Upstairs> {
        /*
         * Creation encryption context if a key is supplied.
         * TODO: sector size is hard coded here.
         */
        let encryption_context = match opt.key_bytes() {
            Some(key) => Some(UpstairsEncryptionContext::new(key, 512)),
            None => None,
        };

        Arc::new(Upstairs {
            guest,
            ds_work: Mutex::new(Work {
                active: HashMap::new(),
                completed: AllocRingBuffer::with_capacity(2048),
                next_id: 1000,
            }),
            flush_info: Mutex::new(FlushInfo::new()),
            ddef: Mutex::new(RegionDefinition::default()),
            downstairs: Mutex::new(Vec::with_capacity(opt.target.len())),
            encryption_context,
        })
    }

    /*
     * If we are doing a flush, the flush number and the rn number
     * must both go up together.  We don't want a lower next_id
     * with a higher flush_number to be possible, as that can introduce
     * dependency deadlock.
     * To also avoid any problems, this method should be called only
     * during the submit_flush method so we know the ds_work and
     * guest_work locks are both held.
     */
    fn next_flush_id(&self) -> u64 {
        let mut fi = self.flush_info.lock().unwrap();
        fi.next_flush()
    }

    pub fn submit_flush(&self, sender: std_mpsc::Sender<i32>) -> Result<()> {
        /*
         * Lock first the guest_work struct where this new job will go,
         * then lock the ds_work struct.  Once we have both we can proceed
         * to build our flush command.
         */
        let mut gw = self.guest.guest_work.lock().unwrap();
        let mut ds_work = self.ds_work.lock().unwrap();

        /*
         * Get the next ID for our new guest work job.  Note that the flush
         * ID and the next_id are connected here, in that all future writes
         * should be flushed at the next flush ID.
         */
        let gw_id: u64 = gw.next_gw_id();
        let next_id = ds_work.next_id();
        let next_flush = self.next_flush_id();

        /*
         * Walk the downstairs work active list, and pull out all the active
         * jobs.  Anything we have not submitted back to the guest.
         *
         * TODO, we can go faster if we:
         * 1. Ignore everything that was before and including the last flush.
         * 2. Ignore reads.
         */
        let mut dep = ds_work.active.keys().cloned().collect::<Vec<u64>>();
        dep.sort_unstable();
        /*
         * TODO: Walk the list of guest work structs and build the same list
         * and make sure it matches.
         */

        /*
         * Build the flush request, and take note of the request ID that
         * will be assigned to this new piece of work.
         */
        let fl = create_flush(next_id, dep, next_flush, gw_id);

        let mut sub = HashMap::new();
        sub.insert(next_id, 0);

        println!("FLUSH: gw_id:{} ds_ids:{:?}", gw_id, sub,);
        let new_gtos =
            GtoS::new(sub, Vec::new(), None, HashMap::new(), sender, None);
        gw.active.insert(gw_id, new_gtos);
        crutrace_gw_start!(|| (gw_id));

        ds_work.enqueue(fl);
        Ok(())
    }

    /*
     * When we have a guest write request with offset and buffer, take them and
     * build both the upstairs work guest tracking struct as well as the downstairs
     * work struct. Once both are ready, submit them to the required places.
     */
    fn submit_write(
        &self,
        offset: u64,
        data: bytes::Bytes,
        sender: std_mpsc::Sender<i32>,
    ) {
        /*
         * Get the next ID for the guest work struct we will make at the
         * end.  This ID is also put into the IO struct we create that
         * handles the operation(s) on the storage side.
         */
        let mut gw = self.guest.guest_work.lock().unwrap();
        let mut ds_work = self.ds_work.lock().unwrap();
        let gw_id: u64 = gw.next_gw_id();

        /*
         * Given the offset and buffer size, figure out what extent and
         * block offset that translates into.  Keep in mind that an offset
         * and length may span two extents, and eventually XXX, two regions.
         */
        let ddef = self.ddef.lock().unwrap();
        let nwo = extent_from_offset(ddef, offset, data.len()).unwrap();
        println!(
            "nwo: {:?} from offset:{} data: {:p} len:{}",
            nwo,
            offset,
            data.as_ptr(),
            data.len()
        );

        /*
         * Now create a downstairs work job for each (eid, bi, len) returned
         * from extent_from_offset
         *
         * Create the list of downstairs request numbers (ds_id) we created
         * on behalf of this guest job.
         */
        let mut sub = HashMap::new();
        let mut new_ds_work = Vec::new();
        let mut next_id: u64;
        let mut cur_offset = 0;

        /* Lock here, through both jobs submitted */
        for (eid, bo, len) in nwo {
            {
                next_id = ds_work.next_id();
            }

            let mut sub_data = data.slice(cur_offset..(cur_offset + len));
            sub.insert(next_id, len);

            if let Some(context) = &self.encryption_context {
                /*
                 * TODO: encryption context is cloned here because encrypt_in_place
                 * will store the id -> offset pair and that will never be removed
                 * from upstairs. We don't need to store cloned context on GtoS because
                 * it is never needed again.
                 */
                let mut mut_data: Vec<u8> = Vec::<u8>::from(&sub_data[..]);
                let mut cloned_context = context.clone();
                cloned_context.encrypt_in_place(
                    &mut mut_data[..],
                    next_id,
                    bo as u128,
                );
                sub_data = Bytes::from(mut_data);
            }

            let wr = create_write_eob(next_id, gw_id, eid, bo, sub_data);
            new_ds_work.push(wr);
            cur_offset = len;
        }
        println!("WRITE: gw_id:{} ds_ids:{:?}", gw_id, sub);

        /*
         * New work created, add to the guest_work HM
         */
        let new_gtos =
            GtoS::new(sub, Vec::new(), None, HashMap::new(), sender, None);
        {
            gw.active.insert(gw_id, new_gtos);
        }
        crutrace_gw_start!(|| (gw_id));

        for wr in new_ds_work {
            ds_work.enqueue(wr);
        }
    }

    /*
     * When we have a guest read request with offset and buffer, take them
     * and build both the upstairs work guest tracking struct as well as the
     * downstairs work struct. Once both are ready, submit them to the
     * required places.
     */
    pub fn submit_read(
        &self,
        offset: u64,
        data: Buffer,
        sender: std_mpsc::Sender<i32>,
    ) {
        /*
         * Get the next ID for the guest work struct we will make at the
         * end.  This ID is also put into the IO struct we create that
         * handles the operation(s) on the storage side.
         */
        let mut gw = self.guest.guest_work.lock().unwrap();
        let mut ds_work = self.ds_work.lock().unwrap();
        let gw_id: u64 = gw.next_gw_id();

        /*
         * We need to know the block size to allow us to convert between
         * bytes and blocks.  Bytes for when we have to slice buffers,
         * blocks for what we send to the downstairs IO.
         */
        let ddef = self.ddef.lock().unwrap();
        let block_size = ddef.block_size() as u32;
        /*
         * Given the offset and buffer size, figure out what extent and
         * block offset that translates into.  Keep in mind that an offset
         * and length may span two extents, and eventually, TODO, two regions.
         */
        let nwo = extent_from_offset(ddef, offset, data.len()).unwrap();
        println!(
            "nwo: {:?} from offset:{} data len:{}",
            nwo,
            offset,
            data.len()
        );
        /*
         * Create the tracking info for downstairs request numbers (ds_id) we
         * will create on behalf of this guest job.
         */
        let mut sub = HashMap::new();
        let mut new_ds_work = Vec::new();
        let mut next_id: u64;

        /*
         * Clone Upstair's encryption context if it exists, attach to
         * GtoS for decryption use before notifying transfer completion.
         */
        let encryption_context = match &self.encryption_context {
            Some(context) => Some(context.clone()),
            None => None,
        };

        /*
         * Now create a downstairs work job for each (eid, bi, len) returned
         * from extent_from_offset
         */
        for (eid, bo, len) in nwo {
            let blocks: u32 = len as u32 / block_size;
            {
                next_id = ds_work.next_id();
            }
            /*
             * When multiple operations are needed to satisfy a read, The offset
             * and length will be divided across two downstairs requests.  It is
             * required (for re-assembly on the other side) that the lower offset
             * corresponds to the lower next_id.  The ID's don't need to be
             * sequential.
             */
            sub.insert(next_id, len);

            /*
            if let Some(context) = encryption_context {
                // Shit.
                context.set_offset(next_id, bo);
            }
            */

            let wr = create_read_eob(next_id, gw_id, eid, bo, blocks);
            new_ds_work.push(wr);
        }

        println!("READ:  gw_id:{} ds_ids:{:?}", gw_id, sub);

        /*
         * New work created, add to the guest_work HM.  New work must be put
         * on the guest_work active HM first, before it lands on the downstairs
         * lists.  We don't want to miss a completion from downstairs.
         */
        assert!(!sub.is_empty());

        let new_gtos = GtoS::new(
            sub,
            Vec::new(),
            Some(data),
            HashMap::new(),
            sender,
            encryption_context,
        );
        {
            gw.active.insert(gw_id, new_gtos);
        }
        crutrace_gw_start!(|| (gw_id));

        for wr in new_ds_work {
            ds_work.enqueue(wr);
        }
    }
}

#[derive(Debug)]
struct FlushInfo {
    flush_numbers: Vec<u64>,
    /*
     * The next flush number to use when a Flush is issued.
     */
    next_flush: u64,
}

impl FlushInfo {
    pub fn new() -> FlushInfo {
        FlushInfo {
            flush_numbers: Vec::new(),
            next_flush: 0,
        }
    }
    /*
     * Upstairs flush_info mutex must be held when calling this.
     * In addition, a downstairs request ID should be obtained at the
     * same time the next flush number is obtained, such that any IO that
     * is given a downstaris request number higer than the request number
     * for the flush will happen after this flush, never before.
     */
    fn next_flush(&mut self) -> u64 {
        let id = self.next_flush;
        self.next_flush += 1;
        id
    }
}
/*
 * I think we will have more states.  If not, then this should just become
 * a bool.
 */
#[derive(Debug, Clone)]
enum DownstairsState {
    _NotReady,
    _Ready,
}

/*
 * A unit of work for downstairs that is put into the hashmap.
 */
#[derive(Debug)]
struct DownstairsIO {
    ds_id: u64,    // This MUST match our hashmap index
    guest_id: u64, // The hahsmap ID from the parent guest work.
    work: IOop,
    /*
     * Hash of work status where key is the downstairs "client id" and the
     * hash value is the current state of the IO request with respect to the
     * upstairs.
     * The length and keys on this hashmap will be used to determine which
     * downstairs will receive the IO request.
     * XXX Determine if it is required for all downstairs to get an entry
     * or if by not putting a downstars in the hash, if that is valid.
     */
    state: HashMap<u8, IOState>,
    /*
     * If the operation is a Read, this holds the resulting buffer
     */
    data: Option<bytes::Bytes>,
}

/*
 * Crucible to storage IO operations.
 */
#[derive(Debug, Clone)]
pub enum IOop {
    Write {
        dependencies: Vec<u64>, // Jobs that must finish before this
        eid: u64,
        block_offset: u64,
        data: bytes::Bytes,
    },
    Read {
        eid: u64,
        block_offset: u64,
        blocks: u32,
    },
    Flush {
        dependencies: Vec<u64>, // Jobs that must finish before this
        flush_number: u64,
    },
}

/*
 * The various states an IO can be in when it is on the work hashmap.
 * There is a state that is unique to each downstairs task we have and
 * they operate independent of each other.
 *
 * New:         A new IO request.
 * InProgress:  The request has been sent to this tasks downstairs.
 * Done:        The response came back from downstairs.
 * Skipped:     The IO request should be ignored.  This situation could be
 *              A read that only needs one downstairs to answer, or we are
 *              doing recovery and we only want a specific downstairs to
 *              do that work.
 * Error:       The IO returned some error.
 */
#[derive(Debug, Clone, PartialEq)]
pub enum IOState {
    New,
    InProgress,
    Done,
    Skipped,
    Error,
}

impl fmt::Display for IOState {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        match self {
            IOState::New => {
                write!(f, " New")
            }
            IOState::InProgress => {
                write!(f, "Sent")
            }
            IOState::Done => {
                write!(f, "Done")
            }
            IOState::Skipped => {
                write!(f, "Skip")
            }
            IOState::Error => {
                write!(f, " Err")
            }
        }
    }
}

/*
 * Provides a shared Buffer that Read operations will write into.
 *
 * Originally BytesMut was used here, but it didn't guarantee that memory
 * was shared between cloned BytesMut objects.
 */
#[derive(Clone, Debug)]
pub struct Buffer {
    data: Arc<Mutex<Vec<u8>>>,
}

impl Buffer {
    /*
     * XXX: For now, assert Buffer size is at least hard coded block size 512.
     */

    pub fn from_vec(vec: Vec<u8>) -> Buffer {
        assert!(vec.len() >= 512);

        Buffer {
            data: Arc::new(Mutex::new(vec)),
        }
    }

    pub fn new(len: usize) -> Buffer {
        assert!(len >= 512);

        let mut vec = Vec::<u8>::with_capacity(len);
        vec.resize(len, 0);

        Buffer {
            data: Arc::new(Mutex::new(vec)),
        }
    }

    pub fn from_slice(buf: &[u8]) -> Buffer {
        assert!(buf.len() >= 512);

        let mut vec = Vec::<u8>::with_capacity(buf.len());
        for item in buf {
            vec.push(*item);
        }

        Buffer::from_vec(vec)
    }

    pub fn len(&self) -> usize {
        self.data.try_lock().unwrap().len()
    }

    pub fn is_empty(&self) -> bool {
        self.len() == 0
    }

    pub fn as_vec(&self) -> MutexGuard<Vec<u8>> {
        self.data.try_lock().unwrap()
    }
}

#[test]
fn test_buffer_len() {
    const READ_SIZE: usize = 512;
    let data = Buffer::from_slice(&[0x99; READ_SIZE]);
    assert_eq!(data.len(), READ_SIZE);
}

#[test]
fn test_buffer_len_after_clone() {
    const READ_SIZE: usize = 512;
    let data = Buffer::from_slice(&[0x99; READ_SIZE]);
    assert_eq!(data.len(), READ_SIZE);

    let new_buffer = data.clone();
    assert_eq!(new_buffer.len(), READ_SIZE);
}

#[test]
#[should_panic(
    expected = "index out of bounds: the len is 512 but the index is 512"
)]
fn test_buffer_len_index_overflow() {
    const READ_SIZE: usize = 512;
    let data = Buffer::from_slice(&[0x99; READ_SIZE]);
    assert_eq!(data.len(), READ_SIZE);

    let mut vec = data.as_vec();
    assert_eq!(vec.len(), 512);

    for i in 0..(READ_SIZE + 1) {
        vec[i] = 0x99;
    }
}

#[test]
fn test_buffer_len_over_block_size() {
    const READ_SIZE: usize = 600;
    let data = Buffer::from_slice(&[0x99; READ_SIZE]);
    assert_eq!(data.len(), READ_SIZE);
}

/*
 * Inspired from Propolis block.rs
 *
 * The following are the operations that Crucible supports from outside callers.
 * We have extended this to cover a bunch of test operations as well.
 * The first three are the supported operations, the other operations
 * tell the upstairs to behave in specific ways.
 */
#[derive(Debug)]
pub enum BlockOp {
    Read { offset: u64, data: Buffer },
    Write { offset: u64, data: bytes::Bytes },
    Flush,
    // Begin testing options.
    Commit,   // Send update to all tasks that there is work on the queue.
    ShowWork, // Show the status of the internal work hashmap and done Vec.
}

/*
 * This structure is for tracking the underlying storage side operations
 * that map to a single Guest IO request. G to S stands for Guest
 * to Storage.
 *
 * The submitted hashmap is indexd by the request number (ds_id) for the
 * downstairs requests issued on behalf of this request.
 */
#[derive(Debug)]
struct GtoS {
    /*
     * Jobs we have submitted (or will soon submit) to the storage side
     * of the upstairs process to send on to the downstairs.
     * The key for the hashmap is the ds_id number in the hashmap for
     * downstairs work. The value is the buffer size of the operation.
     */
    submitted: HashMap<u64, usize>,

    completed: Vec<u64>,

    /*
     * This buffer is provided by the guest request. If this is a read,
     * data will be written here.
     */
    guest_buffer: Option<Buffer>,

    /*
     * When we have an IO between the guest and crucible, it's possible
     * it will be broken into two smaller requests if the range happens
     * to cross an extent boundary.  This hashmap is a list of those
     * buffers with the key being the downstairs request ID.
     *
     * Data moving in/out of this buffer will be encrypted or decrypted
     * depending on the operation.
     */
    downstairs_buffer: HashMap<u64, bytes::Bytes>,

    /*
     * Notify the caller waiting on the job to finish.
     */
    sender: std_mpsc::Sender<i32>,

    encryption_context: Option<UpstairsEncryptionContext>,
}

impl GtoS {
    pub fn new(
        submitted: HashMap<u64, usize>,
        completed: Vec<u64>,
        guest_buffer: Option<Buffer>,
        downstairs_buffer: HashMap<u64, bytes::Bytes>,
        sender: std_mpsc::Sender<i32>,
        encryption_context: Option<UpstairsEncryptionContext>,
    ) -> GtoS {
        GtoS {
            submitted,
            completed,
            guest_buffer,
            downstairs_buffer,
            sender,
            encryption_context,
        }
    }

    /*
     * When all downstairs jobs have completed, and all buffers have been
     * attached to the GtoS struct, we can do the final copy of the data
     * from upstairs memory back to the guest's memory.
     */
    fn transfer(&mut self) {
        if let Some(guest_buffer) = &mut self.guest_buffer {
            self.completed.sort_unstable();
            assert!(!self.completed.is_empty());

            for ds_id in self.completed.iter() {
                // println!("Copy buff from {:?}", ds_id);
                let ds_buf = self.downstairs_buffer.remove(ds_id).unwrap();

                {
                    let mut vec = guest_buffer.as_vec();
                    for i in 0..ds_buf.len() {
                        vec[i] = ds_buf[i];
                    }

                    /*
                     * If there's an encryption context, decrypt the guest memory in place.
                     */
                    if let Some(context) = &self.encryption_context {
                        context.decrypt_in_place(&mut vec[..], *ds_id);
                    }
                }
                // println!("Final data copy {} {:#?}", ds_id, ds_buf);
            }
            println!("Final data copy {:?}", self.completed);
        } else {
            /*
             * Should this panic?  If the caller is requesting a transfer,
             * the guest_buffer should exist.  If it does not exist, then
             * either there is a real problem, or the operation was a write
             * or flush and why are we requesting a transfer for those.
             */
            panic!("No guest buffer, no copy");
        }
    }

    /*
     * Notify corresponding BlockReqWaiter
     */
    pub fn notify(&mut self) {
        /*
         * If the guest is no longer listening and this returns an error,
         * do we care?  This could happen if the guest has given up
         * becuase an IO took too long, or other possible guest side reasons.
         */
        if self.sender.send(0).is_ok() {}
    }
}

/**
 * This structure keeps track of work that Crucible has accepted from the
 * "Guest", aka, Propolis.
 *
 * The active is a hashmap of GtoS structures for all I/Os that are
 * outstanding.  Either just created or in progress operations.  The key
 * for a new job comes from next_gw_id and should always increment.
 *
 * Once we have decided enough downstairs requests are finished, we remove
 * the entry from the active and add the gw_id to the completed vec.
 *
 * TODO: The completed needs to implement some notify back to the Guest, and
 * it should probably be a ring buffer.
 */
#[derive(Debug)]
struct GuestWork {
    active: HashMap<u64, GtoS>,
    next_gw_id: u64,
    completed: AllocRingBuffer<u64>,
}

impl GuestWork {
    fn next_gw_id(&mut self) -> u64 {
        let id = self.next_gw_id;
        self.next_gw_id += 1;
        id
    }

    /**
     * Move a GtoS job from the active to completed.
     * It is at this point we can notify the Guest their IO is done
     * and any buffers provided should now have the data in them.
     */
    fn complete(&mut self, gw_id: u64) {
        let gtos_job = self.active.remove(&gw_id).unwrap();
        assert!(gtos_job.submitted.is_empty());
        self.completed.push(gw_id);
        crutrace_gw_end!(|| (gw_id));
    }

    /**
     * When the required number of downstairs completions for a downstairs
     * ds_id have arrived, we call this method on the parent GuestWork
     * that requested them and include the DownstairsIO struct.
     *
     * If this operation was a read, then we attach the read buffer to the
     * GtoS struct for later transfer, if it is not already present.
     *
     * A single GtoS job may have multiple downstairs jobs it created, so
     * we may not be done yet.  When all the downstairs jobs finish, we
     * can move forward with finishing up the guest work operation.
     * This may include moving/decrypting data buffers from completed reads.
     *
     * TODO: Error handling case needs to come through here in a way that
     * won't break if enough of the IO completed to satisfy the upstairs, but
     * will be handled if all the downstairs have returned error.
     */
    fn ds_complete(&mut self, done: DownstairsIO) {
        let gw_id = done.guest_id;
        let ds_id = done.ds_id;
        /*
         * A job that already finished and results were sent back to
         * the guest could still have a valid ds_id, but no gw_id for
         * it to report to.
         */
        if let Some(gtos_job) = self.active.get_mut(&gw_id) {
            /*
             * If the ds_id is on the submitted list, then we will take it off
             * and Possibly add the read result buffer to the gtos job
             * structure for later copying.
             *
             * If it's not, then verify our ds_id is already on the completed
             * list, just to catch any problems.
             */
            if gtos_job.submitted.remove(&ds_id).is_some() {
                if let Some(data) = done.data {
                    /*
                     * The first read buffer will become the source for the
                     * final response back to the guest.  This buffer will be
                     * combined with other buffers if the upstairs request
                     * required multiple jobs.
                     */
                    if gtos_job.downstairs_buffer.insert(ds_id, data).is_some()
                    {
                        println!(
                            "gw_id:{} read buffer already present for {}",
                            gw_id, ds_id
                        );
                        /*
                         * This may panic at some future point when only one
                         * read should be enough to satisfy the guest
                         * reuquest, though it may depend on how we make the
                         * different downstairs consistent.  XXX
                         */
                    } else {
                        println!(
                            "gw_id:{} Save read buffer for {}",
                            gw_id, ds_id
                        );
                    }
                } // XXX else can we assert we don't expect data?

                gtos_job.completed.push(ds_id);
            } else {
                // XXX Should this just panic?
                println!("gw_id:{} ({}) already removed???", gw_id, ds_id);
                assert!(gtos_job.completed.contains(&ds_id));
            }

            /*
             * If all the downstairs jobs created for this have completed,
             * we can copy (if present) read data back to the guest buffer
             * they provided to us, and notify any waiters.
             */
            if gtos_job.submitted.is_empty() {
                if gtos_job.guest_buffer.is_some() {
                    gtos_job.transfer();
                }
                gtos_job.notify();
                self.complete(gw_id);
            }
        } else {
            /*
             * When we support a single successful read, or 2/3 writes
             * or flushes starting the completion back to the guest, this
             * will no longer be a panic, as that can be a valid state.
             */
            panic!(
                "gw_id {} from removed job {} not on active list",
                gw_id, ds_id
            );
        }
    }
}

/**
 * Couple a BlockOp with a notifier for calling code.
 */
#[derive(Debug)]
pub struct BlockReq {
    op: BlockOp,
    send: std_mpsc::Sender<i32>,
}

impl BlockReq {
    // https://docs.rs/tokio/1.9.0/tokio/sync/mpsc/index.html#communicating-between-sync-and-async-code
    // return the std::sync::mpsc Sender to non-tokio task callers
    fn new(op: BlockOp, send: std_mpsc::Sender<i32>) -> BlockReq {
        Self { op, send }
    }
}

/**
 * When BlockOps are sent to a guest, the calling function receives a
 * waiter that it can block on.
 */
pub struct BlockReqWaiter {
    recv: std_mpsc::Receiver<i32>,
}

impl BlockReqWaiter {
    fn new(recv: std_mpsc::Receiver<i32>) -> BlockReqWaiter {
        Self { recv }
    }

    pub fn block_wait(&mut self) {
        // TODO: Instead of i32, errors should be bubbled up.
        let _ = self.recv.recv();
    }
}

/**
 * This is the structure we use to keep track of work passed into crucible
 * from the the "Guest".
 *
 * Requests from the guest are put into the reqs VecDeque initally.
 *
 * A task on the Crucible side will receive a notification that a new
 * operation has landed on the reqs queue and will take action:
 *   Pop the request off the reqs queue.
 *   Copy (and optionally encrypt) any data buffers provided to us by the Guest.
 *   Create one or more downstairs DownstairsIO structures.
 *   Create a GtoS tracking structure with the id's for each
 *   downstairs task and the read result buffer if required.
 *   Add the GtoS struct to the in GuestWork active work hashmap.
 *   Put all the DownstairsIO strucutres on the downstairs work queue.
 *   Send notification to the upstairs tasks that there is new work.
 *
 * Work here will be added to storage side queues and the responses will
 * be waited on and processed when they arrive.
 *
 * This structure and operations on in handle the translation between
 * outside requests and internal upstairs structures and work queues.
 */
#[derive(Debug)]
pub struct Guest {
    /*
     * New requests from outside go onto this VecDeque.  The notify is how
     * the submittion task tells the listening task that new work has been
     * added.
     */
    reqs: Mutex<VecDeque<BlockReq>>,
    notify: Notify,

    /*
     * When the crucible listening task has noticed a new IO request, it will
     * pull it from the reqs queue and create an GuestWork struct as well as
     * convert the new IO request into the matching downstairs request(s).
     * Each new GuestWork request will get a unique gw_id, which is also
     * the index for that operation into the hashmap.
     *
     * It is during this process that data will encrypted.  For a read, the
     * data is decrypted back to the guest provided buffer after all the
     * required downstairs operations are completed.
     */
    guest_work: Mutex<GuestWork>,
}

/*
 * These methods are how to add or checking for new work on the Guest struct
 */
impl Guest {
    pub fn new() -> Guest {
        Guest {
            /*
             * Incoming I/O requests are added to this queue.
             */
            reqs: Mutex::new(VecDeque::new()),
            notify: Notify::new(),
            /*
             * The active hashmap is for in-flight I/O operations
             * that we have taken off the incoming queue, but we have not
             * received the response from downstairs.
             * Note that a single IO from outside may have multiple I/O
             * requests that need to finish before we can complete that IO.
             */
            guest_work: Mutex::new(GuestWork {
                active: HashMap::new(), // GtoS
                next_gw_id: 1,
                completed: AllocRingBuffer::with_capacity(2048),
            }),
        }
    }

    /*
     * This is used to submit a new BlockOp IO request to Crucible.
     */
    pub fn send(&self, op: BlockOp) -> BlockReqWaiter {
        let (send, recv) = std_mpsc::channel();

        self.reqs.lock().unwrap().push_back(BlockReq::new(op, send));
        self.notify.notify_one();

        BlockReqWaiter::new(recv)
    }

    /*
     * A crucible task will listen for new work using this.
     */
    pub async fn recv(&self) -> BlockReq {
        loop {
            if let Some(req) = self.reqs.lock().unwrap().pop_front() {
                return req;
            }
            self.notify.notified().await;
        }
    }
}

impl Default for Guest {
    fn default() -> Self {
        Self::new()
    }
}

pub struct Target {
    #[allow(dead_code)]
    target: SocketAddrV4,
    input: watch::Sender<u64>,
}

#[derive(Debug)]
struct Condition {
    target: SocketAddrV4,
    connected: bool,
}

/*
 * Send work to all the targets on this vector.
 * This can be much simpler, but we need to (eventually) take special action
 * when we fail to send a message to a task.
 */
fn _send_work(t: &[Target], val: u64) {
    for d_client in t.iter() {
        // println!("#### send to client {:?}", d_client.target);
        let res = d_client.input.send(val);
        if let Err(e) = res {
            panic!("#### error {:#?} sending work to {:?}", e, d_client.target);
            /*
             * TODO Write more code for this error,  If one downstairs
             * never receives a request, it may get picked up on the
             * next request.  However, if the downstairs has gone away,
             * then action will need to be taken, and soon.
             */
        }
    }
}

/**
 * We listen on the ds_done channel to know when all the downstairs requests
 * for a downstairs work task have finished and it is time to complete
 * any buffer transfers (reads) and then notify the guest that their
 * work has been completed.
 */
async fn up_ds_listen(up: &Arc<Upstairs>, mut ds_done_rx: mpsc::Receiver<u64>) {
    while let Some(_ds_id) = ds_done_rx.recv().await {
        /*
         * XXX Do we need to hold the lock while we process all the
         * completed jobs?  We should be continuing to send message over
         * the ds_done_tx channel, so if new things show up while we
         * process the set of things we know are done now, then the
         * ds_done_rx.recv() should trigger when we loop.
         */
        let done_list = up.ds_work.lock().unwrap().completed_work();
        // println!( "rcv:{} Done List: {:?}", ds_id, done_list);

        for ds_id_done in done_list.iter() {
            let mut work = up.ds_work.lock().unwrap();

            /*
             * TODO: retire means the downstairs is "consistent" with
             * regards to this IO, so any internal info about this operation
             * should now consider it as ack'd to the guest.
             */
            let done = work.retire(*ds_id_done);

            println!(
                "RETIRE:  ds_id {} from gw_id:{:?}",
                ds_id_done, done.guest_id,
            );
            drop(work);

            let mut gw = up.guest.guest_work.lock().unwrap();
            gw.ds_complete(done);
        }
    }
}
/*
 * This task will loop forever and watch the Guest structure for new IO
 * operations showing up.  When one is detected, the type is checked and the
 * operation is translated into the corresponding upstairs IO type and put on
 * the internal upstairs queue.
 */
async fn up_listen(up: &Arc<Upstairs>, dst: Vec<Target>) {
    /*
     * XXX Once we move this function to being called after all downstairs are
     * online, we can remove this sleep
     */
    tokio::time::sleep(Duration::from_secs(1)).await;

    let mut lastcast = 1;
    loop {
        let req = up.guest.recv().await;
        match req.op {
            BlockOp::Read { offset, data } => {
                up.submit_read(offset, data, req.send);
                // Send the message that there is new work to do
                dst.iter().for_each(|t| t.input.send(lastcast).unwrap());
                lastcast += 1;
            }
            BlockOp::Write { offset, data } => {
                //guest_submit_write(up, offset, data, req.send);
                up.submit_write(offset, data, req.send);
                dst.iter().for_each(|t| t.input.send(lastcast).unwrap());
                lastcast += 1;
            }
            BlockOp::Flush => {
                up.submit_flush(req.send).unwrap();
                dst.iter().for_each(|t| t.input.send(lastcast).unwrap());
                lastcast += 1;
            }
            BlockOp::ShowWork => {
                show_all_work(up);
            }
            BlockOp::Commit => {
                dst.iter().for_each(|t| t.input.send(lastcast).unwrap());
                lastcast += 1;
            }
        }
    }
}

/*
 * This is the main upstairs task that starts all the other async
 * tasks.
 * XXX At the moment, this function is only half complete, and will
 * probably need a re-write.
 */
pub async fn up_main(opt: Opt, guest: Arc<Guest>) -> Result<()> {
    match register_probes() {
        Ok(()) => {
            println!("DTrace probes registered ok");
        }
        Err(e) => {
            println!("Error registering DTrace probes: {:?}", e);
        }
    }

    /*
     * Build the Upstairs struct that we use to share data between
     * the different async tasks
     */
    let up = Upstairs::new(&opt, guest);

    /*
     * Use this channel to receive updates on target status from each task
     * we create to connect to a downstairs.
     */
    let (ctx, mut crx) = mpsc::channel::<Condition>(32);

    /*
     * Use this channel to indicate in the upstairs that all downstairs
     * operations for a specific request have completed.
     */
    let (ds_done_tx, ds_done_rx) = mpsc::channel(100);

    /*
     * spawn a task to listen for ds completed work which will then
     * take care of transitioning guest work structs to done.
     */
    let upc = Arc::clone(&up);
    tokio::spawn(async move {
        up_ds_listen(&upc, ds_done_rx).await;
    });

    let mut client_id = 0;
    /*
     * Create one downstaris task (dst) for each target in the opt
     * structure that was passed to us.
     */
    let dst = opt
        .target
        .iter()
        .map(|dst| {
            /*
             * Create the channel that we will use to request that the loop
             * check for work to do in the central structure.
             */
            let (itx, irx) = watch::channel(100); // XXX 100?

            let up = Arc::clone(&up);
            let ctx = ctx.clone();
            let t0 = *dst;
            let ds_done_tx = ds_done_tx.clone();
            tokio::spawn(async move {
                looper(t0, irx, ctx, &up, client_id, ds_done_tx).await;
            });
            client_id += 1;

            Target {
                target: *dst,
                input: itx,
            }
        })
        .collect::<Vec<_>>();

    /*
     * Create a task to listen for work from outside.
     *
     * The role of this task is to move work between the outside
     * work queue and the internal Upstairs work queue, as well as send
     * completion messages and/or copy data back to the outside.
     *
     * XXX This needs a little more work.  We should not start to listen
     * to the outside until we know that all our downstairs are ready to
     * take IO operations.
     */
    let upl = Arc::clone(&up);
    tokio::spawn(async move {
        up_listen(&upl, dst).await;
    });

    // async tasks need to tell us they are alive, but they also need to
    // tell us the extent list from any attached downstairs.
    // That part is not connected yet. XXX
    let mut ds_count = 0u32;
    loop {
        let c = crx.recv().await.unwrap();
        if c.connected {
            ds_count += 1;
            println!(
                "#### {:?} #### CONNECTED ######## {}/{}",
                c.target,
                ds_count,
                opt.target.len()
            );
        } else {
            println!("#### {:?} #### DISCONNECTED! ####", c.target);
            ds_count -= 1;
        }
        /*
         * We need some additional way to indicate that this upstairs is ready
         * to receive work.  Just connecting to n downstairs is not enough,
         * we need to also know that they all have the same data.
         */
    }
}

/*
 * Create a write DownstairsIO structure from an EID, and offset, and
 * the data buffer
 */
fn create_write_eob(
    ds_id: u64,
    gw_id: u64,
    eid: u64,
    block_offset: u64,
    data: bytes::Bytes,
) -> DownstairsIO {
    let awrite = IOop::Write {
        dependencies: Vec::new(), // XXX Coming soon
        eid,
        block_offset,
        data,
    };

    let mut state = HashMap::new();
    for cl in 0..3 {
        state.insert(cl, IOState::New);
    }
    DownstairsIO {
        ds_id,
        guest_id: gw_id,
        work: awrite,
        state,
        data: None,
    }
}

/*
 * Create a write DownstairsIO structure from an EID, and offset, and the
 * data buffer.  Used for converting a guest IO reead request into a
 * DownstairsIO that the downstairs can understand.
 */
fn create_read_eob(
    ds_id: u64,
    gw_id: u64,
    eid: u64,
    block_offset: u64,
    blocks: u32,
) -> DownstairsIO {
    let aread = IOop::Read {
        eid,
        block_offset,
        blocks,
    };

    let mut state = HashMap::new();
    for cl in 0..3 {
        state.insert(cl, IOState::New);
    }
    DownstairsIO {
        ds_id,
        guest_id: gw_id,
        work: aread,
        state,
        data: None,
    }
}

/*
 * Create a flush DownstairsIO structure.
 */
fn create_flush(
    ds_id: u64,
    dependencies: Vec<u64>,
    flush_number: u64,
    guest_id: u64,
) -> DownstairsIO {
    let flush = IOop::Flush {
        dependencies,
        flush_number,
    };

    let mut state = HashMap::new();
    for cl in 0..3 {
        state.insert(cl, IOState::New);
    }
    DownstairsIO {
        ds_id,
        guest_id,
        work: flush,
        state,
        data: None,
    }
}

/*
 * Debug function to display the work hashmap with status for all three of
 * the clients.
 */
#[allow(unused_variables)]
fn show_all_work(up: &Arc<Upstairs>) {
    println!("######### Crucible Downstairs work queue #####");
    println!("######### ############################## #####");
    let work = up.ds_work.lock().unwrap();
    let mut kvec: Vec<u64> = work.active.keys().cloned().collect::<Vec<u64>>();
    kvec.sort_unstable();
    for id in kvec.iter() {
        let job = work.active.get(id).unwrap();
        let job_type = match &job.work {
            IOop::Read {
                eid,
                block_offset,
                blocks,
            } => "Read ".to_string(),
            IOop::Write {
                dependencies,
                eid,
                block_offset,
                data,
            } => "Write".to_string(),
            IOop::Flush {
                dependencies,
                flush_number,
            } => "Flush".to_string(),
        };
        print!("JOB:[{:04}] {} ", id, job_type);
        for cid in 0..3 {
            let state = job.state.get(&cid);
            match state {
                Some(state) => {
                    print!("[{}] state: {}  ", cid, state);
                }
                x => {
                    print!("[{}] unknown state:{:#?}", cid, x);
                }
            }
        }
        println!();
    }
    let done = work.completed.to_vec();
    println!("Done tasks: {:?}", done.len());
    drop(work);
    show_guest_work(&up.guest);
}

/*
 * Debug function to dump the guest work structure.
 * This does a bit while holding the mutex, so don't expect performance
 * to get better when calling it.
 *
 * TODO: make this one big dump, where we include the up.work.active
 * printing for each guest_work.  It will be much more dense, but require
 * holding both locks for the duration.
 */
fn show_guest_work(guest: &Arc<Guest>) {
    println!("Guest work:  Active and Completed Jobs:");
    let gw = guest.guest_work.lock().unwrap();
    let mut kvec: Vec<u64> = gw.active.keys().cloned().collect::<Vec<u64>>();
    kvec.sort_unstable();
    for id in kvec.iter() {
        let job = gw.active.get(id).unwrap();
        println!(
            "GW_JOB active:[{:04}] S:{:?} C:{:?} ",
            id, job.submitted, job.completed
        );
    }
    let done = gw.completed.to_vec();
    println!("GW_JOB completed:{:?} ", done);
}

#[cfg(test)]
mod test {
    use super::*;
    /*
     * Beware, if you change these defaults, then you will have to change
     * all the hard coded tests below that use make_upstairs().
     */
    fn make_upstairs() -> Arc<Upstairs> {
        let mut def = RegionDefinition::default();
        def.set_block_size(512);
        def.set_extent_size(100);
        def.set_extent_count(10);

        Arc::new(Upstairs {
            ds_work: Mutex::new(Work {
                active: HashMap::new(),
                completed: AllocRingBuffer::with_capacity(2),
                next_id: 1000,
            }),
            flush_info: Mutex::new(FlushInfo::new()),
            ddef: Mutex::new(def),
            downstairs: Mutex::new(Vec::with_capacity(1)),
            guest: Arc::new(Guest::new()),
            encryption_context: None,
        })
    }

    /*
     * Terrible wrapper, but it allows us to call extent_from_offset()
     * just like the program does.
     */
    fn up_efo(
        up: &Arc<Upstairs>,
        offset: u64,
        len: usize,
    ) -> Result<Vec<(u64, u64, usize)>> {
        let ddef = up.ddef.lock().unwrap();
        extent_from_offset(ddef, offset, len)
    }

    #[test]
    fn off_to_extent_basic() {
        /*
         * Verify the offsets match the expected block_offset for the
         * default size region.
         */
        let up = make_upstairs();

        let exv = vec![(0, 0, 512)];
        assert_eq!(up_efo(&up, 0, 512).unwrap(), exv);
        let exv = vec![(0, 1, 512)];
        assert_eq!(up_efo(&up, 512, 512).unwrap(), exv);
        let exv = vec![(0, 2, 512)];
        assert_eq!(up_efo(&up, 1024, 512).unwrap(), exv);
        let exv = vec![(0, 3, 512)];
        assert_eq!(up_efo(&up, 1024 + 512, 512).unwrap(), exv);
        let exv = vec![(0, 99, 512)];
        assert_eq!(up_efo(&up, 51200 - 512, 512).unwrap(), exv);

        let exv = vec![(1, 0, 512)];
        assert_eq!(up_efo(&up, 51200, 512).unwrap(), exv);
        let exv = vec![(1, 1, 512)];
        assert_eq!(up_efo(&up, 51200 + 512, 512).unwrap(), exv);
        let exv = vec![(1, 2, 512)];
        assert_eq!(up_efo(&up, 51200 + 1024, 512).unwrap(), exv);
        let exv = vec![(1, 99, 512)];
        assert_eq!(up_efo(&up, 102400 - 512, 512).unwrap(), exv);

        let exv = vec![(2, 0, 512)];
        assert_eq!(up_efo(&up, 102400, 512).unwrap(), exv);

        let exv = vec![(9, 99, 512)];
        assert_eq!(up_efo(&up, (512 * 100 * 10) - 512, 512).unwrap(), exv);
    }

    #[test]
    fn off_to_extent_buffer() {
        /*
         * Testing a buffer size larger than the default 512
         */
        let up = make_upstairs();

        let exv = vec![(0, 0, 1024)];
        assert_eq!(up_efo(&up, 0, 1024).unwrap(), exv);
        let exv = vec![(0, 1, 1024)];
        assert_eq!(up_efo(&up, 512, 1024).unwrap(), exv);
        let exv = vec![(0, 2, 1024)];
        assert_eq!(up_efo(&up, 1024, 1024).unwrap(), exv);
        let exv = vec![(0, 98, 1024)];
        assert_eq!(up_efo(&up, 51200 - 1024, 1024).unwrap(), exv);

        let exv = vec![(1, 0, 1024)];
        assert_eq!(up_efo(&up, 51200, 1024).unwrap(), exv);
        let exv = vec![(1, 1, 1024)];
        assert_eq!(up_efo(&up, 51200 + 512, 1024).unwrap(), exv);
        let exv = vec![(1, 2, 1024)];
        assert_eq!(up_efo(&up, 51200 + 1024, 1024).unwrap(), exv);
        let exv = vec![(1, 98, 1024)];
        assert_eq!(up_efo(&up, 102400 - 1024, 1024).unwrap(), exv);

        let exv = vec![(2, 0, 1024)];
        assert_eq!(up_efo(&up, 102400, 1024).unwrap(), exv);

        let exv = vec![(9, 98, 1024)];
        assert_eq!(up_efo(&up, (512 * 100 * 10) - 1024, 1024).unwrap(), exv);
    }

    #[test]
    fn off_to_extent_vbuff() {
        let up = make_upstairs();

        /*
         * Walk the buffer sizes from 512 to the whole extent, make sure
         * it all works as expected
         */
        for bsize in (512..=51200).step_by(512) {
            let exv = vec![(0, 0, bsize)];
            assert_eq!(up_efo(&up, 0, bsize).unwrap(), exv);
        }
    }

    #[test]
    fn off_to_extent_bridge() {
        /*
         * Testing when our buffer crosses extents.
         */
        let up = make_upstairs();
        /*
         * 1024 buffer
         */
        let exv = vec![(0, 99, 512), (1, 0, 512)];
        assert_eq!(up_efo(&up, 51200 - 512, 1024).unwrap(), exv);
        let exv = vec![(0, 98, 1024), (1, 0, 1024)];
        assert_eq!(up_efo(&up, 51200 - 1024, 2048).unwrap(), exv);

        /*
         * Largest buffer
         */
        let exv = vec![(0, 1, 51200 - 512), (1, 0, 512)];
        assert_eq!(up_efo(&up, 512, 51200).unwrap(), exv);
        let exv = vec![(0, 2, 51200 - 1024), (1, 0, 1024)];
        assert_eq!(up_efo(&up, 1024, 51200).unwrap(), exv);
        let exv = vec![(0, 4, 51200 - 2048), (1, 0, 2048)];
        assert_eq!(up_efo(&up, 2048, 51200).unwrap(), exv);

        /*
         * Largest buffer, last block offset possible
         */
        let exv = vec![(0, 99, 512), (1, 0, 51200 - 512)];
        assert_eq!(up_efo(&up, 51200 - 512, 51200).unwrap(), exv);
    }

    /*
     * Testing various invalid inputs
     */
    #[test]
    #[should_panic]
    fn off_to_extent_length_zero() {
        let up = make_upstairs();
        up_efo(&up, 0, 0).unwrap();
    }
    #[test]
    #[should_panic]
    fn off_to_extent_block_align() {
        let up = make_upstairs();
        up_efo(&up, 0, 511).unwrap();
    }
    #[test]
    #[should_panic]
    fn off_to_extent_block_align2() {
        let up = make_upstairs();
        up_efo(&up, 0, 513).unwrap();
    }
    #[test]
    #[should_panic]
    fn off_to_extent_length_big() {
        let up = make_upstairs();
        up_efo(&up, 0, 51200 + 512).unwrap();
    }
    #[test]
    #[should_panic]
    fn off_to_extent_offset_align() {
        let up = make_upstairs();
        up_efo(&up, 511, 512).unwrap();
    }
    #[test]
    #[should_panic]
    fn off_to_extent_offset_align2() {
        let up = make_upstairs();
        up_efo(&up, 513, 512).unwrap();
    }
    #[test]
    #[should_panic]
    fn off_to_extent_offset_big() {
        let up = make_upstairs();
        up_efo(&up, 512000, 512).unwrap();
    }
}
