#![feature(asm)]
use std::collections::{HashMap, VecDeque};
use std::fmt;
use std::io::{Read, Result as IOResult, Seek, SeekFrom, Write};
use std::net::SocketAddrV4;
use std::sync::mpsc as std_mpsc;
use std::sync::{Arc, Mutex, MutexGuard};
use std::time::Duration;

use crucible_common::*;
use crucible_protocol::*;

use anyhow::{anyhow, bail, Result};
use bytes::{BufMut, Bytes, BytesMut};
use futures::{SinkExt, StreamExt};
use rand::prelude::*;
use ringbuffer::{AllocRingBuffer, RingBufferExt, RingBufferWrite};
use tokio::net::tcp::WriteHalf;
use tokio::net::{TcpSocket, TcpStream};
use tokio::sync::{mpsc, watch, Notify};
use tokio::time::{sleep_until, Instant};
use tokio_util::codec::{FramedRead, FramedWrite};
use tracing::{instrument, span, Level};
use usdt::register_probes;

// Include the Rust implementation generated by the build script.
include!(concat!(env!("OUT_DIR"), "/crutrace.rs"));

#[derive(Debug)]
pub struct CrucibleOpts {
    pub target: Vec<SocketAddrV4>,
    pub lossy: bool,
}

pub fn deadline_secs(secs: u64) -> Instant {
    Instant::now()
        .checked_add(Duration::from_secs(secs))
        .unwrap()
}

async fn proc_frame(
    u: &Arc<Upstairs>,
    m: &Message,
    up_coms: UpComs,
) -> Result<()> {
    match m {
        Message::Imok => Ok(()),
        Message::WriteAck(ds_id) => Ok(io_completed(
            u,
            *ds_id,
            up_coms.client_id,
            None,
            up_coms.ds_done_tx,
        )
        .await?),
        Message::FlushAck(ds_id) => Ok(io_completed(
            u,
            *ds_id,
            up_coms.client_id,
            None,
            up_coms.ds_done_tx,
        )
        .await?),
        Message::ReadResponse(ds_id, data) => Ok(io_completed(
            u,
            *ds_id,
            up_coms.client_id,
            Some(data.clone()),
            up_coms.ds_done_tx,
        )
        .await?),
        x => bail!("client {} unexpected frame {:?}", up_coms.client_id, x),
    }
}

/*
 * Convert a virtual block offset and length into a Vec of tuples:
 *
 *     Extent number (EID), Byte offset, Length in bytes
 *
 * - length in bytes can be up to the region size
 */
pub fn extent_from_offset(
    ddef: RegionDefinition,
    offset: u64,
    len: usize,
) -> Result<Vec<(u64, u64, usize)>> {
    assert!(len > 0);

    let mut result = Vec::new();
    let space_per_extent = ddef.block_size() * ddef.extent_size();
    assert!(
        (offset + len as u64)
            <= (space_per_extent * ddef.extent_count() as u64)
    );

    /*
     *
     *  |eid0                  |eid1
     *       |───────────────────────>│
     *  ┌────|─────────────────|──────┼───────────────┐
     *  │    |                 |      │               │
     *  └────|─────────────────|──────┼───────────────┘
     *     offset               offset + len
     */
    let mut o = offset;
    let mut len_left = len;

    while len_left > 0 {
        /*
         * XXX We only support a single region (downstairs).  When we grow to
         * support a LBA size that is larger than a single region, then we will
         * need to write more code. But - that code may live upstairs?
         */
        let eid: u64 = o / space_per_extent;
        assert!((eid as u32) < ddef.extent_count());

        let extent_offset = o % space_per_extent;
        let mut sz = space_per_extent as usize - extent_offset as usize;
        if len_left < sz {
            sz = len_left;
        }

        result.push((eid, extent_offset, sz));

        match len_left.checked_sub(sz) {
            Some(v) => {
                len_left = v;
            }
            None => {
                break;
            }
        }

        o += sz as u64;
    }

    {
        let mut bytes = 0;
        for r in &result {
            bytes += r.2;
        }
        assert_eq!(bytes, len);
    }

    {
        let mut bytes = 0;
        for i in 0..result.len() {
            let r = result[i];
            if i == (result.len() - 1) {
                bytes += r.2 as u64;
            } else {
                bytes += space_per_extent - r.1;
            }
        }
        assert_eq!(bytes, len as u64);
    }

    Ok(result)
}

/*
 * Decide what to do with a downstairs that has just connected and has
 * sent us information about its extents.
 *
 * This will eventually need to message the main thread so it knows
 * when it can starting doing I/O.
 */
fn process_downstairs(
    target: &SocketAddrV4,
    u: &Arc<Upstairs>,
    bs: u64,
    es: u64,
    ec: u32,
    versions: Vec<u64>,
) -> Result<()> {
    println!(
        "{} Evaluate new downstairs : bs:{} es:{} ec:{}",
        target, bs, es, ec
    );

    if versions.len() > 12 {
        println!("{} versions[0..12]: {:?}", target, versions[0..12].to_vec());
    } else {
        println!("{}  versions: {:?}", target, versions);
    }

    let mut fi = u.flush_info.lock().unwrap();
    if fi.flush_numbers.is_empty() {
        /*
         * This is the first version list we have, so
         * we will make it the original and compare
         * whatever comes next.
         */
        fi.flush_numbers = versions;
        fi.next_flush = *fi.flush_numbers.iter().max().unwrap() + 1;
        print!("Set inital Extent versions to");
        if fi.flush_numbers.len() > 12 {
            println!(" [0..12]{:?}", fi.flush_numbers[0..12].to_vec());
        } else {
            println!("{:?}", fi.flush_numbers);
        }
        println!("Next flush: {}", fi.next_flush);
    } else if fi.flush_numbers.len() != versions.len() {
        /*
         * I don't think there is much we can do here, the expected number
         * of flush numbers does not match.  Possibly we have grown one but
         * not the rest of the downstairs?
         */
        panic!(
            "Expected downstairs version \
              len:{:?} does not match new \
              downstairs:{:?}",
            fi.flush_numbers.len(),
            versions.len()
        );
    } else {
        /*
         * We already have a list of versions to compare with.  Make that
         * comparision now against this new list
         */
        let ver_cmp = fi.flush_numbers.iter().eq(versions.iter());
        if !ver_cmp {
            println!(
                "{} MISMATCH expected: {:?} != new: {:?}",
                target, fi.flush_numbers, versions
            );
            // XXX Recovery process should start here
            println!("{} Ignoring this downstairs version info", target);
        }
    }

    /*
     * XXX Here we have another workaround.  We don't know
     * the region info until after we connect to each
     * downstairs, but we share the ARC Upstairs before we
     * know what to expect.  For now I'm using zero as an
     * indication that we don't yet know the valid values
     * and non-zero meaning we have at least one downstairs
     * to compare with.  We might want to consider breaking
     * out the static config info into something different
     * that is updated on initial downstairs setup from the
     * structures we use for work submission.
     *
     * 0 should never be a valid block size
     */
    let mut ddef = u.ddef.lock().unwrap();
    if ddef.block_size() == 0 {
        ddef.set_block_size(bs);
        ddef.set_extent_size(es);
        ddef.set_extent_count(ec);
        println!("Global using: bs:{} es:{} ec:{}", bs, es, ec);
    }

    if ddef.block_size() != bs
        || ddef.extent_size() != es
        || ddef.extent_count() != ec
    {
        // XXX Figure out if we can hande this error.  Possibly not.
        panic!("New downstairs region info mismatch");
    }

    Ok(())
}

/*
 * This function is called when the upstairs task is notified that
 * a downstairs operation has completed.  We add the read buffer to the
 * IOop struct for later processing if required.
 *
 */
#[instrument]
async fn io_completed(
    up: &Arc<Upstairs>,
    ds_id: u64,
    client_id: u8,
    data: Option<Bytes>,
    ds_done_tx: mpsc::Sender<u64>,
) -> Result<()> {
    let mut gw_work_done = false;

    /*
     * We can't call .send with the lock held, so we check to see
     * if we do need to notify the up_ds_listen task that all work
     * is finished for a ds_id.
     */
    {
        let mut work = up.ds_work.lock().unwrap();
        let counts = work.complete(ds_id, client_id, data)?;
        if counts.active == 0 {
            gw_work_done = true;
        }
    }
    if gw_work_done {
        ds_done_tx.send(ds_id).await?
    }

    Ok(())
}

/*
 * This function is called by a worker task after the main task has added
 * work to the hashmap and notified the worker tasks that new work is ready
 * to be serviced.  The worker task will walk the hashmap and build a list
 * of new work that it needs to do.  It will then iterate through those
 * work items and send them over the wire to this tasks waiting downstairs.
 */
#[instrument(skip(fw))]
async fn io_send(
    u: &Arc<Upstairs>,
    fw: &mut FramedWrite<WriteHalf<'_>, CrucibleEncoder>,
    client_id: u8,
    lossy: bool,
) -> Result<()> {
    /*
     * Build ourselves a list of all the jobs on the work hashmap that
     * have the job state for our client id in the IOState::New
     */
    let mut new_work = u.ds_work.lock().unwrap().new_work(client_id);

    /*
     * Now we have a list of all the job IDs that are new for our client id.
     * Walk this list and process each job, marking it InProgress as we
     * do the work.  We do this in two loops because we can't hold the
     * lock for the hashmap while we do work, and if we release the lock
     * to do work, we would have to start over and look at all jobs in the
     * map to see if they are new.
     *
     * This also allows us to sort the job ids and do them in order they
     * were put into the hashmap, though I don't think that is required.
     */
    new_work.sort_unstable();

    for new_id in new_work.iter() {
        /*
         * Walk the list of work to do, update its status as in progress
         * and send the details to our downstairs.
         */
        if lossy && random() && random() {
            continue;
        }

        let job = u.ds_work.lock().unwrap().in_progress(*new_id, client_id);

        match job {
            IOop::Write {
                dependencies,
                eid,
                byte_offset,
                data,
            } => {
                fw.send(Message::Write(
                    *new_id,
                    eid,
                    dependencies.clone(),
                    byte_offset,
                    data.clone(),
                ))
                .await?
            }
            IOop::Flush {
                dependencies,
                flush_number,
            } => {
                fw.send(Message::Flush(
                    *new_id,
                    dependencies.clone(),
                    flush_number,
                ))
                .await?
            }
            IOop::Read {
                dependencies,
                eid,
                byte_offset,
                len,
            } => {
                fw.send(Message::ReadRequest(
                    *new_id,
                    dependencies.clone(),
                    eid,
                    byte_offset,
                    len,
                ))
                .await?
            }
        }
    }
    Ok(())
}

/*
 * Once we have a connection to a downstairs, this task takes over and
 * handles both the initial negotiation and then watches the input for
 * changes, indicating that new work in on the work hashmap.  We will
 * walk the hashmap on the input signal and get any new work for this
 * specific downstairs and mark that job as in progress.
 */
async fn proc(
    target: &SocketAddrV4,
    up: &Arc<Upstairs>,
    mut sock: TcpStream,
    connected: &mut bool,
    mut up_coms: UpComs,
    lossy: bool,
) -> Result<()> {
    let (r, w) = sock.split();
    let mut fr = FramedRead::new(r, CrucibleDecoder::new());
    let mut fw = FramedWrite::new(w, CrucibleEncoder::new());

    /*
     * As the "client", we must begin the negotiation.
     */
    fw.send(Message::HereIAm(1)).await?;

    /*
     * Don't wait more than 5 seconds to hear from the other side.
     * XXX Timeouts, timeouts: always wrong!  Some too short and some too long.
     */
    let mut deadline = deadline_secs(50);
    let mut negotiated = false;

    /*
     * To keep things alive, initiate a ping any time we have been idle for a
     * second.
     */
    let mut pingat = deadline_secs(10);
    let mut needping = false;

    loop {
        /*
         * XXX Just a thought here, could we send so much input that the
         * select would always have input.changed() and starve out the
         * fr.next() select?  Does this select ever work that way?
         */
        tokio::select! {
            _ = sleep_until(deadline) => {
                if !negotiated {
                    bail!("did not negotiate a protocol");
                }
            }
            _ = sleep_until(pingat), if needping => {
                fw.send(Message::Ruok).await?;
                needping = false;
                if lossy {
                    /*
                     * When lossy is set, we don't always send work to a
                     * downstairs when we should.  This means we need to,
                     * every now and then, signal the downstairs task to
                     * check and see if we skipped some work earlier.
                     */
                    io_send(up, &mut fw, up_coms.client_id, lossy).await?;
                }
            }
            _ = up_coms.input.changed() => {
                /*
                let iv = *input.borrow();
                println!("[{}] Input changed with {}", up_coms.client_id, iv);
                 */
                io_send(up, &mut fw, up_coms.client_id, lossy).await?;
            }
            f = fr.next() => {
                /*
                 * Negotiate protocol before we get into specifics.
                 */
                match f.transpose()? {
                    None => {
                        return Ok(())
                    }
                    Some(Message::YesItsMe(version)) => {
                        if negotiated {
                            bail!("negotiated already!");
                        }
                        /*
                         * XXX Valid version to compare with should come
                         * from main task
                         */
                        if version != 1 {
                            bail!("expected version 1, got {}", version);
                        }
                        negotiated = true;
                        needping = true;
                        deadline = deadline_secs(50);

                        /*
                         * Ask for the current version of all extents.
                         */
                        fw.send(Message::ExtentVersionsPlease).await?;
                    }
                    Some(Message::ExtentVersions(bs, es, ec, versions)) => {
                        if !negotiated {
                            bail!("expected YesItsMe first");
                        }
                        process_downstairs(target, up, bs, es, ec, versions)?;

                        /*
                         *  XXX I moved this here for now so we can use this
                         * signal to move forward with I/O.  Eventually this
                         * connected being true state should be sent from the
                         * initial YesItsMe case, and we send something else
                         * through a different watcher that tells the main
                         * task the list of versions, or something like that.
                         */

                        /*
                         * If we get here, we are ready to receive IO
                         */
                        *connected = true;
                        up_coms.output.send(Condition {
                            target: *target,
                            connected: true,
                        }).await
                        .unwrap();
                    }
                    Some(m) => {
                        if !negotiated {
                            bail!("expected YesItsMe first");
                        }
                        proc_frame(up, &m, up_coms.clone()).await?;
                        deadline = deadline_secs(50);
                        pingat = deadline_secs(10);
                        needping = true;
                    }
                }
            }
        }
    }
}

/*
 * Things that allow the various tasks of Upstairs to communicate
 * with each other.
 */
#[derive(Clone)]
struct UpComs {
    client_id: u8,
    input: watch::Receiver<u64>,
    output: mpsc::Sender<Condition>,
    ds_done_tx: mpsc::Sender<u64>,
}

/*
 * This task is responsible for the connection to and traffic between
 * a specific downstairs instance.  In here we handle taking work off of
 * the global work list and performing that work for a specific downstairs.
 */
async fn looper(
    target: SocketAddrV4,
    up: &Arc<Upstairs>,
    up_coms: UpComs,
    lossy: bool,
) {
    let mut firstgo = true;
    let mut connected = false;

    'outer: loop {
        if firstgo {
            firstgo = false;
        } else {
            tokio::time::sleep(Duration::from_secs(1)).await;
        }

        /*
         * Make connection to this downstairs.
         */
        let sock = TcpSocket::new_v4().expect("v4 socket");

        /*
         * Set a connect timeout, and connect to the target:
         */
        println!("{0}[{1}] connecting to {0}", target, up_coms.client_id);
        let deadline = tokio::time::sleep_until(deadline_secs(10));
        tokio::pin!(deadline);
        let tcp = sock.connect(target.into());
        tokio::pin!(tcp);

        let tcp: TcpStream = loop {
            tokio::select! {
                _ = &mut deadline => {
                    println!("connect timeout");
                    continue 'outer;
                }
                tcp = &mut tcp => {
                    match tcp {
                        Ok(tcp) => {
                            println!("{0}[{1}] ok, connected to {0}",
                                target,
                                up_coms.client_id);
                            break tcp;
                        }
                        Err(e) => {
                            println!("{0} connect to {0} failure: {1:?}",
                                target, e);
                            continue 'outer;
                        }
                    }
                }
            }
        };

        /*
         * Once we have a connected downstairs, the proc task takes over and
         * handles negiotation and work processing.
         */
        if let Err(e) =
            proc(&target, up, tcp, &mut connected, up_coms.clone(), lossy).await
        {
            eprintln!("ERROR: {}: proc: {:?}", target, e);
        }

        if connected {
            up_coms
                .output
                .send(Condition {
                    target,
                    connected: false,
                })
                .await
                .unwrap();
            connected = false;
        }
    }
}

/*
 * The structure that tracks downstairs work in progress
 */
#[derive(Debug)]
pub struct Work {
    active: HashMap<u64, DownstairsIO>,
    next_id: u64,
    completed: AllocRingBuffer<u64>,
}

#[derive(Debug, Default)]
pub struct WorkCounts {
    active: u64,
    done: u64,
}

impl Work {
    /**
     * Assign a new downstairs ID.
     */
    fn next_id(&mut self) -> u64 {
        let id = self.next_id;
        self.next_id += 1;
        id
    }

    /**
     * Mark this request as in progress for this client, and return a copy
     * of the details of the request.
     */
    fn in_progress(&mut self, ds_id: u64, client_id: u8) -> IOop {
        let job = self.active.get_mut(&ds_id).unwrap();
        let oldstate = job.state.insert(client_id, IOState::InProgress);
        assert_eq!(oldstate, Some(IOState::New));
        job.work.clone()
    }

    /**
     * Return a list of downstairs request IDs that represent unissued
     * requests for this client.
     */
    fn new_work(&self, client_id: u8) -> Vec<u64> {
        self.active
            .values()
            .filter_map(|job| {
                if let Some(IOState::New) = job.state.get(&client_id) {
                    Some(job.ds_id)
                } else {
                    None
                }
            })
            .collect()
    }

    /**
     * Walk the active hashmap and Return a Vec of downstairs request IDs
     * where all requests have been completed.
     */
    fn completed_work(&mut self) -> Vec<u64> {
        let mut completed = Vec::new();
        let mut kvec = self.active.keys().cloned().collect::<Vec<u64>>();
        kvec.sort_unstable();
        for k in kvec.iter() {
            if self.state_count(*k).unwrap().active == 0 {
                completed.push(*k)
            }
        }
        completed
    }

    /**
     * Enqueue a new downstairs request.
     */
    fn enqueue(&mut self, io: DownstairsIO) {
        self.active.insert(io.ds_id, io);
    }

    /**
     * Collect the state of the jobs from each client.
     */
    fn state_count(&mut self, ds_id: u64) -> Result<WorkCounts> {
        /* XXX Should this support invalid ds_ids? */
        let job = self
            .active
            .get_mut(&ds_id)
            .ok_or_else(|| anyhow!("reqid {} is not active", ds_id))?;

        let mut wc: WorkCounts = Default::default();
        for state in job.state.values() {
            match state {
                IOState::New | IOState::InProgress => wc.active += 1,
                IOState::Done | IOState::Skipped | IOState::Error => {
                    wc.done += 1;
                }
            }
        }
        Ok(wc)
    }

    /**
     * Mark this downstairs request as complete for this client.  Returns
     * counts clients for which this request is still active or has been
     * completed already.
     */
    fn complete(
        &mut self,
        ds_id: u64,
        client_id: u8,
        data: Option<Bytes>,
    ) -> Result<WorkCounts> {
        let job = self
            .active
            .get_mut(&ds_id)
            .ok_or_else(|| anyhow!("reqid {} is not active", ds_id))?;
        let oldstate = job.state.insert(client_id, IOState::Done);
        assert_ne!(oldstate, Some(IOState::Done));

        /*
         * If the data field has a buffer in it, then we attach that buffer
         * (clone really) to the job.data field.  When a read completes it
         * will have a buffer and we keep that buffer around so it can be
         * transferred back to the guest when all IOs that made up that read
         * have returned data.
         */
        if let Some(data) = data {
            if job.data.is_none() {
                job.data = Some(data);
            }
        } // XXX else assert this is not a read

        /*
         * Return the state count for the I/O on this ds_id
         */
        self.state_count(ds_id)
    }

    /**
     * This request is now complete on all peers.  Remove it from the active set
     * and mark it in the completed ring buffer.
     *
     * If there is data in job.data, then we need to transfer that data to the
     * upstairs guest job that started this.
     */
    fn retire(&mut self, ds_id: u64) -> DownstairsIO {
        assert!(!self.completed.contains(&ds_id));
        let old = self.active.remove(&ds_id).unwrap();
        self.completed.push(ds_id);
        old
    }
}

/*
 * XXX Track scheduled storage work in the central structure.  Have the
 * target management task check for work to do here by changing the value in
 * its watch::channel.  Have the main thread determine that an overflow of
 * work to do backing up in here means we need to do something like mark the
 * target as behind or institute some kind of back pressure, etc.
 */
#[derive(Debug)]
pub struct Upstairs {
    /*
     * The guest struct keeps track of jobs accepted from the Guest as they
     * progress through crucible.  A single job submitted can produce
     * multiple downstairs requests.
     */
    guest: Arc<Guest>,
    /*
     * This Work struct keeps track of IO operations going between upstairs
     * and downstairs.  New work for downstairs is generated inside the
     * upstairs on behalf of IO requests coming from the guest.
     */
    ds_work: Mutex<Work>,
    /*
     * The flush info Vec is only used when first connecting or re-connecting
     * to a downstairs.  It is populated with the versions the upstairs
     * considers the "correct".  If a downstairs disconnects and then
     * comes back, it has to match or be made to match what was decided
     * as the correct list.  This may involve having to refresh the versions
     * vec.
     *
     * The versions vec is not enough to solve a mismatch.  We really need
     * Generation number, flush number, and dirty bit for every extent
     * when resolving conflicts.
     *
     * On Startup we determine the highest flush number from all three
     * downstairs.  We add one to that and it becomes the next flush
     * number.  Flush numbers increment by one each time.
     */
    flush_info: Mutex<FlushInfo>,
    /*
     * The global description of the downstairs region we are using.
     * This allows us to verify each downstairs is the same, as well as
     * enables us to tranlate an LBA to an extent and block offset.
     */
    ddef: Mutex<RegionDefinition>,
    /*
     * The state of a downstairs connection, based on client ID
     * Ready here indicates it can receive IO.
     */
    downstairs: Mutex<Vec<DownstairsState>>,
}

impl Upstairs {
    pub fn new(opt: &CrucibleOpts, guest: Arc<Guest>) -> Arc<Upstairs> {
        Arc::new(Upstairs {
            guest,
            ds_work: Mutex::new(Work {
                active: HashMap::new(),
                completed: AllocRingBuffer::with_capacity(2048),
                next_id: 1000,
            }),
            flush_info: Mutex::new(FlushInfo::new()),
            ddef: Mutex::new(RegionDefinition::default()),
            downstairs: Mutex::new(Vec::with_capacity(opt.target.len())),
        })
    }

    /*
     * If we are doing a flush, the flush number and the rn number
     * must both go up together.  We don't want a lower next_id
     * with a higher flush_number to be possible, as that can introduce
     * dependency deadlock.
     * To also avoid any problems, this method should be called only
     * during the submit_flush method so we know the ds_work and
     * guest_work locks are both held.
     */
    fn next_flush_id(&self) -> u64 {
        let mut fi = self.flush_info.lock().unwrap();
        fi.next_flush()
    }

    #[instrument]
    pub fn submit_flush(&self, sender: std_mpsc::Sender<i32>) -> Result<()> {
        /*
         * Lock first the guest_work struct where this new job will go,
         * then lock the ds_work struct.  Once we have both we can proceed
         * to build our flush command.
         */
        let mut gw = self.guest.guest_work.lock().unwrap();
        let mut ds_work = self.ds_work.lock().unwrap();

        /*
         * Get the next ID for our new guest work job.  Note that the flush
         * ID and the next_id are connected here, in that all future writes
         * should be flushed at the next flush ID.
         */
        let gw_id: u64 = gw.next_gw_id();
        let next_id = ds_work.next_id();
        let next_flush = self.next_flush_id();

        /*
         * Walk the downstairs work active list, and pull out all the active
         * jobs.  Anything we have not submitted back to the guest.
         *
         * TODO, we can go faster if we:
         * 1. Ignore everything that was before and including the last flush.
         * 2. Ignore reads.
         */
        let mut dep = ds_work.active.keys().cloned().collect::<Vec<u64>>();
        dep.sort_unstable();
        /*
         * TODO: Walk the list of guest work structs and build the same list
         * and make sure it matches.
         */

        /*
         * Build the flush request, and take note of the request ID that
         * will be assigned to this new piece of work.
         */
        let fl = create_flush(next_id, dep, next_flush, gw_id);

        let mut sub = HashMap::new();
        sub.insert(next_id, 0);

        let new_gtos = GtoS::new(sub, Vec::new(), None, HashMap::new(), sender);
        gw.active.insert(gw_id, new_gtos);
        crutrace_gw_flush_start!(|| (gw_id));

        ds_work.enqueue(fl);
        Ok(())
    }

    /*
     * When we have a guest write request with offset and buffer, take them and
     * build both the upstairs work guest tracking struct as well as the downstairs
     * work struct. Once both are ready, submit them to the required places.
     */
    #[instrument]
    fn submit_write(
        &self,
        offset: u64,
        data: Bytes,
        sender: std_mpsc::Sender<i32>,
    ) {
        /*
         * Get the next ID for the guest work struct we will make at the
         * end.  This ID is also put into the IO struct we create that
         * handles the operation(s) on the storage side.
         */
        let mut gw = self.guest.guest_work.lock().unwrap();
        let mut ds_work = self.ds_work.lock().unwrap();
        let gw_id: u64 = gw.next_gw_id();

        /*
         * Given the offset and buffer size, figure out what extent and
         * byte offset that translates into.  Keep in mind that an offset
         * and length may span two extents, and eventually XXX, two regions.
         */
        let ddef = self.ddef.lock().unwrap();
        let nwo = extent_from_offset(*ddef, offset, data.len()).unwrap();

        /*
         * Now create a downstairs work job for each (eid, bi, len) returned
         * from extent_from_offset
         *
         * Create the list of downstairs request numbers (ds_id) we created
         * on behalf of this guest job.
         */
        let mut sub = HashMap::new();
        let mut new_ds_work = Vec::new();
        let mut next_id: u64;
        let mut cur_offset = 0;

        let mut dep = ds_work.active.keys().cloned().collect::<Vec<u64>>();
        dep.sort_unstable();
        /* Lock here, through both jobs submitted */
        for (eid, bo, len) in nwo {
            {
                next_id = ds_work.next_id();
            }
            /*
             * TODO: This is where encryption will happen, which will probably
             * mean a refactor of how this job is built.
             */
            let sub_data = data.slice(cur_offset..(cur_offset + len));
            sub.insert(next_id, len);

            let wr = create_write_eob(
                next_id,
                dep.clone(),
                gw_id,
                eid,
                bo,
                sub_data,
            );
            new_ds_work.push(wr);
            cur_offset = len;
        }
        /*
         * New work created, add to the guest_work HM
         */
        let new_gtos = GtoS::new(sub, Vec::new(), None, HashMap::new(), sender);
        {
            gw.active.insert(gw_id, new_gtos);
        }
        crutrace_gw_write_start!(|| (gw_id));

        for wr in new_ds_work {
            ds_work.enqueue(wr);
        }
    }

    /*
     * When we have a guest read request with offset and buffer, take them
     * and build both the upstairs work guest tracking struct as well as the
     * downstairs work struct. Once both are ready, submit them to the
     * required places.
     */
    #[instrument]
    pub fn submit_read(
        &self,
        offset: u64,
        data: Buffer,
        sender: std_mpsc::Sender<i32>,
    ) {
        /*
         * Get the next ID for the guest work struct we will make at the
         * end.  This ID is also put into the IO struct we create that
         * handles the operation(s) on the storage side.
         */
        let mut gw = self.guest.guest_work.lock().unwrap();
        let mut ds_work = self.ds_work.lock().unwrap();
        let gw_id: u64 = gw.next_gw_id();

        /*
         * Given the offset and buffer size, figure out what extent and
         * byte offset that translates into. Keep in mind that an offset
         * and length may span many extents, and eventually, TODO, regions.
         */
        let ddef = self.ddef.lock().unwrap();
        let nwo = extent_from_offset(*ddef, offset, data.len()).unwrap();

        /*
         * Create the tracking info for downstairs request numbers (ds_id) we
         * will create on behalf of this guest job.
         */
        let mut sub = HashMap::new();
        let mut new_ds_work = Vec::new();
        let mut next_id: u64;

        /*
         * Now create a downstairs work job for each (eid, bo, len) returned
         * from extent_from_offset
         */
        let mut dep = ds_work.active.keys().cloned().collect::<Vec<u64>>();
        dep.sort_unstable();
        for (eid, bo, len) in nwo {
            {
                next_id = ds_work.next_id();
            }
            /*
             * When multiple operations are needed to satisfy a read, The offset
             * and length will be divided across two downstairs requests.  It is
             * required (for re-assembly on the other side) that the lower offset
             * corresponds to the lower next_id.  The ID's don't need to be
             * sequential.
             */
            sub.insert(next_id, len);
            let wr = create_read_eob(next_id, dep.clone(), gw_id, eid, bo, len);
            new_ds_work.push(wr);
        }

        /*
         * New work created, add to the guest_work HM.  New work must be put
         * on the guest_work active HM first, before it lands on the downstairs
         * lists.  We don't want to miss a completion from downstairs.
         */
        assert!(!sub.is_empty());
        let new_gtos =
            GtoS::new(sub, Vec::new(), Some(data), HashMap::new(), sender);
        {
            gw.active.insert(gw_id, new_gtos);
        }
        crutrace_gw_read_start!(|| (gw_id));

        for wr in new_ds_work {
            ds_work.enqueue(wr);
        }
    }
}

#[derive(Debug)]
struct FlushInfo {
    flush_numbers: Vec<u64>,
    /*
     * The next flush number to use when a Flush is issued.
     */
    next_flush: u64,
}

impl FlushInfo {
    pub fn new() -> FlushInfo {
        FlushInfo {
            flush_numbers: Vec::new(),
            next_flush: 0,
        }
    }
    /*
     * Upstairs flush_info mutex must be held when calling this.
     * In addition, a downstairs request ID should be obtained at the
     * same time the next flush number is obtained, such that any IO that
     * is given a downstairs request number higer than the request number
     * for the flush will happen after this flush, never before.
     */
    fn next_flush(&mut self) -> u64 {
        let id = self.next_flush;
        self.next_flush += 1;
        id
    }
}
/*
 * I think we will have more states.  If not, then this should just become
 * a bool.
 */
#[derive(Debug, Clone)]
enum DownstairsState {
    _NotReady,
    _Ready,
}

/*
 * A unit of work for downstairs that is put into the hashmap.
 */
#[derive(Debug)]
struct DownstairsIO {
    ds_id: u64,    // This MUST match our hashmap index
    guest_id: u64, // The hahsmap ID from the parent guest work.
    work: IOop,
    /*
     * Hash of work status where key is the downstairs "client id" and the
     * hash value is the current state of the IO request with respect to the
     * upstairs.
     * The length and keys on this hashmap will be used to determine which
     * downstairs will receive the IO request.
     * XXX Determine if it is required for all downstairs to get an entry
     * or if by not putting a downstars in the hash, if that is valid.
     */
    state: HashMap<u8, IOState>,
    /*
     * If the operation is a Read, this holds the resulting buffer
     */
    data: Option<Bytes>,
}

/*
 * Crucible to storage IO operations.
 */
#[derive(Debug, Clone)]
pub enum IOop {
    Write {
        dependencies: Vec<u64>, // Jobs that must finish before this
        eid: u64,
        byte_offset: u64,
        data: Bytes,
    },
    Read {
        dependencies: Vec<u64>, // Jobs that must finish before this
        eid: u64,
        byte_offset: u64,
        len: u32, // in bytes
    },
    Flush {
        dependencies: Vec<u64>, // Jobs that must finish before this
        flush_number: u64,
    },
}

/*
 * The various states an IO can be in when it is on the work hashmap.
 * There is a state that is unique to each downstairs task we have and
 * they operate independent of each other.
 *
 * New:         A new IO request.
 * InProgress:  The request has been sent to this tasks downstairs.
 * Done:        The response came back from downstairs.
 * Skipped:     The IO request should be ignored.  This situation could be
 *              A read that only needs one downstairs to answer, or we are
 *              doing recovery and we only want a specific downstairs to
 *              do that work.
 * Error:       The IO returned some error.
 */
#[derive(Debug, Clone, PartialEq)]
pub enum IOState {
    New,
    InProgress,
    Done,
    Skipped,
    Error,
}

impl fmt::Display for IOState {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        match self {
            IOState::New => {
                write!(f, " New")
            }
            IOState::InProgress => {
                write!(f, "Sent")
            }
            IOState::Done => {
                write!(f, "Done")
            }
            IOState::Skipped => {
                write!(f, "Skip")
            }
            IOState::Error => {
                write!(f, " Err")
            }
        }
    }
}

/*
 * Provides a shared Buffer that Read operations will write into.
 *
 * Originally BytesMut was used here, but it didn't guarantee that memory
 * was shared between cloned BytesMut objects.
 */
#[derive(Clone, Debug)]
pub struct Buffer {
    data: Arc<Mutex<Vec<u8>>>,
}

impl Buffer {
    pub fn from_vec(vec: Vec<u8>) -> Buffer {
        Buffer {
            data: Arc::new(Mutex::new(vec)),
        }
    }

    pub fn new(len: usize) -> Buffer {
        Buffer {
            data: Arc::new(Mutex::new(vec![0; len])),
        }
    }

    pub fn from_slice(buf: &[u8]) -> Buffer {
        let mut vec = Vec::<u8>::with_capacity(buf.len());
        for item in buf {
            vec.push(*item);
        }

        Buffer::from_vec(vec)
    }

    pub fn len(&self) -> usize {
        self.data.try_lock().unwrap().len()
    }

    pub fn is_empty(&self) -> bool {
        self.len() == 0
    }

    pub fn as_vec(&self) -> MutexGuard<Vec<u8>> {
        self.data.try_lock().unwrap()
    }
}

#[test]
fn test_buffer_len() {
    const READ_SIZE: usize = 512;
    let data = Buffer::from_slice(&[0x99; READ_SIZE]);
    assert_eq!(data.len(), READ_SIZE);
}

#[test]
fn test_buffer_len_after_clone() {
    const READ_SIZE: usize = 512;
    let data = Buffer::from_slice(&[0x99; READ_SIZE]);
    assert_eq!(data.len(), READ_SIZE);

    let new_buffer = data.clone();
    assert_eq!(new_buffer.len(), READ_SIZE);
}

#[test]
#[should_panic(
    expected = "index out of bounds: the len is 512 but the index is 512"
)]
fn test_buffer_len_index_overflow() {
    const READ_SIZE: usize = 512;
    let data = Buffer::from_slice(&[0x99; READ_SIZE]);
    assert_eq!(data.len(), READ_SIZE);

    let mut vec = data.as_vec();
    assert_eq!(vec.len(), 512);

    for i in 0..(READ_SIZE + 1) {
        vec[i] = 0x99;
    }
}

#[test]
fn test_buffer_len_over_block_size() {
    const READ_SIZE: usize = 600;
    let data = Buffer::from_slice(&[0x99; READ_SIZE]);
    assert_eq!(data.len(), READ_SIZE);
}

/*
 * Inspired from Propolis block.rs
 *
 * The following are the operations that Crucible supports from outside callers.
 * We have extended this to cover a bunch of test operations as well.
 * The first three are the supported operations, the other operations
 * tell the upstairs to behave in specific ways.
 */
#[derive(Debug)]
enum BlockOp {
    Read { offset: u64, data: Buffer },
    Write { offset: u64, data: Bytes },
    Flush,
    // Query ops
    QueryBlockSize { data: Arc<Mutex<u64>> },
    QueryTotalSize { data: Arc<Mutex<u64>> },
    // Begin testing options.
    QueryExtentSize { data: Arc<Mutex<u64>> },
    QueryActive { data: Arc<Mutex<usize>> },
    Commit,   // Send update to all tasks that there is work on the queue.
    ShowWork, // Show the status of the internal work hashmap and done Vec.
}

/*
 * This structure is for tracking the underlying storage side operations
 * that map to a single Guest IO request. G to S stands for Guest
 * to Storage.
 *
 * The submitted hashmap is indexd by the request number (ds_id) for the
 * downstairs requests issued on behalf of this request.
 */
#[derive(Debug)]
struct GtoS {
    /*
     * Jobs we have submitted (or will soon submit) to the storage side
     * of the upstairs process to send on to the downstairs.
     * The key for the hashmap is the ds_id number in the hashmap for
     * downstairs work.  The value is the buffer size of the operation.
     */
    submitted: HashMap<u64, usize>,
    completed: Vec<u64>,

    /*
     * This buffer is provided by the guest request. If this is a read,
     * data will be written here.
     */
    guest_buffer: Option<Buffer>,

    /*
     * When we have an IO between the guest and crucible, it's possible
     * it will be broken into two smaller requests if the range happens
     * to cross an extent boundary.  This hashmap is a list of those
     * buffers with the key being the downstairs request ID.
     *
     * Data moving in/out of this buffer will be encrypted or decrypted
     * depending on the operation.
     */
    downstairs_buffer: HashMap<u64, Bytes>,

    /*
     * Notify the caller waiting on the job to finish.
     */
    sender: std_mpsc::Sender<i32>,
}

impl GtoS {
    pub fn new(
        submitted: HashMap<u64, usize>,
        completed: Vec<u64>,
        guest_buffer: Option<Buffer>,
        downstairs_buffer: HashMap<u64, Bytes>,
        sender: std_mpsc::Sender<i32>,
    ) -> GtoS {
        GtoS {
            submitted,
            completed,
            guest_buffer,
            downstairs_buffer,
            sender,
        }
    }

    /*
     * When all downstairs jobs have completed, and all buffers have been
     * attached to the GtoS struct, we can do the final copy of the data
     * from upstairs memory back to the guest's memory.
     *
     * XXX When encryption/decryption is supported, here is where you will be
     * writing the code to decrypt.
     */
    #[instrument]
    fn transfer(&mut self) {
        if let Some(guest_buffer) = &mut self.guest_buffer {
            self.completed.sort_unstable();
            assert!(!self.completed.is_empty());

            let mut offset = 0;
            for ds_id in self.completed.iter() {
                // println!("Copy buff from {:?}", ds_id);
                let ds_buf = self.downstairs_buffer.remove(ds_id).unwrap();
                {
                    let _ignored =
                        span!(Level::TRACE, "copy to guest buffer").entered();
                    let mut vec = guest_buffer.as_vec();
                    for i in 0..ds_buf.len() {
                        vec[offset] = ds_buf[i];
                        offset += 1;
                    }
                }
                // println!("Final data copy {} {:#?}", ds_id, ds_buf);
            }
        } else {
            /*
             * Should this panic?  If the caller is requesting a transfer,
             * the guest_buffer should exist.  If it does not exist, then
             * either there is a real problem, or the operation was a write
             * or flush and why are we requesting a transfer for those.
             */
            panic!("No guest buffer, no copy");
        }
    }

    /*
     * Notify corresponding BlockReqWaiter
     */
    pub fn notify(&mut self) {
        /*
         * If the guest is no longer listening and this returns an error,
         * do we care?  This could happen if the guest has given up
         * becuase an IO took too long, or other possible guest side reasons.
         */
        if self.sender.send(0).is_ok() {}
    }
}

/*
 * This function just does the match on IOop type and updates the dtrace
 * probe for that operaion finishing.
 */
fn crutrace_work_done(work: IOop, gw_id: u64) {
    match work {
        IOop::Read {
            dependencies: _,
            eid: _,
            byte_offset: _,
            len: _,
        } => {
            crutrace_gw_read_end!(|| (gw_id));
        }
        IOop::Write {
            dependencies: _,
            eid: _,
            byte_offset: _,
            data: _,
        } => {
            crutrace_gw_write_end!(|| (gw_id));
        }
        IOop::Flush {
            dependencies: _,
            flush_number: _,
        } => {
            crutrace_gw_flush_end!(|| (gw_id));
        }
    }
}

/**
 * This structure keeps track of work that Crucible has accepted from the
 * "Guest", aka, Propolis.
 *
 * The active is a hashmap of GtoS structures for all I/Os that are
 * outstanding.  Either just created or in progress operations.  The key
 * for a new job comes from next_gw_id and should always increment.
 *
 * Once we have decided enough downstairs requests are finished, we remove
 * the entry from the active and add the gw_id to the completed vec.
 *
 * TODO: The completed needs to implement some notify back to the Guest, and
 * it should probably be a ring buffer.
 */
#[derive(Debug)]
struct GuestWork {
    active: HashMap<u64, GtoS>,
    next_gw_id: u64,
    completed: AllocRingBuffer<u64>,
}

impl GuestWork {
    fn next_gw_id(&mut self) -> u64 {
        let id = self.next_gw_id;
        self.next_gw_id += 1;
        id
    }

    fn active_count(&mut self) -> usize {
        self.active.len()
    }

    /**
     * Move a GtoS job from the active to completed.
     * It is at this point we can notify the Guest their IO is done
     * and any buffers provided should now have the data in them.
     */
    fn complete(&mut self, gw_id: u64) {
        let gtos_job = self.active.remove(&gw_id).unwrap();
        assert!(gtos_job.submitted.is_empty());
        self.completed.push(gw_id);
    }

    /**
     * When the required number of downstairs completions for a downstairs
     * ds_id have arrived, we call this method on the parent GuestWork
     * that requested them and include the DownstairsIO struct.
     *
     * If this operation was a read, then we attach the read buffer to the
     * GtoS struct for later transfer, if it is not already present.
     *
     * A single GtoS job may have multiple downstairs jobs it created, so
     * we may not be done yet.  When all the downstairs jobs finish, we
     * can move forward with finishing up the guest work operation.
     * This may include moving/decrypting data buffers from completed reads.
     *
     * TODO: Error handling case needs to come through here in a way that
     * won't break if enough of the IO completed to satisfy the upstairs, but
     * will be handled if all the downstairs have returned error.
     */
    #[instrument]
    fn ds_complete(&mut self, done: DownstairsIO) {
        let gw_id = done.guest_id;
        let ds_id = done.ds_id;
        /*
         * A job that already finished and results were sent back to
         * the guest could still have a valid ds_id, but no gw_id for
         * it to report to.
         */
        if let Some(gtos_job) = self.active.get_mut(&gw_id) {
            /*
             * If the ds_id is on the submitted list, then we will take it off
             * and Possibly add the read result buffer to the gtos job
             * structure for later copying.
             *
             * If it's not, then verify our ds_id is already on the completed
             * list, just to catch any problems.
             */
            if gtos_job.submitted.remove(&ds_id).is_some() {
                if let Some(data) = done.data {
                    /*
                     * The first read buffer will become the source for the
                     * final response back to the guest.  This buffer will be
                     * combined with other buffers if the upstairs request
                     * required multiple jobs.
                     */
                    if gtos_job.downstairs_buffer.insert(ds_id, data).is_some()
                    {
                        println!(
                            "gw_id:{} read buffer already present for {}",
                            gw_id, ds_id
                        );
                        /*
                         * This may panic at some future point when only one
                         * read should be enough to satisfy the guest
                         * reuquest, though it may depend on how we make the
                         * different downstairs consistent.  XXX
                         */
                    } else {
                    }
                } // XXX else can we assert we don't expect data?

                gtos_job.completed.push(ds_id);
            } else {
                // XXX Should this just panic?
                println!("gw_id:{} ({}) already removed???", gw_id, ds_id);
                assert!(gtos_job.completed.contains(&ds_id));
            }

            /*
             * If all the downstairs jobs created for this have completed,
             * we can copy (if present) read data back to the guest buffer
             * they provided to us, and notify any waiters.
             */
            if gtos_job.submitted.is_empty() {
                if gtos_job.guest_buffer.is_some() {
                    gtos_job.transfer();
                }
                gtos_job.notify();
                self.complete(gw_id);
                crutrace_work_done(done.work, gw_id);
            }
        } else {
            /*
             * When we support a single successful read, or 2/3 writes
             * or flushes starting the completion back to the guest, this
             * will no longer be a panic, as that can be a valid state.
             */
            panic!(
                "gw_id {} from removed job {} not on active list",
                gw_id, ds_id
            );
        }
    }
}

/**
 * Couple a BlockOp with a notifier for calling code.
 */
#[derive(Debug)]
pub struct BlockReq {
    op: BlockOp,
    send: std_mpsc::Sender<i32>,
}

impl BlockReq {
    // https://docs.rs/tokio/1.9.0/tokio/sync/mpsc/index.html#communicating-between-sync-and-async-code
    // return the std::sync::mpsc Sender to non-tokio task callers
    fn new(op: BlockOp, send: std_mpsc::Sender<i32>) -> BlockReq {
        Self { op, send }
    }
}

/**
 * When BlockOps are sent to a guest, the calling function receives a
 * waiter that it can block on.
 */
pub struct BlockReqWaiter {
    recv: std_mpsc::Receiver<i32>,
}

impl BlockReqWaiter {
    fn new(recv: std_mpsc::Receiver<i32>) -> BlockReqWaiter {
        Self { recv }
    }

    pub fn block_wait(&mut self) {
        // TODO: Instead of i32, errors should be bubbled up.
        let _ = self.recv.recv();
    }
}

/**
 * This is the structure we use to keep track of work passed into crucible
 * from the the "Guest".
 *
 * Requests from the guest are put into the reqs VecDeque initally.
 *
 * A task on the Crucible side will receive a notification that a new
 * operation has landed on the reqs queue and will take action:
 *   Pop the request off the reqs queue.
 *   Copy (TODO: and encrypt) any data buffers provided to us by the Guest.
 *   Create one or more downstairs DownstairsIO structures.
 *   Create a GtoS tracking structure with the id's for each
 *   downstairs task and the read result buffer if required.
 *   Add the GtoS struct to the in GuestWork active work hashmap.
 *   Put all the DownstairsIO strucutres on the downstairs work queue.
 *   Send notification to the upstairs tasks that there is new work.
 *
 * Work here will be added to storage side queues and the responses will
 * be waited on and processed when they arrive.
 *
 * This structure and operations on in handle the translation between
 * outside requests and internal upstairs structures and work queues.
 */
#[derive(Debug)]
pub struct Guest {
    /*
     * New requests from outside go onto this VecDeque.  The notify is how
     * the submittion task tells the listening task that new work has been
     * added.
     */
    reqs: Mutex<VecDeque<BlockReq>>,
    notify: Notify,

    /*
     * When the crucible listening task has noticed a new IO request, it will
     * pull it from the reqs queue and create an GuestWork struct as well as
     * convert the new IO request into the matching downstairs request(s).
     * Each new GuestWork request will get a unique gw_id, which is also
     * the index for that operation into the hashmap.
     *
     * It is during this process that data will encrypted.  For a read, the
     * data is decrypted back to the guest provided buffer after all the
     * required downstairs operations are completed.
     */
    guest_work: Mutex<GuestWork>,
}

/*
 * These methods are how to add or checking for new work on the Guest struct
 */
impl Guest {
    pub fn new() -> Guest {
        Guest {
            /*
             * Incoming I/O requests are added to this queue.
             */
            reqs: Mutex::new(VecDeque::new()),
            notify: Notify::new(),
            /*
             * The active hashmap is for in-flight I/O operations
             * that we have taken off the incoming queue, but we have not
             * received the response from downstairs.
             * Note that a single IO from outside may have multiple I/O
             * requests that need to finish before we can complete that IO.
             */
            guest_work: Mutex::new(GuestWork {
                active: HashMap::new(), // GtoS
                next_gw_id: 1,
                completed: AllocRingBuffer::with_capacity(2048),
            }),
        }
    }

    /*
     * This is used to submit a new BlockOp IO request to Crucible.
     */
    fn send(&self, op: BlockOp) -> BlockReqWaiter {
        let (send, recv) = std_mpsc::channel();

        self.reqs.lock().unwrap().push_back(BlockReq::new(op, send));
        self.notify.notify_one();

        BlockReqWaiter::new(recv)
    }

    /*
     * A crucible task will listen for new work using this.
     */
    async fn recv(&self) -> BlockReq {
        loop {
            if let Some(req) = self.reqs.lock().unwrap().pop_front() {
                return req;
            }
            self.notify.notified().await;
        }
    }

    // TODO: get status from waiter, bubble that up as a Result?

    pub fn read(&self, offset: u64, data: Buffer) -> BlockReqWaiter {
        let rio = BlockOp::Read { offset, data };
        self.send(rio)
    }

    pub fn write(&self, offset: u64, data: Bytes) -> BlockReqWaiter {
        let wio = BlockOp::Write { offset, data };
        self.send(wio)
    }

    pub fn flush(&self) -> BlockReqWaiter {
        self.send(BlockOp::Flush)
    }

    pub fn query_block_size(&self) -> u64 {
        let data = Arc::new(Mutex::new(0));
        let size_query = BlockOp::QueryBlockSize { data: data.clone() };
        self.send(size_query).block_wait();
        return *data.lock().unwrap();
    }

    pub fn query_total_size(&self) -> u64 {
        let data = Arc::new(Mutex::new(0));
        let size_query = BlockOp::QueryTotalSize { data: data.clone() };
        self.send(size_query).block_wait();
        return *data.lock().unwrap();
    }

    pub fn query_extent_size(&self) -> u64 {
        let data = Arc::new(Mutex::new(0));
        let extent_query = BlockOp::QueryExtentSize { data: data.clone() };
        self.send(extent_query).block_wait();
        return *data.lock().unwrap();
    }

    pub fn query_active(&self) -> usize {
        let data = Arc::new(Mutex::new(0));
        let active_query = BlockOp::QueryActive { data: data.clone() };
        self.send(active_query).block_wait();
        return *data.lock().unwrap();
    }

    pub fn commit(&self) {
        self.send(BlockOp::Commit).block_wait();
    }

    pub fn show_work(&self) {
        self.send(BlockOp::ShowWork).block_wait();
    }
}

impl Default for Guest {
    fn default() -> Self {
        Self::new()
    }
}

pub struct Target {
    #[allow(dead_code)]
    target: SocketAddrV4,
    input: watch::Sender<u64>,
}

#[derive(Debug)]
struct Condition {
    target: SocketAddrV4,
    connected: bool,
}

/*
 * Send work to all the targets on this vector.
 * This can be much simpler, but we need to (eventually) take special action
 * when we fail to send a message to a task.
 */
fn _send_work(t: &[Target], val: u64) {
    for d_client in t.iter() {
        // println!("#### send to client {:?}", d_client.target);
        let res = d_client.input.send(val);
        if let Err(e) = res {
            panic!("#### error {:#?} sending work to {:?}", e, d_client.target);
            /*
             * TODO Write more code for this error,  If one downstairs
             * never receives a request, it may get picked up on the
             * next request.  However, if the downstairs has gone away,
             * then action will need to be taken, and soon.
             */
        }
    }
}

/**
 * We listen on the ds_done channel to know when all the downstairs requests
 * for a downstairs work task have finished and it is time to complete
 * any buffer transfers (reads) and then notify the guest that their
 * work has been completed.
 */
async fn up_ds_listen(up: &Arc<Upstairs>, mut ds_done_rx: mpsc::Receiver<u64>) {
    while let Some(_ds_id) = ds_done_rx.recv().await {
        /*
         * XXX Do we need to hold the lock while we process all the
         * completed jobs?  We should be continuing to send message over
         * the ds_done_tx channel, so if new things show up while we
         * process the set of things we know are done now, then the
         * ds_done_rx.recv() should trigger when we loop.
         */
        let done_list = up.ds_work.lock().unwrap().completed_work();
        // println!( "rcv:{} Done List: {:?}", ds_id, done_list);

        for ds_id_done in done_list.iter() {
            let mut work = up.ds_work.lock().unwrap();

            /*
             * TODO: retire means the downstairs is "consistent" with
             * regards to this IO, so any internal info about this operation
             * should now consider it as ack'd to the guest.
             */
            let done = work.retire(*ds_id_done);

            drop(work);

            let mut gw = up.guest.guest_work.lock().unwrap();
            gw.ds_complete(done);
        }
    }
}
/*
 * This task will loop forever and watch the Guest structure for new IO
 * operations showing up.  When one is detected, the type is checked and the
 * operation is translated into the corresponding upstairs IO type and put on
 * the internal upstairs queue.
 */
async fn up_listen(up: &Arc<Upstairs>, dst: Vec<Target>) {
    /*
     * XXX Once we move this function to being called after all downstairs are
     * online, we can remove this sleep
     */
    tokio::time::sleep(Duration::from_secs(1)).await;

    let mut lastcast = 1;
    loop {
        let req = up.guest.recv().await;
        match req.op {
            BlockOp::Read { offset, data } => {
                up.submit_read(offset, data, req.send);
                dst.iter().for_each(|t| t.input.send(lastcast).unwrap());
                lastcast += 1;
            }
            BlockOp::Write { offset, data } => {
                up.submit_write(offset, data, req.send);
                dst.iter().for_each(|t| t.input.send(lastcast).unwrap());
                lastcast += 1;
            }
            BlockOp::Flush => {
                up.submit_flush(req.send).unwrap();
                dst.iter().for_each(|t| t.input.send(lastcast).unwrap());
                lastcast += 1;
            }
            // Query ops
            BlockOp::QueryBlockSize { data } => {
                *data.lock().unwrap() = up.ddef.lock().unwrap().block_size();
                let _ = req.send.send(0);
            }
            BlockOp::QueryTotalSize { data } => {
                *data.lock().unwrap() = up.ddef.lock().unwrap().total_size();
                let _ = req.send.send(0);
            }
            // Testing options
            BlockOp::QueryExtentSize { data } => {
                // Yes, test only
                *data.lock().unwrap() = up.ddef.lock().unwrap().extent_size();
                let _ = req.send.send(0);
            }
            BlockOp::QueryActive { data } => {
                *data.lock().unwrap() =
                    up.guest.guest_work.lock().unwrap().active_count();
                let _ = req.send.send(0);
            }
            BlockOp::ShowWork => {
                show_all_work(up);
            }
            BlockOp::Commit => {
                dst.iter().for_each(|t| t.input.send(lastcast).unwrap());
                lastcast += 1;
            }
        }
    }
}

/*
 * This is the main upstairs task that starts all the other async
 * tasks.
 * XXX At the moment, this function is only half complete, and will
 * probably need a re-write.
 */
pub async fn up_main(opt: CrucibleOpts, guest: Arc<Guest>) -> Result<()> {
    match register_probes() {
        Ok(()) => {
            println!("DTrace probes registered ok");
        }
        Err(e) => {
            println!("Error registering DTrace probes: {:?}", e);
        }
    }

    let lossy = opt.lossy;
    /*
     * Build the Upstairs struct that we use to share data between
     * the different async tasks
     */
    let up = Upstairs::new(&opt, guest);

    /*
     * Use this channel to receive updates on target status from each task
     * we create to connect to a downstairs.
     */
    let (ctx, mut crx) = mpsc::channel::<Condition>(32);

    /*
     * Use this channel to indicate in the upstairs that all downstairs
     * operations for a specific request have completed.
     */
    let (ds_done_tx, ds_done_rx) = mpsc::channel(100);

    /*
     * spawn a task to listen for ds completed work which will then
     * take care of transitioning guest work structs to done.
     */
    let upc = Arc::clone(&up);
    tokio::spawn(async move {
        up_ds_listen(&upc, ds_done_rx).await;
    });

    let mut client_id = 0;
    /*
     * Create one downstairs task (dst) for each target in the opt
     * structure that was passed to us.
     */
    let dst = opt
        .target
        .iter()
        .map(|dst| {
            /*
             * Create the channel that we will use to request that the loop
             * check for work to do in the central structure.
             */
            let (itx, irx) = watch::channel(100); // XXX 100?

            let up = Arc::clone(&up);
            let ctx = ctx.clone();
            let t0 = *dst;
            let ds_done_tx = ds_done_tx.clone();
            let up_coms = UpComs {
                client_id,
                input: irx,
                output: ctx,
                ds_done_tx,
            };
            tokio::spawn(async move {
                looper(t0, &up, up_coms, lossy).await;
            });
            client_id += 1;

            Target {
                target: *dst,
                input: itx,
            }
        })
        .collect::<Vec<_>>();

    /*
     * Create a task to listen for work from outside.
     *
     * The role of this task is to move work between the outside
     * work queue and the internal Upstairs work queue, as well as send
     * completion messages and/or copy data back to the outside.
     *
     * XXX This needs a little more work.  We should not start to listen
     * to the outside until we know that all our downstairs are ready to
     * take IO operations.
     */
    let upl = Arc::clone(&up);
    tokio::spawn(async move {
        up_listen(&upl, dst).await;
    });

    // async tasks need to tell us they are alive, but they also need to
    // tell us the extent list from any attached downstairs.
    // That part is not connected yet. XXX
    let mut ds_count = 0u32;
    loop {
        let c = crx.recv().await.unwrap();
        if c.connected {
            ds_count += 1;
            println!(
                "#### {:?} #### CONNECTED ######## {}/{}",
                c.target,
                ds_count,
                opt.target.len()
            );
        } else {
            println!("#### {:?} #### DISCONNECTED! ####", c.target);
            ds_count -= 1;
        }
        /*
         * We need some additional way to indicate that this upstairs is ready
         * to receive work.  Just connecting to n downstairs is not enough,
         * we need to also know that they all have the same data.
         */
    }
}

/*
 * Create a write DownstairsIO structure from an EID, and offset, and
 * the data buffer
 */
fn create_write_eob(
    ds_id: u64,
    dependencies: Vec<u64>,
    gw_id: u64,
    eid: u64,
    byte_offset: u64,
    data: Bytes,
) -> DownstairsIO {
    /*
     * Note to self:  Should the dependency list cover everything since
     * the last flush, or everything that is currently outstanding?
     */
    let awrite = IOop::Write {
        dependencies,
        eid,
        byte_offset,
        data,
    };

    let mut state = HashMap::new();
    for cl in 0..3 {
        state.insert(cl, IOState::New);
    }

    DownstairsIO {
        ds_id,
        guest_id: gw_id,
        work: awrite,
        state,
        data: None,
    }
}

/*
 * Create a write DownstairsIO structure from an EID, and offset, and the
 * data buffer.  Used for converting a guest IO read request into a
 * DownstairsIO that the downstairs can understand.
 */
fn create_read_eob(
    ds_id: u64,
    dependencies: Vec<u64>,
    gw_id: u64,
    eid: u64,
    byte_offset: u64,
    len: usize,
) -> DownstairsIO {
    let aread = IOop::Read {
        dependencies,
        eid,
        byte_offset,
        len: len as u32,
    };

    let mut state = HashMap::new();
    for cl in 0..3 {
        state.insert(cl, IOState::New);
    }

    DownstairsIO {
        ds_id,
        guest_id: gw_id,
        work: aread,
        state,
        data: None,
    }
}

/*
 * Create a flush DownstairsIO structure.
 */
fn create_flush(
    ds_id: u64,
    dependencies: Vec<u64>,
    flush_number: u64,
    guest_id: u64,
) -> DownstairsIO {
    let flush = IOop::Flush {
        dependencies,
        flush_number,
    };

    let mut state = HashMap::new();
    for cl in 0..3 {
        state.insert(cl, IOState::New);
    }
    DownstairsIO {
        ds_id,
        guest_id,
        work: flush,
        state,
        data: None,
    }
}

/*
 * Debug function to display the work hashmap with status for all three of
 * the clients.
 */
#[allow(unused_variables)]
fn show_all_work(up: &Arc<Upstairs>) {
    let work = up.ds_work.lock().unwrap();
    let mut kvec: Vec<u64> = work.active.keys().cloned().collect::<Vec<u64>>();
    if kvec.is_empty() {
        println!("# Crucible Downstairs work queue -> Empty #");
    } else {
        println!("# Crucible Downstairs work queue #");
        kvec.sort_unstable();
        for id in kvec.iter() {
            let job = work.active.get(id).unwrap();
            let job_type = match &job.work {
                IOop::Read {
                    dependencies,
                    eid,
                    byte_offset,
                    len,
                } => "Read ".to_string(),
                IOop::Write {
                    dependencies,
                    eid,
                    byte_offset,
                    data,
                } => "Write".to_string(),
                IOop::Flush {
                    dependencies,
                    flush_number,
                } => "Flush".to_string(),
            };
            print!("JOB:[{:04}] {} ", id, job_type);
            for cid in 0..3 {
                let state = job.state.get(&cid);
                match state {
                    Some(state) => {
                        print!("[{}] state: {}  ", cid, state);
                    }
                    x => {
                        print!("[{}] unknown state:{:#?}", cid, x);
                    }
                }
            }
            println!();
        }
    }
    let done = work.completed.to_vec();
    println!("Done tasks count: {:?}", done.len());
    drop(work);
    show_guest_work(&up.guest);
}

/*
 * Debug function to dump the guest work structure.
 * This does a bit while holding the mutex, so don't expect performance
 * to get better when calling it.
 *
 * TODO: make this one big dump, where we include the up.work.active
 * printing for each guest_work.  It will be much more dense, but require
 * holding both locks for the duration.
 */
fn show_guest_work(guest: &Arc<Guest>) {
    println!("Guest work:  Active and Completed Jobs:");
    let gw = guest.guest_work.lock().unwrap();
    let mut kvec: Vec<u64> = gw.active.keys().cloned().collect::<Vec<u64>>();
    kvec.sort_unstable();
    for id in kvec.iter() {
        let job = gw.active.get(id).unwrap();
        println!(
            "GW_JOB active:[{:04}] S:{:?} C:{:?} ",
            id, job.submitted, job.completed
        );
    }
    let done = gw.completed.to_vec();
    println!("GW_JOB completed count:{:?} ", done.len());
}

/*
 * Wrap a Crucible guest and implement Read + Write + Seek traits.
 */
pub struct CruciblePseudoFile {
    guest: Arc<Guest>,
    offset: u64,
    sz: u64,
}

impl CruciblePseudoFile {
    pub fn from_guest(guest: Arc<Guest>) -> Self {
        let sz = guest.query_total_size() as u64;
        CruciblePseudoFile {
            guest,
            offset: 0,
            sz,
        }
    }

    pub fn sz(&self) -> u64 {
        self.sz
    }
}

/*
 * The Read + Write impls here translate arbitrary sized operations into
 * calls for the underlying Crucible API.
 */
impl Read for CruciblePseudoFile {
    fn read(&mut self, buf: &mut [u8]) -> IOResult<usize> {
        let data = Buffer::from_slice(buf);

        let mut waiter = self.guest.read(self.offset, data.clone());
        waiter.block_wait();

        // TODO: for block devices, we can't increment offset past the
        // device size but we're supposed to be pretending to be a proper
        // file here
        self.offset += buf.len() as u64;

        // TODO: is there a better way to do this fill?
        {
            let vec = data.as_vec();
            for i in 0..buf.len() {
                buf[i] = vec[i];
            }
        }

        Ok(buf.len())
    }
}

impl Write for CruciblePseudoFile {
    fn write(&mut self, buf: &[u8]) -> IOResult<usize> {
        let mut data = BytesMut::with_capacity(buf.len());
        data.put_slice(buf);

        let mut waiter = self.guest.write(self.offset, data.freeze());
        waiter.block_wait();

        // TODO: can't increment offset past the device size
        self.offset += buf.len() as u64;

        Ok(buf.len())
    }

    fn flush(&mut self) -> IOResult<()> {
        let mut waiter = self.guest.flush();
        waiter.block_wait();

        Ok(())
    }
}

impl Seek for CruciblePseudoFile {
    fn seek(&mut self, pos: SeekFrom) -> IOResult<u64> {
        // TODO: does not check against block device size
        let mut offset: i64 = self.offset as i64;
        match pos {
            SeekFrom::Start(v) => {
                offset = v as i64;
            }
            SeekFrom::Current(v) => {
                offset += v;
            }
            SeekFrom::End(v) => {
                offset = self.sz as i64 + v;
            }
        }

        if offset < 0 {
            Err(std::io::Error::new(
                std::io::ErrorKind::Other,
                "offset is negative!",
            ))
        } else {
            // offset >= 0
            self.offset = offset as u64;
            Ok(self.offset)
        }
    }

    fn stream_position(&mut self) -> IOResult<u64> {
        self.seek(SeekFrom::Current(0))
    }
}

#[cfg(test)]
mod test {
    use super::*;

    #[test]
    fn test_extent_from_offset() {
        let mut ddef = RegionDefinition::default();
        ddef.set_block_size(512);
        ddef.set_extent_size(2);
        ddef.set_extent_count(10);

        // Test less than block size
        assert_eq!(
            extent_from_offset(ddef, 0, 128).unwrap(),
            vec![(0, 0, 128),]
        );

        // Test block size, less than extent size
        assert_eq!(
            extent_from_offset(ddef, 0, 512).unwrap(),
            vec![(0, 0, 512),]
        );

        // Test greater than block size, less than extent size
        assert_eq!(
            extent_from_offset(ddef, 0, 1024).unwrap(),
            vec![(0, 0, 1024),]
        );

        // Test greater than extent size
        assert_eq!(
            extent_from_offset(ddef, 0, 2048).unwrap(),
            vec![(0, 0, 1024), (1, 0, 1024),]
        );

        // Test offsets
        assert_eq!(
            extent_from_offset(ddef, 1, 2048).unwrap(),
            vec![(0, 1, 1024 - 1), (1, 0, 1024), (2, 0, 1),]
        );
        assert_eq!(
            extent_from_offset(ddef, 1023, 2048).unwrap(),
            vec![(0, 1023, 1024 - 1023), (1, 0, 1024), (2, 0, 1023),]
        );
        assert_eq!(
            extent_from_offset(ddef, 1024, 2048).unwrap(),
            vec![(1, 0, 1024), (2, 0, 1024),]
        );
        for i in 0..9 {
            for j in 1..1023 {
                assert_eq!(
                    extent_from_offset(ddef, 1024 * i + j, 1).unwrap(),
                    vec![(i, j, 1),]
                );
            }
        }

        // Test large sizes plus offset
        assert_eq!(
            extent_from_offset(ddef, 1024 + 780, 1024 * 8).unwrap(),
            vec![
                (1, 780, 1024 - 780),
                (2, 0, 1024),
                (3, 0, 1024),
                (4, 0, 1024),
                (5, 0, 1024),
                (6, 0, 1024),
                (7, 0, 1024),
                (8, 0, 1024),
                (9, 0, 780),
            ]
        );
    }

    #[test]
    fn test_extent_from_offset_hammer_fail() {
        let mut ddef = RegionDefinition::default();
        ddef.set_block_size(512);
        ddef.set_extent_size(100);
        ddef.set_extent_count(1000);

        // Test hammer fails

        // NBD reads two blocks in, writes out one big block
        /*
        NBD_CMD_READ typ=0 handle=144115188831223808 off=17657856 len=4096

        >>> 17657856.0 / (512*100)
        344.88
        >>> 17657856 - 344 * (512*100)
        45056
        >>> 345 * (512*100) - 17657856
        6144
        */
        assert_eq!(
            extent_from_offset(ddef, 17657856, 4096).unwrap(),
            vec![(344, 45056, 4096),],
        );
        /*
        NBD_CMD_READ typ=0 handle=216172782869151744 off=17661952 len=4096

        >>> 17661952 / (512*100)
        344.96
        >>> 17661952 - 344 * (512*100)
        49152
        >>> 345 * (512*100) - 17661952
        2048
        */
        assert_eq!(
            extent_from_offset(ddef, 17661952, 4096).unwrap(),
            vec![(344, 49152, 2048), (345, 0, 2048),],
        );
        /*
        NBD_CMD_WRITE typ=1 handle=288230376990965760 off=17657856 len=8192

        >>> 17657856 / (512*100)
        344.88
        >>> 17657856 - 344 * (512*100)
        45056
        >>> 345 * (512*100) - 17657856
        6144
        >>> 8192 - 6144
        2048
        */
        assert_eq!(
            extent_from_offset(ddef, 17657856, 8192).unwrap(),
            vec![(344, 45056, 6144), (345, 0, 2048),],
        );
    }

    /*
     * Beware, if you change these defaults, then you will have to change
     * all the hard coded tests below that use make_upstairs().
     */
    fn make_upstairs() -> Arc<Upstairs> {
        let mut def = RegionDefinition::default();
        def.set_block_size(512);
        def.set_extent_size(100);
        def.set_extent_count(10);

        Arc::new(Upstairs {
            ds_work: Mutex::new(Work {
                active: HashMap::new(),
                completed: AllocRingBuffer::with_capacity(2),
                next_id: 1000,
            }),
            flush_info: Mutex::new(FlushInfo::new()),
            ddef: Mutex::new(def),
            downstairs: Mutex::new(Vec::with_capacity(1)),
            guest: Arc::new(Guest::new()),
        })
    }

    /*
     * Terrible wrapper, but it allows us to call extent_from_offset()
     * just like the program does.
     */
    fn up_efo(
        up: &Arc<Upstairs>,
        offset: u64,
        len: usize,
    ) -> Result<Vec<(u64, u64, usize)>> {
        let ddef = up.ddef.lock().unwrap();
        extent_from_offset(*ddef, offset, len)
    }

    #[test]
    fn off_to_extent_basic() {
        /*
         * Verify the offsets match the expected byte_offset for the
         * default size region.
         */
        let up = make_upstairs();

        let exv = vec![(0, 0 * 512, 512)];
        assert_eq!(up_efo(&up, 0, 512).unwrap(), exv);
        let exv = vec![(0, 1 * 512, 512)];
        assert_eq!(up_efo(&up, 512, 512).unwrap(), exv);
        let exv = vec![(0, 2 * 512, 512)];
        assert_eq!(up_efo(&up, 1024, 512).unwrap(), exv);
        let exv = vec![(0, 3 * 512, 512)];
        assert_eq!(up_efo(&up, 1024 + 512, 512).unwrap(), exv);
        let exv = vec![(0, 99 * 512, 512)];
        assert_eq!(up_efo(&up, 51200 - 512, 512).unwrap(), exv);

        let exv = vec![(1, 0 * 512, 512)];
        assert_eq!(up_efo(&up, 51200, 512).unwrap(), exv);
        let exv = vec![(1, 1 * 512, 512)];
        assert_eq!(up_efo(&up, 51200 + 512, 512).unwrap(), exv);
        let exv = vec![(1, 2 * 512, 512)];
        assert_eq!(up_efo(&up, 51200 + 1024, 512).unwrap(), exv);
        let exv = vec![(1, 99 * 512, 512)];
        assert_eq!(up_efo(&up, 102400 - 512, 512).unwrap(), exv);

        let exv = vec![(2, 0 * 512, 512)];
        assert_eq!(up_efo(&up, 102400, 512).unwrap(), exv);

        let exv = vec![(9, 99 * 512, 512)];
        assert_eq!(up_efo(&up, (512 * 100 * 10) - 512, 512).unwrap(), exv);
    }

    #[test]
    fn off_to_extent_buffer() {
        /*
         * Testing a buffer size larger than the default 512
         */
        let up = make_upstairs();

        let exv = vec![(0, 0 * 512, 1024)];
        assert_eq!(up_efo(&up, 0, 1024).unwrap(), exv);
        let exv = vec![(0, 1 * 512, 1024)];
        assert_eq!(up_efo(&up, 512, 1024).unwrap(), exv);
        let exv = vec![(0, 2 * 512, 1024)];
        assert_eq!(up_efo(&up, 1024, 1024).unwrap(), exv);
        let exv = vec![(0, 98 * 512, 1024)];
        assert_eq!(up_efo(&up, 51200 - 1024, 1024).unwrap(), exv);

        let exv = vec![(1, 0 * 512, 1024)];
        assert_eq!(up_efo(&up, 51200, 1024).unwrap(), exv);
        let exv = vec![(1, 1 * 512, 1024)];
        assert_eq!(up_efo(&up, 51200 + 512, 1024).unwrap(), exv);
        let exv = vec![(1, 2 * 512, 1024)];
        assert_eq!(up_efo(&up, 51200 + 1024, 1024).unwrap(), exv);
        let exv = vec![(1, 98 * 512, 1024)];
        assert_eq!(up_efo(&up, 102400 - 1024, 1024).unwrap(), exv);

        let exv = vec![(2, 0 * 512, 1024)];
        assert_eq!(up_efo(&up, 102400, 1024).unwrap(), exv);

        let exv = vec![(9, 98 * 512, 1024)];
        assert_eq!(up_efo(&up, (512 * 100 * 10) - 1024, 1024).unwrap(), exv);
    }

    #[test]
    fn off_to_extent_vbuff() {
        let up = make_upstairs();

        /*
         * Walk the buffer sizes from 512 to the whole extent, make sure
         * it all works as expected
         */
        for bsize in (512..=51200).step_by(512) {
            let exv = vec![(0, 0, bsize)];
            assert_eq!(up_efo(&up, 0, bsize).unwrap(), exv);
        }
    }

    #[test]
    fn off_to_extent_bridge() {
        /*
         * Testing when our buffer crosses extents.
         */
        let up = make_upstairs();
        /*
         * 1024 buffer
         */
        let exv = vec![(0, 99 * 512, 512), (1, 0, 512)];
        assert_eq!(up_efo(&up, 51200 - 512, 1024).unwrap(), exv);
        let exv = vec![(0, 98 * 512, 1024), (1, 0, 1024)];
        assert_eq!(up_efo(&up, 51200 - 1024, 2048).unwrap(), exv);

        /*
         * Largest buffer
         */
        let exv = vec![(0, 1 * 512, 51200 - 512), (1, 0, 512)];
        assert_eq!(up_efo(&up, 512, 51200).unwrap(), exv);
        let exv = vec![(0, 2 * 512, 51200 - 1024), (1, 0, 1024)];
        assert_eq!(up_efo(&up, 1024, 51200).unwrap(), exv);
        let exv = vec![(0, 4 * 512, 51200 - 2048), (1, 0, 2048)];
        assert_eq!(up_efo(&up, 2048, 51200).unwrap(), exv);

        /*
         * Largest buffer, last block offset possible
         */
        let exv = vec![(0, 99 * 512, 512), (1, 0, 51200 - 512)];
        assert_eq!(up_efo(&up, 51200 - 512, 51200).unwrap(), exv);
    }

    /*
     * Testing various invalid inputs
     */
    #[test]
    #[should_panic]
    fn off_to_extent_length_zero() {
        let up = make_upstairs();
        up_efo(&up, 0, 0).unwrap();
    }
    #[test]
    fn off_to_extent_length_almost_too_big() {
        let up = make_upstairs();
        up_efo(&up, 0, 512 * 100 * 10).unwrap();
    }
    #[test]
    #[should_panic]
    fn off_to_extent_length_too_big() {
        let up = make_upstairs();
        up_efo(&up, 0, 512 * 100 * 10 + 1).unwrap();
    }
    #[test]
    fn off_to_extent_length_and_offset_almost_too_big() {
        let up = make_upstairs();
        up_efo(&up, 512 * 100 * 9, 512 * 100).unwrap();
    }
    #[test]
    #[should_panic]
    fn off_to_extent_length_and_offset_too_big() {
        let up = make_upstairs();
        up_efo(&up, 512 * 100 * 9, 512 * 100 + 1).unwrap();
    }
}
