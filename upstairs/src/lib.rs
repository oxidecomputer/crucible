// Copyright 2021 Oxide Computer Company
#![feature(asm)]
use std::clone::Clone;
use std::collections::{HashMap, VecDeque};
use std::fmt;
use std::fmt::{Debug, Formatter};
use std::io::{Read, Result as IOResult, Seek, SeekFrom, Write};
use std::net::SocketAddrV4;
use std::sync::mpsc as std_mpsc;
use std::sync::{Arc, Mutex, MutexGuard, RwLock};
use std::time::Duration;

use crucible_common::*;
use crucible_protocol::*;

use anyhow::{anyhow, bail, Result};
use bytes::{Bytes, BytesMut};
use futures::{SinkExt, StreamExt};
use rand::prelude::*;
use ringbuffer::{AllocRingBuffer, RingBufferExt, RingBufferWrite};
use tokio::net::tcp::WriteHalf;
use tokio::net::{TcpSocket, TcpStream};
use tokio::sync::{mpsc, watch, Notify};
use tokio::time::{sleep_until, Instant};
use tokio_util::codec::{FramedRead, FramedWrite};
use tracing::{instrument, span, Level};
use usdt::register_probes;

use aes::cipher::generic_array::GenericArray;
use aes::{Aes128, NewBlockCipher};
use xts_mode::{get_tweak_default, Xts128};

// Include the Rust implementation generated by the build script.
include!(concat!(env!("OUT_DIR"), "/crutrace.rs"));

#[derive(Debug)]
pub struct CrucibleOpts {
    pub target: Vec<SocketAddrV4>,
    pub lossy: bool,
    pub key: Option<String>,
}

impl CrucibleOpts {
    pub fn key_bytes(&self) -> Option<Vec<u8>> {
        if let Some(key) = &self.key {
            // For xts, key size must be 32 bytes
            let decoded_key =
                base64::decode(key).expect("could not base64 decode key!");

            if decoded_key.len() != 32 {
                panic!("Key length must be 32 bytes!");
            }

            Some(decoded_key)
        } else {
            None
        }
    }
}

pub fn deadline_secs(secs: u64) -> Instant {
    Instant::now()
        .checked_add(Duration::from_secs(secs))
        .unwrap()
}

async fn proc_frame(
    u: &Arc<Upstairs>,
    m: &Message,
    up_coms: UpComs,
) -> Result<()> {
    match m {
        Message::Imok => Ok(()),
        Message::WriteAck(ds_id, result) => Ok(io_completed(
            u,
            *ds_id,
            up_coms.client_id,
            None,
            up_coms.ds_done_tx,
            result.clone(),
        )
        .await?),
        Message::FlushAck(ds_id, result) => Ok(io_completed(
            u,
            *ds_id,
            up_coms.client_id,
            None,
            up_coms.ds_done_tx,
            result.clone(),
        )
        .await?),
        Message::ReadResponse(ds_id, data, result) => Ok(io_completed(
            u,
            *ds_id,
            up_coms.client_id,
            Some(data.clone()),
            up_coms.ds_done_tx,
            result.clone(),
        )
        .await?),
        x => bail!("client {} unexpected frame {:?}", up_coms.client_id, x),
    }
}

/*
 * Convert a virtual block offset and length into a Vec of tuples:
 *
 *     Extent number (EID), Block offset, Length in Blocks
 *
 * - length in blocks can be up to the region size
 */
pub fn extent_from_offset(
    ddef: RegionDefinition,
    offset: Block,
    num_blocks: u64,
) -> Result<Vec<(u64, Block, u64)>> {
    assert!(num_blocks > 0);
    assert!(
        (offset.value + num_blocks)
            <= (ddef.extent_size().value * ddef.extent_count() as u64)
    );
    assert_eq!(offset.block_size_in_bytes() as u64, ddef.block_size());

    /*
     *
     *  |eid0                  |eid1
     *  |────────────────────────────────────────────>│
     *  ┌──────────────────────|──────────────────────┐
     *  │                      |                      │
     *  └──────────────────────|──────────────────────┘
     *  |offset                                       |offset + len
     */
    let mut result = Vec::new();
    let mut o: u64 = offset.value;
    let mut blocks_left: u64 = num_blocks;

    while blocks_left > 0 {
        /*
         * XXX We only support a single region (downstairs).  When we grow to
         * support a LBA size that is larger than a single region, then we will
         * need to write more code. But - that code may live upstairs?
         */
        let eid: u64 = o / ddef.extent_size().value;
        assert!((eid as u32) < ddef.extent_count());

        let extent_offset: u64 = o % ddef.extent_size().value;
        let mut sz: u64 = ddef.extent_size().value - extent_offset;
        if blocks_left < sz {
            sz = blocks_left;
        }

        result.push((eid, Block::new_with_ddef(extent_offset, &ddef), sz));

        match blocks_left.checked_sub(sz) {
            Some(v) => {
                blocks_left = v;
            }
            None => {
                break;
            }
        }

        o += sz;
    }

    {
        let mut blocks = 0;
        for r in &result {
            blocks += r.2;
        }
        assert_eq!(blocks, num_blocks);
    }

    Ok(result)
}

/*
 * Decide what to do with a downstairs that has just connected and has
 * sent us information about its extents.
 *
 * XXX At the moment we are doing both wait quorum and verify consistency
 * in the same function.  This will soon break out into two separate places
 * where we then decide what to do with each downstairs.
 */
fn process_downstairs(
    target: &SocketAddrV4,
    u: &Arc<Upstairs>,
    bs: u64,
    es: u64,
    ec: u32,
    versions: Vec<u64>,
) -> Result<()> {
    println!(
        "{} Evaluate new downstairs : bs:{} es:{} ec:{}",
        target, bs, es, ec
    );

    if versions.len() > 12 {
        println!("{} versions[0..12]: {:?}", target, versions[0..12].to_vec());
    } else {
        println!("{}  versions: {:?}", target, versions);
    }

    let mut fi = u.flush_info.lock().unwrap();
    if fi.flush_numbers.is_empty() {
        /*
         * This is the first version list we have, so
         * we will make it the original and compare
         * whatever comes next.
         */
        fi.flush_numbers = versions;
        fi.next_flush = *fi.flush_numbers.iter().max().unwrap() + 1;
        print!("Set inital Extent versions to");
        if fi.flush_numbers.len() > 12 {
            println!(" [0..12]{:?}", fi.flush_numbers[0..12].to_vec());
        } else {
            println!("{:?}", fi.flush_numbers);
        }
        println!("Next flush: {}", fi.next_flush);
    } else if fi.flush_numbers.len() != versions.len() {
        /*
         * I don't think there is much we can do here, the expected number
         * of flush numbers does not match.  Possibly we have grown one but
         * not the rest of the downstairs?
         */
        panic!(
            "Expected downstairs version \
              len:{:?} does not match new \
              downstairs:{:?}",
            fi.flush_numbers.len(),
            versions.len()
        );
    } else {
        /*
         * We already have a list of versions to compare with.  Make that
         * comparision now against this new list
         */
        let ver_cmp = fi.flush_numbers.iter().eq(versions.iter());
        if !ver_cmp {
            println!(
                "{} MISMATCH expected: {:?} != new: {:?}",
                target, fi.flush_numbers, versions
            );
            // XXX Recovery process should start here
            println!("{} Ignoring this downstairs version info", target);
        }
    }

    /*
     * XXX Here we have another workaround.  We don't know
     * the region info until after we connect to each
     * downstairs, but we share the ARC Upstairs before we
     * know what to expect.  For now I'm using zero as an
     * indication that we don't yet know the valid values
     * and non-zero meaning we have at least one downstairs
     * to compare with.  We might want to consider breaking
     * out the static config info into something different
     * that is updated on initial downstairs setup from the
     * structures we use for work submission.
     *
     * 0 should never be a valid block size
     */
    let mut ddef = u.ddef.lock().unwrap();
    if ddef.block_size() == 0 {
        ddef.set_block_size(bs);
        ddef.set_extent_size(Block::new(es, bs.trailing_zeros()));
        ddef.set_extent_count(ec);
        println!("Global using: bs:{} es:{} ec:{}", bs, es, ec);
    }

    if ddef.block_size() != bs
        || ddef.extent_size().value != es
        || ddef.extent_size().block_size_in_bytes() != bs as u32
        || ddef.extent_count() != ec
    {
        // XXX Figure out if we can hande this error.  Possibly not.
        panic!("New downstairs region info mismatch");
    }

    Ok(())
}

/*
 * This function is called when the upstairs task is notified that
 * a downstairs operation has completed. We add the read buffer to the
 * IOop struct for later processing if required.
 */
#[instrument]
async fn io_completed(
    up: &Arc<Upstairs>,
    ds_id: u64,
    client_id: u8,
    data: Option<Bytes>,
    ds_done_tx: mpsc::Sender<u64>,
    result: Result<(), CrucibleError>,
) -> Result<()> {
    // Mark this ds_id for the client_id as completed.
    let gw_work_done = {
        let mut work = up.ds_work.lock().unwrap();
        work.complete(ds_id, client_id, data, result)?
    };

    if gw_work_done {
        ds_done_tx.send(ds_id).await?
    }

    Ok(())
}

/*
 * This function is called by a worker task after the main task has added
 * work to the hashmap and notified the worker tasks that new work is ready
 * to be serviced.  The worker task will walk the hashmap and build a list
 * of new work that it needs to do.  It will then iterate through those
 * work items and send them over the wire to this tasks waiting downstairs.
 */
#[instrument(skip(fw))]
async fn io_send(
    u: &Arc<Upstairs>,
    fw: &mut FramedWrite<WriteHalf<'_>, CrucibleEncoder>,
    client_id: u8,
    lossy: bool,
) -> Result<()> {
    /*
     * Build ourselves a list of all the jobs on the work hashmap that
     * have the job state for our client id in the IOState::New
     *
     * The length of this list (new work for a downstairs) can give us
     * an idea of how that downstairs is doing.  If the number of jobs
     * to be submitted is too big (for some value of big) then there is
     * a problem.  All sorts of back pressure information can be
     * gathered here.  As (for the moment) the same task does both
     * transmit and receive, we can starve the receive side by spending
     * all our time sending work.
     *
     * This XXX is for coming back here and making a better job of
     * flow control.
     */
    let mut new_work = u.ds_work.lock().unwrap().new_work(client_id);

    /*
     * Now we have a list of all the job IDs that are new for our client id.
     * Walk this list and process each job, marking it InProgress as we
     * do the work.  We do this in two loops because we can't hold the
     * lock for the hashmap while we do work, and if we release the lock
     * to do work, we would have to start over and look at all jobs in the
     * map to see if they are new.
     *
     * This also allows us to sort the job ids and do them in order they
     * were put into the hashmap, though I don't think that is required.
     */
    new_work.sort_unstable();

    for new_id in new_work.iter() {
        /*
         * Walk the list of work to do, update its status as in progress
         * and send the details to our downstairs.
         */
        if lossy && random() && random() {
            continue;
        }

        let job = u.ds_work.lock().unwrap().in_progress(*new_id, client_id);
        match job {
            IOop::Write {
                dependencies,
                eid,
                offset,
                data,
            } => {
                fw.send(Message::Write(
                    *new_id,
                    eid,
                    dependencies.clone(),
                    offset,
                    data.clone(),
                ))
                .await?
            }
            IOop::Flush {
                dependencies,
                flush_number,
            } => {
                fw.send(Message::Flush(
                    *new_id,
                    dependencies.clone(),
                    flush_number,
                ))
                .await?
            }
            IOop::Read {
                dependencies,
                eid,
                offset,
                num_blocks,
            } => {
                fw.send(Message::ReadRequest(
                    *new_id,
                    dependencies.clone(),
                    eid,
                    offset,
                    num_blocks,
                ))
                .await?
            }
        }
    }
    Ok(())
}

/*
 * Once we have a connection to a downstairs, this task takes over and
 * handles both the initial negotiation and then watches the input for
 * changes, indicating that new work in on the work hashmap.  We will
 * walk the hashmap on the input signal and get any new work for this
 * specific downstairs and mark that job as in progress.
 */
async fn proc(
    target: &SocketAddrV4,
    up: &Arc<Upstairs>,
    mut sock: TcpStream,
    connected: &mut bool,
    mut up_coms: UpComs,
    lossy: bool,
) -> Result<()> {
    let (r, w) = sock.split();
    let mut fr = FramedRead::new(r, CrucibleDecoder::new());
    let mut fw = FramedWrite::new(w, CrucibleEncoder::new());

    /*
     * As the "client", we must begin the negotiation.
     */
    fw.send(Message::HereIAm(1)).await?;

    /*
     * Don't wait more than 50 seconds to hear from the other side.
     * XXX Timeouts, timeouts: always wrong!  Some too short and some too long.
     * TODO: 50 is too long, but what is the correct value?
     */
    let mut deadline = deadline_secs(50);
    let mut negotiated = false;

    /*
     * To keep things alive, initiate a ping any time we have been idle for a
     * second.
     */
    let mut pingat = deadline_secs(10);
    let mut needping = false;

    loop {
        /*
         * XXX Just a thought here, could we send so much input that the
         * select would always have ds_work_rx.changed() and starve out the
         * fr.next() select?  Does this select ever work that way?
         */
        tokio::select! {
            _ = sleep_until(deadline) => {
                if !negotiated {
                    up.ds_transition(up_coms.client_id, DsState::Disconnected);
                    bail!("did not negotiate a protocol");
                }
                /*
                 * XXX What should happen here?  At the moment we just
                 * ignore it..
                 */
                println!("Deadline ignored");
            }
            _ = sleep_until(pingat), if needping => {
                fw.send(Message::Ruok).await?;
                needping = false;
                if lossy {
                    /*
                     * When lossy is set, we don't always send work to a
                     * downstairs when we should.  This means we need to,
                     * every now and then, signal the downstairs task to
                     * check and see if we skipped some work earlier.
                     */
                    io_send(up, &mut fw, up_coms.client_id, lossy).await?;
                }
            }
            _ = up_coms.ds_work_rx.changed() => {
                /*
                 * A change here indicates the work hashmap has changed
                 * and we should go look for new work to do.  It is possible
                 * that there is no new work but we won't know until we
                 * check.
                 */
                /*
                 * Debug XXX
                let iv = *ds_work_rx.borrow();
                println!("[{}] Input changed with {}", up_coms.client_id, iv);
                 */
                io_send(up, &mut fw, up_coms.client_id, lossy).await?;
            }
            f = fr.next() => {
                /*
                 * Negotiate protocol before we get into specifics.
                 */
                match f.transpose()? {
                    None => {
                        return Ok(())
                    }
                    Some(Message::YesItsMe(version)) => {
                        if negotiated {
                            bail!("negotiated already!");
                        }
                        /*
                         * XXX Valid version to compare with should come
                         * from main task
                         */
                        if version != 1 {
                            up.ds_transition(up_coms.client_id, DsState::BadVersion);
                            bail!("expected version 1, got {}", version);
                        }
                        /*
                         * XXX BadRegion check will come when the upstairs
                         * starts with the expected region size and we can
                         * validate it.
                         *
                         * Until then, we just jump to waiting for quorum.
                         */
                        negotiated = true;
                        needping = true;
                        deadline = deadline_secs(50);

                        /*
                         * Ask for the current version of all extents.
                         */
                        fw.send(Message::ExtentVersionsPlease).await?;
                    }
                    Some(Message::ExtentVersions(bs, es, ec, versions)) => {
                        if !negotiated {
                            bail!("expected YesItsMe first");
                        }

                        /*
                         * We should be able to verify the bs, es, and ec
                         * based on what the upstairs knows already.
                         * For comparing the region data, we need to collect
                         * version and dirty bit info from all three
                         * downstairs, and make the decision on which data is
                         * correct once we have everything.
                         */
                        process_downstairs(target, up, bs, es, ec, versions)?;

                        up.ds_transition(up_coms.client_id, DsState::WaitQuorum);
                        up.ds_state_show();

                        /*
                         * If we get here, we are ready to receive IO
                         */
                        *connected = true;
                        up_coms.ds_status_tx.send(Condition {
                            target: *target,
                            connected: true,
                        }).await
                        .unwrap();
                    }
                    Some(m) => {
                        if !negotiated {
                            bail!("expected YesItsMe first");
                        }
                        /*
                         * TODO: Add a check here to make sure we are
                         * connected and in the proper state before we
                         * accept any commands.
                         */
                        proc_frame(up, &m, up_coms.clone()).await?;
                        deadline = deadline_secs(50);
                        pingat = deadline_secs(10);
                        needping = true;
                    }
                }
            }
        }
    }
}

/*
 * Things that allow the various tasks of Upstairs to communicate
 * with each other.
 */
#[derive(Clone)]
struct UpComs {
    /**
     * The client ID who will be using these channels.
     */
    client_id: u8,
    /**
     * This channel is used to receive a notification that new work has
     * (possibly) arrived on the work queue and this client should go
     * see what new work has arrived
     */
    ds_work_rx: watch::Receiver<u64>,
    /**
     * This channel is used to transmit that the state of the connection
     * to this downstairs has changed.
     */
    ds_status_tx: mpsc::Sender<Condition>,
    /**
     * This channel is used to transmit that an IO request sent by the
     * upstairs to all required downstairs has completed.
     */
    ds_done_tx: mpsc::Sender<u64>,
}

/*
 * This task is responsible for the connection to a specific downstairs
 * instance.
 */
async fn looper(
    target: SocketAddrV4,
    up: &Arc<Upstairs>,
    up_coms: UpComs,
    lossy: bool,
) {
    let mut firstgo = true;
    let mut connected = false;

    'outer: loop {
        if firstgo {
            firstgo = false;
        } else {
            tokio::time::sleep(Duration::from_secs(1)).await;
        }

        /*
         * Make connection to this downstairs.
         */
        let sock = TcpSocket::new_v4().expect("v4 socket");

        /*
         * Set a connect timeout, and connect to the target:
         */
        println!("{0}[{1}] connecting to {0}", target, up_coms.client_id);
        let deadline = tokio::time::sleep_until(deadline_secs(10));
        tokio::pin!(deadline);
        let tcp = sock.connect(target.into());
        tokio::pin!(tcp);

        let tcp: TcpStream = loop {
            tokio::select! {
                _ = &mut deadline => {
                    println!("connect timeout");
                    continue 'outer;
                }
                tcp = &mut tcp => {
                    match tcp {
                        Ok(tcp) => {
                            println!("{0}[{1}] ok, connected to {0}",
                                target,
                                up_coms.client_id);
                            break tcp;
                        }
                        Err(e) => {
                            println!("{0} connect to {0} failure: {1:?}",
                                target, e);
                            continue 'outer;
                        }
                    }
                }
            }
        };

        /*
         * Once we have a connected downstairs, the proc task takes over and
         * handles negiotation and work processing.
         */
        if let Err(e) =
            proc(&target, up, tcp, &mut connected, up_coms.clone(), lossy).await
        {
            eprintln!("ERROR: {}: proc: {:?}", target, e);
        }
        /*
         * If the connection goes down here, we need to know what state we
         * were in to decide what state to transition to.
         */
        up.ds_transition(up_coms.client_id, DsState::Disconnected);
        up.ds_state_show();

        println!(
            "{0}[{1}] connection to {0} closed",
            target, up_coms.client_id
        );
        if connected {
            up_coms
                .ds_status_tx
                .send(Condition {
                    target,
                    connected: false,
                })
                .await
                .unwrap();
            connected = false;
        }
    }
}

/*
 * The structure that tracks downstairs work in progress
 */
#[derive(Debug)]
pub struct Work {
    active: HashMap<u64, DownstairsIO>,
    next_id: u64,
    completed: AllocRingBuffer<u64>,
}

/*
 * These counts describe the various states that a Downstairs IO can
 * be in.
 */
#[derive(Debug, Default)]
pub struct WorkCounts {
    active: u64,    // New or in flight to downstairs.
    ack_ready: u64, // Downstairs done, use this for the ACK
    acked: u64,     // This IO was used for the ACK
    error: u64,     // This IO had an error.
    done: u64,      // This IO has completed (but not used for ACK)
}

impl WorkCounts {
    fn completed_ok(&self) -> u64 {
        self.ack_ready + self.acked + self.done
    }
}

impl Work {
    /**
     * Assign a new downstairs ID.
     */
    fn next_id(&mut self) -> u64 {
        let id = self.next_id;
        self.next_id += 1;
        id
    }

    /**
     * Mark this request as in progress for this client, and return a copy
     * of the details of the request.
     */
    fn in_progress(&mut self, ds_id: u64, client_id: u8) -> IOop {
        let job = self.active.get_mut(&ds_id).unwrap();
        let oldstate = job.state.insert(client_id, IOState::InProgress);
        assert_eq!(oldstate, Some(IOState::New));
        job.work.clone()
    }

    /**
     * Return a list of downstairs request IDs that represent unissued
     * requests for this client.
     */
    fn new_work(&self, client_id: u8) -> Vec<u64> {
        self.active
            .values()
            .filter_map(|job| {
                if let Some(IOState::New) = job.state.get(&client_id) {
                    Some(job.ds_id)
                } else {
                    None
                }
            })
            .collect()
    }

    /**
     * Build an in order list of jobs that are ready to be acked.
     */
    fn ackable_work(&mut self) -> Vec<u64> {
        let mut ackable = Vec::new();
        let mut kvec = self.active.keys().cloned().collect::<Vec<u64>>();
        kvec.sort_unstable();
        for ds_id in kvec.iter() {
            let ac = self.state_count(*ds_id).unwrap().ack_ready;
            if ac == 1 {
                ackable.push(*ds_id);
            } else {
                // Only one downstairs IO should be AckReady
                assert_eq!(ac, 0);
            }
        }
        ackable
    }

    /**
     * Enqueue a new downstairs request.
     */
    fn enqueue(&mut self, io: DownstairsIO) {
        self.active.insert(io.ds_id, io);
    }

    /**
     * Collect the state of the jobs from each client.
     */
    fn state_count(&mut self, ds_id: u64) -> Result<WorkCounts> {
        /* XXX Should this support invalid ds_ids? */
        let job = self
            .active
            .get_mut(&ds_id)
            .ok_or_else(|| anyhow!("reqid {} is not active", ds_id))?;

        let mut wc: WorkCounts = Default::default();
        for state in job.state.values() {
            match state {
                IOState::New | IOState::InProgress => wc.active += 1,
                IOState::AckReady => wc.ack_ready += 1,
                IOState::Error(_) => wc.error += 1,
                IOState::Acked | IOState::Done | IOState::Skipped => {
                    wc.done += 1;
                }
            }
        }

        Ok(wc)
    }

    fn ack(&mut self, ds_id: u64) -> bool {
        /*
         * Move AckReady to Acked.
         *
         * Returns false if no AckReady found.
         */

        let job = self
            .active
            .get_mut(&ds_id)
            .ok_or_else(|| anyhow!("reqid {} is not active", ds_id))
            .unwrap();

        let mut ack_ready = false;
        for cid in 0..3 {
            if let Some(IOState::AckReady) = job.state.get(&cid) {
                job.state.insert(cid, IOState::Acked);
                ack_ready = true;
                break;
            }
        }

        ack_ready
    }

    fn result(&mut self, ds_id: u64) -> Result<(), CrucibleError> {
        /*
         * If enough downstairs returned an error, then return an error to the Guest
         *
         * Not ok:
         * - 2+ errors for Write/Flush
         * - 3+ errors for Reads
         *
         * TODO: this doesn't tell the Guest what the error(s) were?
         */
        let wc = self.state_count(ds_id).unwrap();

        let job = self
            .active
            .get_mut(&ds_id)
            .ok_or_else(|| anyhow!("reqid {} is not active", ds_id))?;

        /*
         * XXX: this code assumes that 3 downstairs is the max that we'll ever support.
         */
        let bad_job = match &job.work {
            IOop::Read {
                dependencies: _dependencies,
                eid: _eid,
                offset: _offset,
                num_blocks: _num_blocks,
            } => wc.error == 3,
            IOop::Write {
                dependencies: _dependencies,
                eid: _eid,
                data: _data,
                offset: _offset,
            } => wc.error >= 2,
            IOop::Flush {
                dependencies: _dependencies,
                flush_number: _flush_number,
            } => wc.error >= 2,
        };

        if bad_job {
            Err(CrucibleError::IoError(format!(
                "{} out of 3 downstairs returned an error",
                wc.error
            )))
        } else {
            Ok(())
        }
    }

    /*
     * This function just does the match on IOop type and updates the dtrace
     * probe for that operation finishing.
     */
    fn crutrace_gw_work_done(&self, ds_id: u64, gw_id: u64) {
        let job = self
            .active
            .get(&ds_id)
            .ok_or_else(|| anyhow!("reqid {} is not active", ds_id))
            .unwrap();

        match &job.work {
            IOop::Read {
                dependencies: _,
                eid: _,
                offset: _,
                num_blocks: _,
            } => {
                crutrace_gw_read_end!(|| (gw_id));
            }
            IOop::Write {
                dependencies: _,
                eid: _,
                offset: _,
                data: _,
            } => {
                crutrace_gw_write_end!(|| (gw_id));
            }
            IOop::Flush {
                dependencies: _,
                flush_number: _,
            } => {
                crutrace_gw_flush_end!(|| (gw_id));
            }
        }
    }

    /**
     * Mark this downstairs request as complete for this client.  Returns
     * true if this completion is enough that we should message the
     * upstairs task that handles returning completions to the guest.
     *
     * This is where we decide the number of successful completions required
     * before setting the AckReady state on a ds_id, which another upstairs
     * task is looking for to then ACK back to the guest.
     */
    fn complete(
        &mut self,
        ds_id: u64,
        client_id: u8,
        read_data: Option<Bytes>,
        result: Result<(), CrucibleError>,
    ) -> Result<bool> {
        /*
         * Assume we don't have enough completed jobs, and only change
         * it if we have the exact amount required
         */
        let mut notify_guest = false;

        /*
         * Get the completed count now,
         * because the job self ref won't let us call state_count once we are
         * using that ref, and the number won't change while we are in
         * this method (you did get the lock first, right??).
         */
        let wc = self.state_count(ds_id)?;
        let mut jobs_completed_ok = wc.completed_ok();

        let job = self
            .active
            .get_mut(&ds_id)
            .ok_or_else(|| anyhow!("reqid {} is not active", ds_id))?;

        let newstate = if let Err(e) = result {
            IOState::Error(e)
        } else {
            jobs_completed_ok += 1;
            IOState::Done
        };

        let oldstate = job.state.insert(client_id, newstate);

        if let Some(oldstate) = oldstate {
            // we shouldn't be transitioning a state that was already transitioned
            assert_eq!(oldstate, IOState::InProgress);
        } else {
            panic!("no old state! that's bad!");
        }

        /*
         * Transition this job from Done to AckReady if enough have returned ok.
         *
         * XXX Any error needs to be passed to Nexus
         */
        match &job.work {
            IOop::Read {
                dependencies: _dependencies,
                eid: _eid,
                offset: _offset,
                num_blocks: _num_blocks,
            } => {
                assert!(read_data.is_some());
                if jobs_completed_ok == 1 {
                    assert!(job.data.is_none());
                    job.data = read_data;
                    notify_guest = true;
                    job.state.insert(client_id, IOState::AckReady);
                }
            }
            IOop::Write {
                dependencies: _dependencies,
                eid: _eid,
                data: _data,
                offset: _offset,
            } => {
                assert!(read_data.is_none());
                if jobs_completed_ok == 2 {
                    notify_guest = true;
                    job.state.insert(client_id, IOState::AckReady);
                }
            }
            IOop::Flush {
                dependencies: _dependencies,
                flush_number: _flush_number,
            } => {
                assert!(read_data.is_none());
                if jobs_completed_ok == 2 {
                    notify_guest = true;
                    job.state.insert(client_id, IOState::AckReady);
                }
            }
        }

        /*
         * If all 3 jobs are done, we can check here to see if we can
         * remove this job from the DS list. If we have completed the ack
         * to the guest, then there will be no more work on this job.
         */
        self.retire_check(ds_id);

        Ok(notify_guest)
    }

    /**
     * This request is now complete on all peers. Remove it from the active set and mark it in the
     * completed ring buffer. Note we shall not retire a job until it has been ack'd back to the
     * guest. Just being ack ready is not enough.
     */
    fn retire_check(&mut self, ds_id: u64) {
        let wc = self.state_count(ds_id).unwrap();

        if (wc.error + wc.done) == 3 {
            assert!(!self.completed.contains(&ds_id));
            assert_eq!(wc.active, 0);
            assert_eq!(wc.ack_ready, 0);
            self.active.remove(&ds_id).unwrap();
            self.completed.push(ds_id);
        }
    }
}

/// Implement XTS encryption
/// See: https://en.wikipedia.org/wiki/Disk_encryption_theory#XEX-based_tweaked-codebook_mode_with_ciphertext_stealing_(XTS)
pub struct EncryptionContext {
    xts: Xts128<Aes128>,
    key: Vec<u8>,
    block_size: usize,
}

impl Debug for EncryptionContext {
    fn fmt(&self, f: &mut Formatter<'_>) -> Result<(), fmt::Error> {
        f.debug_struct("EncryptionContext")
            .field("block_size", &self.block_size)
            .finish()
    }
}

impl Clone for EncryptionContext {
    fn clone(&self) -> Self {
        EncryptionContext::new(self.key.clone(), self.block_size)
    }

    fn clone_from(&mut self, source: &Self) {
        *self = EncryptionContext::new(source.key.clone(), source.block_size);
    }
}

impl EncryptionContext {
    pub fn new(key: Vec<u8>, block_size: usize) -> EncryptionContext {
        assert!(key.len() == 32);

        let cipher_1 = Aes128::new(GenericArray::from_slice(&key[..16]));
        let cipher_2 = Aes128::new(GenericArray::from_slice(&key[16..]));

        let xts = Xts128::<Aes128>::new(cipher_1, cipher_2);

        EncryptionContext {
            xts,
            key,
            block_size,
        }
    }

    pub fn key(&self) -> &Vec<u8> {
        &self.key
    }

    pub fn block_size(&self) -> usize {
        self.block_size
    }

    pub fn encrypt_in_place(&self, data: &mut [u8], sector_index: u128) {
        self.xts.encrypt_area(
            data,
            self.block_size,
            sector_index,
            get_tweak_default,
        );
    }

    pub fn decrypt_in_place(&self, data: &mut [u8], sector_index: u128) {
        self.xts.decrypt_area(
            data,
            self.block_size,
            sector_index,
            get_tweak_default,
        );
    }
}

/*
 * XXX Track scheduled storage work in the central structure.  Have the
 * target management task check for work to do here by changing the value in
 * its watch::channel.  Have the main thread determine that an overflow of
 * work to do backing up in here means we need to do something like mark the
 * target as behind or institute some kind of back pressure, etc.
 */
#[derive(Debug)]
pub struct Upstairs {
    /*
     * The guest struct keeps track of jobs accepted from the Guest as they
     * progress through crucible.  A single job submitted can produce
     * multiple downstairs requests.
     */
    guest: Arc<Guest>,

    /*
     * The state of a downstairs connection, based on client ID
     * Ready here indicates it can receive IO.
     * TODO: When growing to more than one region, should this become
     * a 2d Vec? index for region, then index for the DS?
     */
    ds_state: Mutex<Vec<DsState>>,

    /*
     * This Work struct keeps track of IO operations going between upstairs
     * and downstairs.  New work for downstairs is generated inside the
     * upstairs on behalf of IO requests coming from the guest.
     */
    ds_work: Mutex<Work>,

    /*
     * The flush info Vec is only used when first connecting or re-connecting
     * to a downstairs.  It is populated with the versions the upstairs
     * considers the "correct".  If a downstairs disconnects and then
     * comes back, it has to match or be made to match what was decided
     * as the correct list.  This may involve having to refresh the versions
     * vec.
     *
     * The versions vec is not enough to solve a mismatch.  We really need
     * Generation number, flush number, and dirty bit for every extent
     * when resolving conflicts.
     *
     * On Startup we determine the highest flush number from all three
     * downstairs.  We add one to that and it becomes the next flush
     * number.  Flush numbers increment by one each time.
     */
    flush_info: Mutex<FlushInfo>,

    /*
     * The global description of the downstairs region we are using.
     * This allows us to verify each downstairs is the same, as well as
     * enables us to tranlate an LBA to an extent and block offset.
     */
    ddef: Mutex<RegionDefinition>,

    /*
     * Optional encryption context - Some if a key was supplied in the CrucibleOpts
     */
    encryption_context: Option<EncryptionContext>,
}

impl Upstairs {
    pub fn new(opt: &CrucibleOpts, guest: Arc<Guest>) -> Arc<Upstairs> {
        /*
         * XXX Make sure we have three and only three downstairs
         */
        assert_eq!(opt.target.len(), 3);

        // create an encryption context if a key is supplied.
        let encryption_context = opt.key_bytes().map(|key| {
            EncryptionContext::new(
                key,
                /*
                 * XXX: It would be good to do BlockOp::QueryBlockSize here, but
                 * this creates a deadlock. Upstairs::new runs before up_ds_listen in up_main,
                 * and up_ds_listen needs to run to answer BlockOp::QueryBlockSize.
                 *
                 * At this point ddef is the default, the downstairs haven't reported in.
                 */
                512,
            )
        });

        let ds_state = vec![DsState::New; 3];
        Arc::new(Upstairs {
            guest,
            ds_state: Mutex::new(ds_state),
            ds_work: Mutex::new(Work {
                active: HashMap::new(),
                completed: AllocRingBuffer::with_capacity(2048),
                next_id: 1000,
            }),
            flush_info: Mutex::new(FlushInfo::new()),
            ddef: Mutex::new(RegionDefinition::default()),
            encryption_context,
        })
    }

    /*
     * If we are doing a flush, the flush number and the rn number
     * must both go up together.  We don't want a lower next_id
     * with a higher flush_number to be possible, as that can introduce
     * dependency deadlock.
     * To also avoid any problems, this method should be called only
     * during the submit_flush method so we know the ds_work and
     * guest_work locks are both held.
     */
    fn next_flush_id(&self) -> u64 {
        let mut fi = self.flush_info.lock().unwrap();
        fi.next_flush()
    }

    #[instrument]
    pub fn submit_flush(
        &self,
        sender: std_mpsc::Sender<Result<(), CrucibleError>>,
    ) -> Result<(), CrucibleError> {
        /*
         * Lock first the guest_work struct where this new job will go,
         * then lock the ds_work struct.  Once we have both we can proceed
         * to build our flush command.
         */
        let mut gw = self.guest.guest_work.lock().unwrap();
        let mut ds_work = self.ds_work.lock().unwrap();

        /*
         * Get the next ID for our new guest work job.  Note that the flush
         * ID and the next_id are connected here, in that all future writes
         * should be flushed at the next flush ID.
         */
        let gw_id: u64 = gw.next_gw_id();
        let next_id = ds_work.next_id();
        let next_flush = self.next_flush_id();

        /*
         * Walk the downstairs work active list, and pull out all the active
         * jobs.  Anything we have not submitted back to the guest.
         *
         * TODO, we can go faster if we:
         * 1. Ignore everything that was before and including the last flush.
         * 2. Ignore reads.
         */
        let mut dep = ds_work.active.keys().cloned().collect::<Vec<u64>>();
        dep.sort_unstable();
        /*
         * TODO: Walk the list of guest work structs and build the same list
         * and make sure it matches.
         */

        /*
         * Build the flush request, and take note of the request ID that
         * will be assigned to this new piece of work.
         */
        let fl = create_flush(next_id, dep, next_flush, gw_id);

        let mut sub = HashMap::new();
        sub.insert(next_id, 0);

        let new_gtos = GtoS::new(
            sub,
            Vec::new(),
            None,
            HashMap::new(),
            HashMap::new(),
            sender,
            None,
        );
        gw.active.insert(gw_id, new_gtos);
        crutrace_gw_flush_start!(|| (gw_id));

        ds_work.enqueue(fl);

        Ok(())
    }

    /*
     * When we have a guest write request with offset and buffer, take them and
     * build both the upstairs work guest tracking struct as well as the downstairs
     * work struct. Once both are ready, submit them to the required places.
     */
    #[instrument]
    fn submit_write(
        &self,
        offset: Block,
        data: Bytes,
        sender: std_mpsc::Sender<Result<(), CrucibleError>>,
    ) -> Result<(), CrucibleError> {
        /*
         * Get the next ID for the guest work struct we will make at the
         * end.  This ID is also put into the IO struct we create that
         * handles the operation(s) on the storage side.
         */
        let mut gw = self.guest.guest_work.lock().unwrap();
        let mut ds_work = self.ds_work.lock().unwrap();
        let gw_id: u64 = gw.next_gw_id();

        /*
         * Given the offset and buffer size, figure out what extent and
         * byte offset that translates into.  Keep in mind that an offset
         * and length may span two extents, and eventually XXX, two regions.
         */
        let ddef = self.ddef.lock().unwrap();
        let nwo = extent_from_offset(
            *ddef,
            offset,
            data.len() as u64 / ddef.block_size(),
        )
        .unwrap();

        /*
         * Now create a downstairs work job for each (eid, bi, len) returned
         * from extent_from_offset
         *
         * Create the list of downstairs request numbers (ds_id) we created
         * on behalf of this guest job.
         */
        let mut sub = HashMap::new();
        let mut new_ds_work = Vec::new();
        let mut next_id: u64;
        let mut cur_offset: usize = 0;

        let mut dep = ds_work.active.keys().cloned().collect::<Vec<u64>>();
        dep.sort_unstable();
        /* Lock here, through both jobs submitted */
        for (eid, bo, num_blocks) in nwo {
            {
                next_id = ds_work.next_id();
            }

            let byte_len: usize =
                num_blocks as usize * ddef.block_size() as usize;

            let sub_data = if let Some(context) = &self.encryption_context {
                // Encrypt here
                let mut mut_data =
                    data.slice(cur_offset..(cur_offset + byte_len)).to_vec();
                context.encrypt_in_place(&mut mut_data[..], bo.value as u128);
                Bytes::copy_from_slice(&mut_data)
            } else {
                // Unencrypted
                data.slice(cur_offset..(cur_offset + byte_len))
            };

            sub.insert(next_id, num_blocks);

            let wr = create_write_eob(
                next_id,
                dep.clone(),
                gw_id,
                eid,
                bo,
                sub_data,
            );

            new_ds_work.push(wr);
            cur_offset += byte_len;
        }

        /*
         * New work created, add to the guest_work HM
         */
        let new_gtos = GtoS::new(
            sub,
            Vec::new(),
            None,
            HashMap::new(),
            HashMap::new(),
            sender,
            None,
        );
        {
            gw.active.insert(gw_id, new_gtos);
        }
        crutrace_gw_write_start!(|| (gw_id));

        for wr in new_ds_work {
            ds_work.enqueue(wr);
        }

        Ok(())
    }

    /*
     * When we have a guest read request with offset and buffer, take them
     * and build both the upstairs work guest tracking struct as well as the
     * downstairs work struct. Once both are ready, submit them to the
     * required places.
     */
    #[instrument]
    pub fn submit_read(
        &self,
        offset: Block,
        data: Buffer,
        sender: std_mpsc::Sender<Result<(), CrucibleError>>,
    ) -> Result<(), CrucibleError> {
        /*
         * Get the next ID for the guest work struct we will make at the
         * end.  This ID is also put into the IO struct we create that
         * handles the operation(s) on the storage side.
         */
        let mut gw = self.guest.guest_work.lock().unwrap();
        let mut ds_work = self.ds_work.lock().unwrap();
        let gw_id: u64 = gw.next_gw_id();

        /*
         * Given the offset and buffer size, figure out what extent and
         * byte offset that translates into. Keep in mind that an offset
         * and length may span many extents, and eventually, TODO, regions.
         */
        let ddef = self.ddef.lock().unwrap();
        let nwo = extent_from_offset(
            *ddef,
            offset,
            data.len() as u64 / ddef.block_size(),
        )
        .unwrap();

        /*
         * Create the tracking info for downstairs request numbers (ds_id) we
         * will create on behalf of this guest job.
         */
        let mut sub = HashMap::new();
        let mut new_ds_work = Vec::new();
        let mut downstairs_buffer_sector_index = HashMap::new();
        let mut next_id: u64;

        /*
         * Now create a downstairs work job for each (eid, bo, len) returned
         * from extent_from_offset
         */
        let mut dep = ds_work.active.keys().cloned().collect::<Vec<u64>>();
        dep.sort_unstable();
        for (eid, bo, num_blocks) in nwo {
            {
                next_id = ds_work.next_id();
            }

            /*
             * When multiple operations are needed to satisfy a read, The offset
             * and length will be divided across two downstairs requests.  It is
             * required (for re-assembly on the other side) that the lower offset
             * corresponds to the lower next_id.  The ID's don't need to be
             * sequential.
             */
            sub.insert(next_id, num_blocks);
            downstairs_buffer_sector_index.insert(next_id, bo.value as u128);
            let wr = create_read_eob(
                next_id,
                dep.clone(),
                gw_id,
                eid,
                bo,
                num_blocks,
            );
            new_ds_work.push(wr);
        }

        /*
         * New work created, add to the guest_work HM.  New work must be put
         * on the guest_work active HM first, before it lands on the downstairs
         * lists.  We don't want to miss a completion from downstairs.
         */
        assert!(!sub.is_empty());
        let new_gtos = GtoS::new(
            sub,
            Vec::new(),
            Some(data),
            HashMap::new(),
            downstairs_buffer_sector_index,
            sender,
            self.encryption_context.clone(),
        );
        {
            gw.active.insert(gw_id, new_gtos);
        }
        crutrace_gw_read_start!(|| (gw_id));

        for wr in new_ds_work {
            ds_work.enqueue(wr);
        }

        Ok(())
    }

    /*
     * Move a single downstairs to this new state.
     */
    fn ds_transition(&self, client_id: u8, new_state: DsState) {
        let mut state = self.ds_state.lock().unwrap();
        println!(
            "Transition [{}] from {:?} to {:?}",
            client_id, state[client_id as usize], new_state,
        );
        state[client_id as usize] = new_state;
    }

    fn ds_state_show(&self) {
        let state = self.ds_state.lock().unwrap();
        for (index, dst) in state.iter().enumerate() {
            println!("[{}] State {:?}", index, dst);
        }
    }

    /*
     * Move all downstairs to this new state.
     */
    fn ds_transition_all(&self, new_state: DsState) {
        let mut state = self.ds_state.lock().unwrap();

        state.iter_mut().for_each(|ds_state| {
            println!("Transition from {:?} to {:?}", *ds_state, new_state,);
            match new_state {
                DsState::Active => {
                    assert_eq!(*ds_state, DsState::WaitQuorum);
                    *ds_state = new_state;
                }
                _ => {
                    panic!(
                        "Unsupported state transition {:?} -> {:?}",
                        *ds_state, new_state
                    );
                }
            }
        });
    }
}

#[derive(Debug)]
struct FlushInfo {
    flush_numbers: Vec<u64>,
    /*
     * The next flush number to use when a Flush is issued.
     */
    next_flush: u64,
}

impl FlushInfo {
    pub fn new() -> FlushInfo {
        FlushInfo {
            flush_numbers: Vec::new(),
            next_flush: 0,
        }
    }
    /*
     * Upstairs flush_info mutex must be held when calling this.
     * In addition, a downstairs request ID should be obtained at the
     * same time the next flush number is obtained, such that any IO that
     * is given a downstairs request number higer than the request number
     * for the flush will happen after this flush, never before.
     */
    fn next_flush(&mut self) -> u64 {
        let id = self.next_flush;
        self.next_flush += 1;
        id
    }
}
/*
 * States a downstairs can be in.
 * XXX This very much still under development.  Most of these are place
 * holders and the final set of states will change.
 */
#[derive(Debug, Copy, Clone, PartialEq)]
enum DsState {
    /*
     * New connection
     */
    New,
    /*
     * Incompatable software version reported.
     */
    BadVersion,
    /*
     * Waiting for the minimum number of downstairs to be present.
     */
    WaitQuorum,
    /*
     * Incompatable region format reported.
     */
    _BadRegion,
    /*
     * We were connected, but have since gone offline.
     */
    Disconnected,
    /*
     * Comparing downstairs for consistency.
     */
    _Verifying,
    /*
     * Failed when attempting to make consistent.
     */
    _FailedRepair,
    /*
     * Ready for and/or currently receiving IO
     */
    Active,
    /*
     * IO attempts to this downstairs are failing at too high of a
     * rate, or it is not able to keep up., or it is having some
     * error such that we can no longer use it.
     */
    _Failed,
    /*
     * This downstairs is being migrated to a new location
     */
    _Migrating,
    /*
     * This downstairs was active, but is now no longer connected.
     */
    _Offline,
    /*
     * This downstairs was offline but is now back online and we are
     * sending it all the I/O it missed when it was unavailable.
     */
    _Replay,
}

/*
 * A unit of work for downstairs that is put into the hashmap.
 */
#[derive(Debug)]
struct DownstairsIO {
    ds_id: u64,    // This MUST match our hashmap index
    guest_id: u64, // The hahsmap ID from the parent guest work.
    work: IOop,
    /*
     * Hash of work status where key is the downstairs "client id" and the
     * hash value is the current state of the IO request with respect to the
     * upstairs.
     * The length and keys on this hashmap will be used to determine which
     * downstairs will receive the IO request.
     * XXX Determine if it is required for all downstairs to get an entry
     * or if by not putting a downstars in the hash, if that is valid.
     */
    state: HashMap<u8, IOState>,
    /*
     * If the operation is a Read, this holds the resulting buffer
     */
    data: Option<Bytes>,
}

/*
 * Crucible to storage IO operations.
 */
#[derive(Debug, Clone)]
pub enum IOop {
    Write {
        dependencies: Vec<u64>, // Jobs that must finish before this
        eid: u64,
        offset: Block,
        data: Bytes,
    },
    Read {
        dependencies: Vec<u64>, // Jobs that must finish before this
        eid: u64,
        offset: Block,
        num_blocks: u64,
    },
    Flush {
        dependencies: Vec<u64>, // Jobs that must finish before this
        flush_number: u64,
    },
}

/*
 * The various states an IO can be in when it is on the work hashmap.
 * There is a state that is unique to each downstairs task we have and
 * they operate independent of each other.
 */
#[derive(Debug, Clone, PartialEq)]
pub enum IOState {
    // A new IO request.
    New,
    // The request has been sent to this tasks downstairs.
    InProgress,
    // This IO has been completed by the downstairs, and we should use the
    // data in this IO as the source for a response back to the upstairs.
    // Only one of the three downstairs IOs should be "AckReady".
    AckReady,
    // This IO was used to ACK back to the guest and we are done with it.
    // We give it a special AckDone state to leave some breadcrumbs in case
    // we want to know which IO was the ACK.
    Acked,
    // The successful response came back from downstairs.
    Done,
    // XXX Unused.  The IO request should be ignored.  This situation could be
    // A read that only needs one downstairs to answer, or we are doing
    // recovery and we only want a specific downstairs to do that work.
    Skipped,
    // The IO returned an error.
    Error(CrucibleError),
}

impl fmt::Display for IOState {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        // Make sure to right-align output on 4 characters
        match self {
            IOState::New => {
                write!(f, " New")
            }
            IOState::InProgress => {
                write!(f, "Sent")
            }
            IOState::AckReady => {
                write!(f, "AckR")
            }
            IOState::Acked => {
                write!(f, "Ackd")
            }
            IOState::Done => {
                write!(f, "Done")
            }
            IOState::Skipped => {
                write!(f, "Skip")
            }
            IOState::Error(e) => {
                write!(f, " Err: {:?}", e)
            }
        }
    }
}

/*
 * Provides a shared Buffer that Read operations will write into.
 *
 * Originally BytesMut was used here, but it didn't guarantee that memory
 * was shared between cloned BytesMut objects.
 */
#[derive(Clone, Debug)]
pub struct Buffer {
    data: Arc<Mutex<Vec<u8>>>,
}

impl Buffer {
    pub fn from_vec(vec: Vec<u8>) -> Buffer {
        Buffer {
            data: Arc::new(Mutex::new(vec)),
        }
    }

    pub fn new(len: usize) -> Buffer {
        Buffer {
            data: Arc::new(Mutex::new(vec![0; len])),
        }
    }

    pub fn from_slice(buf: &[u8]) -> Buffer {
        let mut vec = Vec::<u8>::with_capacity(buf.len());
        for item in buf {
            vec.push(*item);
        }

        Buffer::from_vec(vec)
    }

    pub fn len(&self) -> usize {
        self.data.try_lock().unwrap().len()
    }

    pub fn is_empty(&self) -> bool {
        self.len() == 0
    }

    pub fn as_vec(&self) -> MutexGuard<Vec<u8>> {
        self.data.try_lock().unwrap()
    }
}

#[test]
fn test_buffer_len() {
    const READ_SIZE: usize = 512;
    let data = Buffer::from_slice(&[0x99; READ_SIZE]);
    assert_eq!(data.len(), READ_SIZE);
}

#[test]
fn test_buffer_len_after_clone() {
    const READ_SIZE: usize = 512;
    let data = Buffer::from_slice(&[0x99; READ_SIZE]);
    assert_eq!(data.len(), READ_SIZE);

    let new_buffer = data.clone();
    assert_eq!(new_buffer.len(), READ_SIZE);
}

#[test]
#[should_panic(
    expected = "index out of bounds: the len is 512 but the index is 512"
)]
fn test_buffer_len_index_overflow() {
    const READ_SIZE: usize = 512;
    let data = Buffer::from_slice(&[0x99; READ_SIZE]);
    assert_eq!(data.len(), READ_SIZE);

    let mut vec = data.as_vec();
    assert_eq!(vec.len(), 512);

    for i in 0..(READ_SIZE + 1) {
        vec[i] = 0x99;
    }
}

#[test]
fn test_buffer_len_over_block_size() {
    const READ_SIZE: usize = 600;
    let data = Buffer::from_slice(&[0x99; READ_SIZE]);
    assert_eq!(data.len(), READ_SIZE);
}

/*
 * Inspired from Propolis block.rs
 *
 * The following are the operations that Crucible supports from outside callers.
 * We have extended this to cover a bunch of test operations as well.
 * The first three are the supported operations, the other operations
 * tell the upstairs to behave in specific ways.
 */
#[derive(Debug)]
enum BlockOp {
    Read { offset: Block, data: Buffer },
    Write { offset: Block, data: Bytes },
    Flush,
    // Query ops
    QueryBlockSize { data: Arc<Mutex<u64>> },
    QueryTotalSize { data: Arc<Mutex<u64>> },
    // Begin testing options.
    QueryExtentSize { data: Arc<Mutex<Block>> },
    QueryActive { data: Arc<Mutex<usize>> },
    Commit, // Send update to all tasks that there is work on the queue.
    // Show internal work queue, return outstanding IO requests.
    ShowWork { data: Arc<Mutex<WQCounts>> },
}

/*
 * This structure is for tracking the underlying storage side operations
 * that map to a single Guest IO request. G to S stands for Guest
 * to Storage.
 *
 * The submitted hashmap is indexd by the request number (ds_id) for the
 * downstairs requests issued on behalf of this request.
 */
#[derive(Debug)]
struct GtoS {
    /*
     * Jobs we have submitted (or will soon submit) to the storage side
     * of the upstairs process to send on to the downstairs.
     * The key for the hashmap is the ds_id number in the hashmap for
     * downstairs work. The value is the buffer size of the operation in
     * blocks.
     */
    submitted: HashMap<u64, u64>,
    completed: Vec<u64>,

    /*
     * This buffer is provided by the guest request. If this is a read,
     * data will be written here.
     */
    guest_buffer: Option<Buffer>,

    /*
     * When we have an IO between the guest and crucible, it's possible
     * it will be broken into two smaller requests if the range happens
     * to cross an extent boundary.  This hashmap is a list of those
     * buffers with the key being the downstairs request ID.
     *
     * Data moving in/out of this buffer will be encrypted or decrypted
     * depending on the operation.
     */
    downstairs_buffer: HashMap<u64, Bytes>,
    downstairs_buffer_sector_index: HashMap<u64, u128>,

    /*
     * Notify the caller waiting on the job to finish.
     */
    sender: std_mpsc::Sender<Result<(), CrucibleError>>,

    /*
     * Optional encryption context - Some if the corresponding Upstairs is Some.
     */
    encryption_context: Option<EncryptionContext>,
}

impl GtoS {
    pub fn new(
        submitted: HashMap<u64, u64>,
        completed: Vec<u64>,
        guest_buffer: Option<Buffer>,
        downstairs_buffer: HashMap<u64, Bytes>,
        downstairs_buffer_sector_index: HashMap<u64, u128>,
        sender: std_mpsc::Sender<Result<(), CrucibleError>>,
        encryption_context: Option<EncryptionContext>,
    ) -> GtoS {
        GtoS {
            submitted,
            completed,
            guest_buffer,
            downstairs_buffer,
            downstairs_buffer_sector_index,
            sender,
            encryption_context,
        }
    }

    /*
     * When all downstairs jobs have completed, and all buffers have been
     * attached to the GtoS struct, we can do the final copy of the data
     * from upstairs memory back to the guest's memory.
     */
    #[instrument]
    fn transfer(&mut self) {
        if let Some(guest_buffer) = &mut self.guest_buffer {
            self.completed.sort_unstable();
            assert!(!self.completed.is_empty());

            let mut offset = 0;
            for ds_id in self.completed.iter() {
                let mut ds_vec =
                    self.downstairs_buffer.remove(ds_id).unwrap().to_vec();

                // if there's an encryption context, decrypt the downstairs buffer.
                if let Some(context) = &self.encryption_context {
                    context.decrypt_in_place(
                        &mut ds_vec[..],
                        *self
                            .downstairs_buffer_sector_index
                            .get(ds_id)
                            .unwrap(),
                    );
                }

                // Copy over into guest memory.
                {
                    let _ignored =
                        span!(Level::TRACE, "copy to guest buffer").entered();
                    let mut vec = guest_buffer.as_vec();
                    for i in &ds_vec {
                        vec[offset] = *i;
                        offset += 1;
                    }
                }
            }
        } else {
            /*
             * Should this panic?  If the caller is requesting a transfer,
             * the guest_buffer should exist.  If it does not exist, then
             * either there is a real problem, or the operation was a write
             * or flush and why are we requesting a transfer for those.
             */
            panic!("No guest buffer, no copy");
        }
    }

    /*
     * Notify corresponding BlockReqWaiter
     */
    pub fn notify(&mut self, result: Result<(), CrucibleError>) {
        /*
         * XXX: If the guest is no longer listening and this returns an error,
         * do we care?  This could happen if the guest has given up
         * becuase an IO took too long, or other possible guest side reasons.
         */
        let _send_result = self.sender.send(result);
    }
}

/**
 * This structure keeps track of work that Crucible has accepted from the
 * "Guest", aka, Propolis.
 *
 * The active is a hashmap of GtoS structures for all I/Os that are
 * outstanding.  Either just created or in progress operations.  The key
 * for a new job comes from next_gw_id and should always increment.
 *
 * Once we have decided enough downstairs requests are finished, we remove
 * the entry from the active and add the gw_id to the completed vec.
 *
 * TODO: The completed needs to implement some notify back to the Guest, and
 * it should probably be a ring buffer.
 */
#[derive(Debug)]
struct GuestWork {
    active: HashMap<u64, GtoS>,
    next_gw_id: u64,
    completed: AllocRingBuffer<u64>,
}

impl GuestWork {
    fn next_gw_id(&mut self) -> u64 {
        let id = self.next_gw_id;
        self.next_gw_id += 1;
        id
    }

    fn active_count(&mut self) -> usize {
        self.active.len()
    }

    /**
     * Move a GtoS job from the active to completed.
     * At this point we should have already sent the guest a message
     * saying their IO is done.
     */
    fn complete(&mut self, gw_id: u64) {
        let gtos_job = self.active.remove(&gw_id).unwrap();
        assert!(gtos_job.submitted.is_empty());
        self.completed.push(gw_id);
    }

    /*
     * When the required number of completions for a downstairs
     * ds_id have arrived, we call this method on the parent GuestWork
     * that requested them and include the Option<Bytes> from the IO.
     *
     * If this operation was a read, then we attach the Bytes read to the
     * GtoS struct for later transfer.
     *
     * A single GtoS job may have multiple downstairs jobs it created, so
     * we may not be done yet.  When the required number of completions have
     * arrived from all the downstairs jobs we created, then we
     * can move forward with finishing up the guest work operation.
     * This may include moving/decrypting data buffers from completed reads.
     */
    #[instrument]
    fn ds_complete(
        &mut self,
        gw_id: u64,
        ds_id: u64,
        data: Option<Bytes>,
        result: Result<(), CrucibleError>,
    ) {
        /*
         * A gw_id that already finished and results were sent back to
         * the guest could still have an outstanding ds_id.
         */
        if let Some(gtos_job) = self.active.get_mut(&gw_id) {
            /*
             * If the ds_id is on the submitted list, then we will take it off
             * and, if it is a read, add the read result buffer to the gtos job
             * structure for later copying.
             */
            if gtos_job.submitted.remove(&ds_id).is_some() {
                if let Some(data) = data {
                    /*
                     * The first read buffer will become the source for the
                     * final response back to the guest. This buffer will be
                     * combined with other buffers if the upstairs request
                     * required multiple jobs.
                     */
                    if gtos_job.downstairs_buffer.insert(ds_id, data).is_some()
                    {
                        /*
                         * Only the first successful read should fill the
                         * slot in the downstairs buffer for a ds_id. If
                         * more than one is trying to, then we have a problem.
                         */
                        panic!(
                            "gw_id:{} read buffer already present for {}",
                            gw_id, ds_id
                        );
                    }
                }
                gtos_job.completed.push(ds_id);
            } else {
                println!("gw_id:{} ({}) already removed???", gw_id, ds_id);
                assert!(gtos_job.completed.contains(&ds_id));
                panic!(
                    "{} Attempting to complete ds_id {} we already completed",
                    gw_id, ds_id
                );
            }

            /*
             * If all the downstairs jobs created for this have completed,
             * we can copy (if present) read data back to the guest buffer
             * they provided to us, and notify any waiters.
             */
            if gtos_job.submitted.is_empty() {
                if result.is_ok() && gtos_job.guest_buffer.is_some() {
                    gtos_job.transfer();
                }

                gtos_job.notify(result);
                self.complete(gw_id);
            }
        } else {
            /*
             * XXX This is just so I can see if ever does happen.
             */
            println!(
                "gw_id {} from removed job {} not on active list",
                gw_id, ds_id
            );
        }
    }
}

/**
 * Couple a BlockOp with a notifier for calling code.
 */
#[derive(Debug)]
pub struct BlockReq {
    op: BlockOp,
    send: std_mpsc::Sender<Result<(), CrucibleError>>,
}

impl BlockReq {
    // https://docs.rs/tokio/1.9.0/tokio/sync/mpsc/index.html#communicating-between-sync-and-async-code
    // return the std::sync::mpsc Sender to non-tokio task callers
    fn new(
        op: BlockOp,
        send: std_mpsc::Sender<Result<(), CrucibleError>>,
    ) -> BlockReq {
        Self { op, send }
    }
}

/**
 * When BlockOps are sent to a guest, the calling function receives a
 * waiter that it can block on.
 */
pub struct BlockReqWaiter {
    recv: std_mpsc::Receiver<Result<(), CrucibleError>>,
}

impl BlockReqWaiter {
    fn new(
        recv: std_mpsc::Receiver<Result<(), CrucibleError>>,
    ) -> BlockReqWaiter {
        Self { recv }
    }

    pub fn block_wait(&mut self) -> Result<(), CrucibleError> {
        match self.recv.recv() {
            Ok(v) => v,
            Err(_) => crucible_bail!(RecvDisconnected),
        }
    }
}

/**
 * This is the structure we use to keep track of work passed into crucible
 * from the the "Guest".
 *
 * Requests from the guest are put into the reqs VecDeque initally.
 *
 * A task on the Crucible side will receive a notification that a new
 * operation has landed on the reqs queue and will take action:
 *   Pop the request off the reqs queue.
 *   Copy (and optionally encrypt) any data buffers provided to us by the Guest.
 *   Create one or more downstairs DownstairsIO structures.
 *   Create a GtoS tracking structure with the id's for each
 *   downstairs task and the read result buffer if required.
 *   Add the GtoS struct to the in GuestWork active work hashmap.
 *   Put all the DownstairsIO strucutres on the downstairs work queue.
 *   Send notification to the upstairs tasks that there is new work.
 *
 * Work here will be added to storage side queues and the responses will
 * be waited on and processed when they arrive.
 *
 * This structure and operations on in handle the translation between
 * outside requests and internal upstairs structures and work queues.
 */
#[derive(Debug)]
pub struct Guest {
    /*
     * New requests from outside go onto this VecDeque.  The notify is how
     * the submittion task tells the listening task that new work has been
     * added.
     */
    reqs: Mutex<VecDeque<BlockReq>>,
    notify: Notify,

    /*
     * When the crucible listening task has noticed a new IO request, it will
     * pull it from the reqs queue and create an GuestWork struct as well as
     * convert the new IO request into the matching downstairs request(s).
     * Each new GuestWork request will get a unique gw_id, which is also
     * the index for that operation into the hashmap.
     *
     * It is during this process that data will encrypted.  For a read, the
     * data is decrypted back to the guest provided buffer after all the
     * required downstairs operations are completed.
     */
    guest_work: Mutex<GuestWork>,
}

/*
 * These methods are how to add or checking for new work on the Guest struct
 */
impl Guest {
    pub fn new() -> Guest {
        Guest {
            /*
             * Incoming I/O requests are added to this queue.
             */
            reqs: Mutex::new(VecDeque::new()),
            notify: Notify::new(),
            /*
             * The active hashmap is for in-flight I/O operations
             * that we have taken off the incoming queue, but we have not
             * received the response from downstairs.
             * Note that a single IO from outside may have multiple I/O
             * requests that need to finish before we can complete that IO.
             */
            guest_work: Mutex::new(GuestWork {
                active: HashMap::new(), // GtoS
                next_gw_id: 1,
                completed: AllocRingBuffer::with_capacity(2048),
            }),
        }
    }

    /*
     * This is used to submit a new BlockOp IO request to Crucible.
     */
    fn send(&self, op: BlockOp) -> BlockReqWaiter {
        let (send, recv) = std_mpsc::channel();

        self.reqs.lock().unwrap().push_back(BlockReq::new(op, send));
        self.notify.notify_one();

        BlockReqWaiter::new(recv)
    }

    /*
     * A crucible task will listen for new work using this.
     */
    async fn recv(&self) -> BlockReq {
        loop {
            if let Some(req) = self.reqs.lock().unwrap().pop_front() {
                return req;
            }
            self.notify.notified().await;
        }
    }

    /*
     * `read` and `write` accept a block offset, and data must be a multiple of block size.
     */
    pub fn read(
        &self,
        offset: Block,
        data: Buffer,
    ) -> Result<BlockReqWaiter, CrucibleError> {
        let bs = self.query_block_size()?;

        if (data.len() % bs as usize) != 0 {
            crucible_bail!(DataLenUnaligned);
        }

        if offset.block_size_in_bytes() as u64 != bs {
            crucible_bail!(BlockSizeMismatch);
        }

        let rio = BlockOp::Read { offset, data };
        Ok(self.send(rio))
    }

    pub fn write(
        &self,
        offset: Block,
        data: Bytes,
    ) -> Result<BlockReqWaiter, CrucibleError> {
        let bs = self.query_block_size()?;

        if (data.len() % bs as usize) != 0 {
            crucible_bail!(DataLenUnaligned);
        }

        if offset.block_size_in_bytes() as u64 != bs {
            crucible_bail!(BlockSizeMismatch);
        }

        let wio = BlockOp::Write { offset, data };
        Ok(self.send(wio))
    }

    /*
     * `read_from_byte_offset` and `write_to_byte_offset` accept a byte offset, and data must be a
     * multiple of block size.
     */
    pub fn read_from_byte_offset(
        &self,
        offset: u64,
        data: Buffer,
    ) -> Result<BlockReqWaiter, CrucibleError> {
        let bs = self.query_block_size()?;

        if (offset % bs) != 0 {
            crucible_bail!(OffsetUnaligned);
        }

        self.read(Block::new(offset / bs, bs.trailing_zeros()), data)
    }

    pub fn write_to_byte_offset(
        &self,
        offset: u64,
        data: Bytes,
    ) -> Result<BlockReqWaiter, CrucibleError> {
        let bs = self.query_block_size()?;

        if (offset % bs) != 0 {
            crucible_bail!(OffsetUnaligned);
        }

        self.write(Block::new(offset / bs, bs.trailing_zeros()), data)
    }

    pub fn flush(&self) -> BlockReqWaiter {
        self.send(BlockOp::Flush)
    }

    pub fn query_block_size(&self) -> Result<u64, CrucibleError> {
        let data = Arc::new(Mutex::new(0));
        let size_query = BlockOp::QueryBlockSize { data: data.clone() };
        self.send(size_query).block_wait()?;
        return Ok(*data.lock().map_err(|_| CrucibleError::DataLockError)?);
    }

    pub fn query_total_size(&self) -> Result<u64, CrucibleError> {
        let data = Arc::new(Mutex::new(0));
        let size_query = BlockOp::QueryTotalSize { data: data.clone() };
        self.send(size_query).block_wait()?;
        return Ok(*data.lock().map_err(|_| CrucibleError::DataLockError)?);
    }

    pub fn query_extent_size(&self) -> Result<Block, CrucibleError> {
        let data = Arc::new(Mutex::new(Block::new(0, 9)));
        let extent_query = BlockOp::QueryExtentSize { data: data.clone() };
        self.send(extent_query).block_wait()?;
        return Ok(*data.lock().map_err(|_| CrucibleError::DataLockError)?);
    }

    pub fn query_active(&self) -> Result<usize, CrucibleError> {
        let data = Arc::new(Mutex::new(0));
        let active_query = BlockOp::QueryActive { data: data.clone() };
        self.send(active_query).block_wait()?;
        return Ok(*data.lock().map_err(|_| CrucibleError::DataLockError)?);
    }

    pub fn commit(&self) {
        self.send(BlockOp::Commit).block_wait().unwrap();
    }

    /*
     * Test call that displays the internal job queue on the upstairs, and
     * returns the guest side and downstairs side job queue depths.
     */
    pub fn show_work(&self) -> WQCounts {
        let wc = WQCounts {
            up_count: 0,
            ds_count: 0,
        };
        let data = Arc::new(Mutex::new(wc));
        let sw = BlockOp::ShowWork { data: data.clone() };
        self.send(sw).block_wait().unwrap();
        return *data.lock().unwrap();
    }
}

/*
 * Work Queue Counts, for debug ShowWork IO type
 */
#[derive(Debug, Copy, Clone)]
pub struct WQCounts {
    pub up_count: usize,
    pub ds_count: usize,
}

impl Default for Guest {
    fn default() -> Self {
        Self::new()
    }
}

pub struct Target {
    #[allow(dead_code)]
    target: SocketAddrV4,
    ds_work_tx: watch::Sender<u64>,
}

#[derive(Debug)]
struct Condition {
    target: SocketAddrV4,
    connected: bool,
}

/*
 * Send work to all the targets on this vector.
 * This can be much simpler, but we need to (eventually) take special action
 * when we fail to send a message to a task.
 */
fn _send_work(t: &[Target], val: u64) {
    for d_client in t.iter() {
        // println!("#### send to client {:?}", d_client.target);
        let res = d_client.ds_work_tx.send(val);
        if let Err(e) = res {
            panic!("#### error {:#?} sending work to {:?}", e, d_client.target);
            /*
             * TODO Write more code for this error,  If one downstairs
             * never receives a request, it may get picked up on the
             * next request.  However, if the downstairs has gone away,
             * then action will need to be taken, and soon.
             */
        }
    }
}

/**
 * We listen on the ds_done channel to know when enough of the downstairs requests
 * for a downstairs work task have finished and it is time to complete
 * any buffer transfers (reads) and then notify the guest that their
 * work has been completed.
 */
async fn up_ds_listen(up: &Arc<Upstairs>, mut ds_done_rx: mpsc::Receiver<u64>) {
    while let Some(_ds_id) = ds_done_rx.recv().await {
        /*
         * XXX Do we need to hold the lock while we process all the
         * completed jobs?  We should be continuing to send message over
         * the ds_done_tx channel, so if new things show up while we
         * process the set of things we know are done now, then the
         * ds_done_rx.recv() should trigger when we loop.
         */
        let ack_list = up.ds_work.lock().unwrap().ackable_work();

        let mut gw = up.guest.guest_work.lock().unwrap();
        for ds_id_done in ack_list.iter() {
            let mut work = up.ds_work.lock().unwrap();

            let done = work.active.get_mut(ds_id_done).unwrap();

            let gw_id = done.guest_id;
            let ds_id = done.ds_id;
            assert_eq!(*ds_id_done, ds_id);

            let data = done.data.take();

            // Verify we did find an AckReady downstairs IO
            assert!(work.ack(ds_id));

            gw.ds_complete(gw_id, ds_id, data, work.result(ds_id));

            work.crutrace_gw_work_done(ds_id, gw_id);

            work.retire_check(ds_id);
        }
    }
    println!("up_ds_listen loop done");
}

/**
 * The upstairs has recieved a new IO request from the guest.  Here we
 * decide what to for that request.
 * For IO operations, we build the downstairs work and if required split
 * the single IO into multiple IOs to the downstairs.  Once we have built
 * the work and updated the upstairs and downstairs work queues, we signal
 * to all the downstairs tasks there is new work for them to do.
 */
fn process_new_io(
    up: &Arc<Upstairs>,
    dst: &[Target],
    req: BlockReq,
    lastcast: &mut u64,
) {
    /*
     * If any of the submit_* functions fail to send to the downstairs, they return an error.
     * These are reported to the Guest.
     */
    match req.op {
        BlockOp::Read { offset, data } => {
            if let Err(e) = up.submit_read(offset, data, req.send.clone()) {
                let _ = req.send.send(Err(e));
                return;
            }
            dst.iter()
                .for_each(|t| t.ds_work_tx.send(*lastcast).unwrap());
            *lastcast += 1;
        }
        BlockOp::Write { offset, data } => {
            if let Err(e) = up.submit_write(offset, data, req.send.clone()) {
                let _ = req.send.send(Err(e));
                return;
            }
            dst.iter()
                .for_each(|t| t.ds_work_tx.send(*lastcast).unwrap());
            *lastcast += 1;
        }
        BlockOp::Flush => {
            if let Err(e) = up.submit_flush(req.send.clone()) {
                let _ = req.send.send(Err(e));
                return;
            }
            dst.iter()
                .for_each(|t| t.ds_work_tx.send(*lastcast).unwrap());
            *lastcast += 1;
        }
        // Query ops
        BlockOp::QueryBlockSize { data } => {
            *data.lock().unwrap() = up.ddef.lock().unwrap().block_size();
            let _ = req.send.send(Ok(()));
        }
        BlockOp::QueryTotalSize { data } => {
            *data.lock().unwrap() = up.ddef.lock().unwrap().total_size();
            let _ = req.send.send(Ok(()));
        }
        // Testing options
        BlockOp::QueryExtentSize { data } => {
            // Yes, test only
            *data.lock().unwrap() = up.ddef.lock().unwrap().extent_size();
            let _ = req.send.send(Ok(()));
        }
        BlockOp::QueryActive { data } => {
            *data.lock().unwrap() =
                up.guest.guest_work.lock().unwrap().active_count();
            let _ = req.send.send(Ok(()));
        }
        BlockOp::ShowWork { data } => {
            *data.lock().unwrap() = show_all_work(up);
            let _ = req.send.send(Ok(()));
        }
        BlockOp::Commit => {
            dst.iter()
                .for_each(|t| t.ds_work_tx.send(*lastcast).unwrap());
            *lastcast += 1;
        }
    }
}

/*
 * This task will loop forever and wait for three downstairs to get into the
 * ready state.  We are notified of that through the ds_status_rx channel.
 * Once we have three connections, we then also listen for work requests
 * to come over the guest channel.
 * If we lose a connection to downstairs, we just panic. XXX Eventually we
 * will handle that situation.
 */
async fn up_listen(
    up: &Arc<Upstairs>,
    dst: Vec<Target>,
    mut ds_status_rx: mpsc::Receiver<Condition>,
) {
    println!("Wait for all three downstairs to come online");
    let mut ds_count = 0u32;
    let mut lastcast = 1;

    loop {
        /*
         * For now, we need all three connections to proceed.
         */
        while ds_count < 3 {
            let c = ds_status_rx.recv().await.unwrap();
            if c.connected {
                ds_count += 1;
                println!(
                    "#### {:?} #### CONNECTED ######## {}/??",
                    c.target, ds_count,
                );
            } else {
                println!("#### {:?} #### DISCONNECTED! ####", c.target);
                ds_count -= 1;
            }
        }

        println!("All expected targets are online, Now accepting IO requests");
        up.ds_transition_all(DsState::Active);
        up.ds_state_show();

        /*
         * We have three connections, so we can now start listening for
         * more IO to come in.  We also need to make sure our downstairs
         * stay connected, and we watch the ds_status_rx.recv() for that
         * to change which is our notification that a disconnect has happened.
         */
        loop {
            tokio::select! {
                c = ds_status_rx.recv() => {
                    /*
                     * If this is anything other than a disconnect, then
                     * panic at this time.  Given our outer loop is doing the
                     * work of connecting all three downstairs, all this
                     * should ever have to do (right now) is detatch a
                     * downstairs and stop taking I/O.
                     */
                    if let Some(ref c) = c {
                        if !c.connected {
                            println!("{} offline, stop IO ", c.target);
                            panic!("Can't recover if downstairs goes offline");
                        }
                    }
                    /*
                     * Any thing other than a transition from connected to
                     * disconnected is wrong.
                     */
                    panic!("error n ds_status_rx: {:?}", c);
                }
                req = up.guest.recv() => {
                    process_new_io(up, &dst, req, &mut lastcast);
                }
            }
        }
    }
}

/*
 * This is the main upstairs task that starts all the other async
 * tasks.
 *
 * XXX At the moment, this function is only half complete, and will
 * probably need a re-write.
 */
pub async fn up_main(opt: CrucibleOpts, guest: Arc<Guest>) -> Result<()> {
    match register_probes() {
        Ok(()) => {
            println!("DTrace probes registered ok");
        }
        Err(e) => {
            println!("Error registering DTrace probes: {:?}", e);
        }
    }

    let lossy = opt.lossy;
    /*
     * Build the Upstairs struct that we use to share data between
     * the different async tasks
     */
    let up = Upstairs::new(&opt, guest);

    /*
     * Use this channel to receive updates on target status from each task
     * we create to connect to a downstairs.
     */
    let (ds_status_tx, ds_status_rx) = mpsc::channel::<Condition>(32);

    /*
     * Use this channel to indicate in the upstairs that all downstairs
     * operations for a specific request have completed.
     */
    let (ds_done_tx, ds_done_rx) = mpsc::channel(100);

    /*
     * spawn a task to listen for ds completed work which will then
     * take care of transitioning guest work structs to done.
     */
    let upc = Arc::clone(&up);
    tokio::spawn(async move {
        up_ds_listen(&upc, ds_done_rx).await;
    });

    let mut client_id = 0;
    /*
     * Create one downstairs task (dst) for each target in the opt
     * structure that was passed to us.
     */
    let dst = opt
        .target
        .iter()
        .map(|dst| {
            /*
             * Create the channel that we will use to request that the loop
             * check for work to do in the central structure.
             */
            let (ds_work_tx, ds_work_rx) = watch::channel(100); // XXX 100?

            let up = Arc::clone(&up);
            let t0 = *dst;
            let up_coms = UpComs {
                client_id,
                ds_work_rx,
                ds_status_tx: ds_status_tx.clone(),
                ds_done_tx: ds_done_tx.clone(),
            };
            tokio::spawn(async move {
                looper(t0, &up, up_coms, lossy).await;
            });
            client_id += 1;

            Target {
                target: *dst,
                ds_work_tx,
            }
        })
        .collect::<Vec<_>>();

    /*
     * The final step is to call this function to wait for our downstairs
     * tasks to connect to their respective downstairs instance.
     * Once connected, we then take work requests from the guest and
     * submit them into the upstairs
     */
    up_listen(&up, dst, ds_status_rx).await;
    Ok(())
}

/*
 * Create a write DownstairsIO structure from an EID, and offset, and
 * the data buffer
 */
fn create_write_eob(
    ds_id: u64,
    dependencies: Vec<u64>,
    gw_id: u64,
    eid: u64,
    offset: Block,
    data: Bytes,
) -> DownstairsIO {
    /*
     * Note to self:  Should the dependency list cover everything since
     * the last flush, or everything that is currently outstanding?
     */
    let awrite = IOop::Write {
        dependencies,
        eid,
        offset,
        data,
    };

    let mut state = HashMap::new();
    for cl in 0..3 {
        state.insert(cl, IOState::New);
    }

    DownstairsIO {
        ds_id,
        guest_id: gw_id,
        work: awrite,
        state,
        data: None,
    }
}

/*
 * Create a write DownstairsIO structure from an EID, and offset, and the
 * data buffer.  Used for converting a guest IO read request into a
 * DownstairsIO that the downstairs can understand.
 */
fn create_read_eob(
    ds_id: u64,
    dependencies: Vec<u64>,
    gw_id: u64,
    eid: u64,
    offset: Block,
    num_blocks: u64,
) -> DownstairsIO {
    let aread = IOop::Read {
        dependencies,
        eid,
        offset,
        num_blocks,
    };

    let mut state = HashMap::new();
    for cl in 0..3 {
        state.insert(cl, IOState::New);
    }

    DownstairsIO {
        ds_id,
        guest_id: gw_id,
        work: aread,
        state,
        data: None,
    }
}

/*
 * Create a flush DownstairsIO structure.
 */
fn create_flush(
    ds_id: u64,
    dependencies: Vec<u64>,
    flush_number: u64,
    guest_id: u64,
) -> DownstairsIO {
    let flush = IOop::Flush {
        dependencies,
        flush_number,
    };

    let mut state = HashMap::new();
    for cl in 0..3 {
        state.insert(cl, IOState::New);
    }
    DownstairsIO {
        ds_id,
        guest_id,
        work: flush,
        state,
        data: None,
    }
}

/*
 * Debug function to display the work hashmap with status for all three of
 * the clients.
 */
#[allow(unused_variables)]
fn show_all_work(up: &Arc<Upstairs>) -> WQCounts {
    let work = up.ds_work.lock().unwrap();
    let mut kvec: Vec<u64> = work.active.keys().cloned().collect::<Vec<u64>>();
    if kvec.is_empty() {
        println!("# Crucible Downstairs work queue -> Empty #");
    } else {
        println!("# Crucible Downstairs work queue #");
        kvec.sort_unstable();
        for id in kvec.iter() {
            let job = work.active.get(id).unwrap();
            let job_type = match &job.work {
                IOop::Read {
                    dependencies,
                    eid,
                    offset,
                    num_blocks,
                } => "Read ".to_string(),
                IOop::Write {
                    dependencies,
                    eid,
                    offset,
                    data,
                } => "Write".to_string(),
                IOop::Flush {
                    dependencies,
                    flush_number,
                } => "Flush".to_string(),
            };
            print!("JOB:[{:04}] {} ", id, job_type);
            for cid in 0..3 {
                let state = job.state.get(&cid);
                match state {
                    Some(state) => {
                        print!("[{}] state: {}  ", cid, state);
                    }
                    x => {
                        print!("[{}] unknown state:{:#?}", cid, x);
                    }
                }
            }
            println!();
        }
    }
    let done = work.completed.to_vec();
    println!("Done tasks count: {:?}", done.len());
    drop(work);
    let up_count = show_guest_work(&up.guest);

    WQCounts {
        up_count,
        ds_count: kvec.len(),
    }
}

/*
 * Debug function to dump the guest work structure.
 * This does a bit while holding the mutex, so don't expect performance
 * to get better when calling it.
 *
 * TODO: make this one big dump, where we include the up.work.active
 * printing for each guest_work.  It will be much more dense, but require
 * holding both locks for the duration.
 */
fn show_guest_work(guest: &Arc<Guest>) -> usize {
    println!("Guest work:  Active and Completed Jobs:");
    let gw = guest.guest_work.lock().unwrap();
    let mut kvec: Vec<u64> = gw.active.keys().cloned().collect::<Vec<u64>>();
    kvec.sort_unstable();
    for id in kvec.iter() {
        let job = gw.active.get(id).unwrap();
        println!(
            "GW_JOB active:[{:04}] S:{:?} C:{:?} ",
            id, job.submitted, job.completed
        );
    }
    let done = gw.completed.to_vec();
    println!("GW_JOB completed count:{:?} ", done.len());
    kvec.len()
}

/*
 * IO operations are ok to submit directly to Upstairs if:
 *
 * - the offset is block aligned, and
 * - the size is a multiple of block size
 *
 * If either of these is not true, then perform some fixup here.
 */
#[derive(Debug)]
struct IOSpan {
    // IOP details
    offset: u64,
    sz: u64,

    // Block and Guest details
    block_size: u64,
    phase: u64,
    buffer: Buffer,
    affected_block_numbers: Vec<u64>,
}

impl IOSpan {
    // Create an IOSpan given a IO operation at offset and size.
    fn new(offset: u64, sz: u64, block_size: u64) -> IOSpan {
        let start_block = offset / block_size;
        let end_block = (offset + sz - 1) / block_size;

        let affected_block_numbers: Vec<u64> =
            (start_block..=end_block).collect();

        Self {
            offset,
            sz,
            block_size,
            phase: offset % block_size,
            buffer: Buffer::new(
                affected_block_numbers.len() * block_size as usize,
            ),
            affected_block_numbers,
        }
    }

    fn is_block_regular(&self) -> bool {
        let is_block_aligned = (self.offset % self.block_size) == 0;
        let is_block_sized = (self.sz % self.block_size) == 0;

        is_block_aligned && is_block_sized
    }

    #[cfg(test)]
    fn affected_block_count(&self) -> usize {
        self.affected_block_numbers.len()
    }

    #[instrument]
    fn read_affected_blocks_from_guest(
        &mut self,
        guest: &Guest,
    ) -> Result<BlockReqWaiter, CrucibleError> {
        guest.read(
            Block::new(
                self.affected_block_numbers[0],
                self.block_size.trailing_zeros(),
            ),
            self.buffer.clone(),
        )
    }

    #[instrument]
    fn write_affected_blocks_to_guest(
        &self,
        guest: &Guest,
    ) -> Result<BlockReqWaiter, CrucibleError> {
        let bytes = Bytes::from(self.buffer.as_vec().clone());
        guest.write(
            Block::new(
                self.affected_block_numbers[0],
                self.block_size.trailing_zeros(),
            ),
            bytes,
        )
    }

    #[instrument]
    fn read_from_blocks_into_buffer(&self, data: &mut [u8]) {
        assert_eq!(data.len(), self.sz as usize);

        for (i, item) in data.iter_mut().enumerate() {
            *item = self.buffer.as_vec()[self.phase as usize + i];
        }
    }

    #[instrument]
    fn write_from_buffer_into_blocks(&self, data: &[u8]) {
        assert_eq!(data.len(), self.sz as usize);

        for (i, item) in data.iter().enumerate() {
            self.buffer.as_vec()[self.phase as usize + i] = *item;
        }
    }
}

/*
 * Wrap a Crucible guest and implement Read + Write + Seek traits.
 */
pub struct CruciblePseudoFile {
    guest: Arc<Guest>,
    offset: u64,
    sz: u64,
    block_size: u64,
    rmw_lock: RwLock<bool>,
}

impl CruciblePseudoFile {
    pub fn from_guest(guest: Arc<Guest>) -> Result<Self, CrucibleError> {
        let sz = guest.query_total_size()? as u64;
        let block_size = guest.query_block_size()? as u64;
        Ok(CruciblePseudoFile {
            guest,
            offset: 0,
            sz,
            block_size,
            rmw_lock: RwLock::new(false),
        })
    }

    pub fn sz(&self) -> u64 {
        self.sz
    }
}

/*
 * The Read + Write impls here translate arbitrary sized operations into
 * calls for the underlying Crucible API.
 */
impl Read for CruciblePseudoFile {
    fn read(&mut self, buf: &mut [u8]) -> IOResult<usize> {
        self._read(buf)
            .map_err(|e| std::io::Error::new(std::io::ErrorKind::Other, e))
    }
}

impl Write for CruciblePseudoFile {
    fn write(&mut self, buf: &[u8]) -> IOResult<usize> {
        self._write(buf)
            .map_err(|e| std::io::Error::new(std::io::ErrorKind::Other, e))
    }

    fn flush(&mut self) -> IOResult<()> {
        self._flush()
            .map_err(|e| std::io::Error::new(std::io::ErrorKind::Other, e))
    }
}

impl Seek for CruciblePseudoFile {
    fn seek(&mut self, pos: SeekFrom) -> IOResult<u64> {
        // TODO: let guard = self.rmw_lock.write().unwrap() ?
        // TODO: does not check against block device size

        let mut offset: i64 = self.offset as i64;
        match pos {
            SeekFrom::Start(v) => {
                offset = v as i64;
            }
            SeekFrom::Current(v) => {
                offset += v;
            }
            SeekFrom::End(v) => {
                offset = self.sz as i64 + v;
            }
        }

        if offset < 0 {
            Err(std::io::Error::new(
                std::io::ErrorKind::Other,
                "offset is negative!",
            ))
        } else {
            // offset >= 0
            self.offset = offset as u64;
            Ok(self.offset)
        }
    }

    fn stream_position(&mut self) -> IOResult<u64> {
        self.seek(SeekFrom::Current(0))
    }
}

impl CruciblePseudoFile {
    fn _read(&mut self, buf: &mut [u8]) -> Result<usize, CrucibleError> {
        let _guard = self.rmw_lock.read().unwrap();

        let mut span =
            IOSpan::new(self.offset, buf.len() as u64, self.block_size);

        let mut waiter = span.read_affected_blocks_from_guest(&self.guest)?;
        waiter.block_wait()?;

        span.read_from_blocks_into_buffer(buf);

        // TODO: for block devices, we can't increment offset past the
        // device size but we're supposed to be pretending to be a proper
        // file here
        self.offset += buf.len() as u64;

        Ok(buf.len())
    }

    fn _write(&mut self, buf: &[u8]) -> Result<usize, CrucibleError> {
        let mut span =
            IOSpan::new(self.offset, buf.len() as u64, self.block_size);

        /*
         * Crucible's dependency system will properly resolve requests in the order they are
         * received but if the request is not block aligned and block sized we need to do
         * read-modify-write (RMW) here. Use a reader-writer lock, and grab the write portion of
         * the lock when doing RMW to cause all other operations (which only grab the read portion
         * of the lock) to pause. Otherwise all operations can use the read portion of this lock
         * and Crucible will sort it out.
         */
        if !span.is_block_regular() {
            let _guard = self.rmw_lock.write().unwrap();

            let mut waiter =
                span.read_affected_blocks_from_guest(&self.guest)?;
            waiter.block_wait()?;

            span.write_from_buffer_into_blocks(buf);

            let mut waiter =
                span.write_affected_blocks_to_guest(&self.guest)?;
            waiter.block_wait()?;
        } else {
            let _guard = self.rmw_lock.read().unwrap();

            let offset = Block::new(
                self.offset / self.block_size,
                self.block_size.trailing_zeros(),
            );
            let bytes = BytesMut::from(buf);
            let mut waiter = self.guest.write(offset, bytes.freeze())?;
            waiter.block_wait()?;
        }

        // TODO: can't increment offset past the device size
        self.offset += buf.len() as u64;

        Ok(buf.len())
    }

    fn _flush(&mut self) -> Result<(), CrucibleError> {
        let _guard = self.rmw_lock.read().unwrap();

        let mut waiter = self.guest.flush();
        waiter.block_wait()?;

        Ok(())
    }
}

#[cfg(test)]
mod test {
    use super::*;
    use ringbuffer::RingBuffer;

    #[test]
    fn test_extent_from_offset() {
        let mut ddef = RegionDefinition::default();
        ddef.set_block_size(512);
        ddef.set_extent_size(Block::new_512(2));
        ddef.set_extent_count(10);

        // Test block size, less than extent size
        assert_eq!(
            extent_from_offset(ddef, Block::new_512(0), 1).unwrap(),
            vec![(0, Block::new_512(0), 1),]
        );

        // Test greater than block size, less than extent size
        assert_eq!(
            extent_from_offset(ddef, Block::new_512(0), 2).unwrap(),
            vec![(0, Block::new_512(0), 2),]
        );

        // Test greater than extent size
        assert_eq!(
            extent_from_offset(ddef, Block::new_512(0), 4).unwrap(),
            vec![(0, Block::new_512(0), 2), (1, Block::new_512(0), 2),]
        );

        // Test offsets
        assert_eq!(
            extent_from_offset(ddef, Block::new_512(1), 4).unwrap(),
            vec![
                (0, Block::new_512(1), 1),
                (1, Block::new_512(0), 2),
                (2, Block::new_512(0), 1),
            ]
        );

        assert_eq!(
            extent_from_offset(ddef, Block::new_512(2), 4).unwrap(),
            vec![(1, Block::new_512(0), 2), (2, Block::new_512(0), 2),]
        );

        assert_eq!(
            extent_from_offset(ddef, Block::new_512(2), 16).unwrap(),
            vec![
                (1, Block::new_512(0), 2),
                (2, Block::new_512(0), 2),
                (3, Block::new_512(0), 2),
                (4, Block::new_512(0), 2),
                (5, Block::new_512(0), 2),
                (6, Block::new_512(0), 2),
                (7, Block::new_512(0), 2),
                (8, Block::new_512(0), 2),
            ]
        );
    }

    #[test]
    fn test_iospan() {
        let span = IOSpan::new(512, 1024, 512);
        assert!(span.is_block_regular());
        assert_eq!(span.affected_block_count(), 2);

        let span = IOSpan::new(513, 1024, 512);
        assert!(!span.is_block_regular());
        assert_eq!(span.affected_block_count(), 3);

        let span = IOSpan::new(512, 500, 512);
        assert!(!span.is_block_regular());
        assert_eq!(span.affected_block_count(), 1);

        let span = IOSpan::new(512, 512, 4096);
        assert!(!span.is_block_regular());
        assert_eq!(span.affected_block_count(), 1);

        let span = IOSpan::new(500, 4096 * 10, 4096);
        assert!(!span.is_block_regular());
        assert_eq!(span.affected_block_count(), 10 + 1);

        let span = IOSpan::new(500, 4096 * 3 + (4096 - 500 + 1), 4096);
        assert!(!span.is_block_regular());
        assert_eq!(span.affected_block_count(), 3 + 2);

        // Some from hammer
        let span = IOSpan::new(137690, 1340, 512);
        assert!(!span.is_block_regular());
        assert_eq!(span.affected_block_count(), 4);
        assert_eq!(span.affected_block_numbers, vec![268, 269, 270, 271]);
    }

    #[test]
    fn test_iospan_buffer_read_write() {
        let span = IOSpan::new(500, 64, 512);
        assert_eq!(span.affected_block_count(), 2);
        assert_eq!(span.affected_block_numbers, vec![0, 1]);

        span.write_from_buffer_into_blocks(&Bytes::from(vec![1; 64]));

        for i in 0..500 {
            assert_eq!(span.buffer.as_vec()[i], 0);
        }
        for i in 500..512 {
            assert_eq!(span.buffer.as_vec()[i], 1);
        }
        for i in 512..(512 + 64 - 12) {
            assert_eq!(span.buffer.as_vec()[i], 1);
        }
        for i in (512 + 64 - 12)..1024 {
            assert_eq!(span.buffer.as_vec()[i], 0);
        }

        let data = Buffer::new(64);
        span.read_from_blocks_into_buffer(&mut data.as_vec()[..]);

        for i in 0..64 {
            assert_eq!(data.as_vec()[i], 1);
        }
    }

    /*
     * Beware, if you change these defaults, then you will have to change
     * all the hard coded tests below that use make_upstairs().
     */
    fn make_upstairs() -> Arc<Upstairs> {
        let mut def = RegionDefinition::default();
        def.set_block_size(512);
        def.set_extent_size(Block::new_512(100));
        def.set_extent_count(10);

        Arc::new(Upstairs {
            guest: Arc::new(Guest::new()),
            ds_state: Mutex::new(vec![DsState::New; 3]),
            ds_work: Mutex::new(Work {
                active: HashMap::new(),
                completed: AllocRingBuffer::with_capacity(2),
                next_id: 1000,
            }),
            flush_info: Mutex::new(FlushInfo::new()),
            ddef: Mutex::new(def),
            encryption_context: None,
        })
    }

    /*
     * Terrible wrapper, but it allows us to call extent_from_offset()
     * just like the program does.
     */
    fn up_efo(
        up: &Arc<Upstairs>,
        offset: Block,
        num_blocks: u64,
    ) -> Result<Vec<(u64, Block, u64)>> {
        let ddef = up.ddef.lock().unwrap();
        extent_from_offset(*ddef, offset, num_blocks)
    }

    #[test]
    fn off_to_extent_one_block() {
        let up = make_upstairs();

        for i in 0..100 {
            let exv = vec![(0, Block::new_512(i), 1)];
            assert_eq!(up_efo(&up, Block::new_512(i), 1).unwrap(), exv);
        }

        for i in 0..100 {
            let exv = vec![(1, Block::new_512(i), 1)];
            assert_eq!(up_efo(&up, Block::new_512(100 + i), 1).unwrap(), exv);
        }

        let exv = vec![(2, Block::new_512(0), 1)];
        assert_eq!(up_efo(&up, Block::new_512(200), 1).unwrap(), exv);

        let exv = vec![(9, Block::new_512(99), 1)];
        assert_eq!(up_efo(&up, Block::new_512(999), 1).unwrap(), exv);
    }

    #[test]
    fn off_to_extent_two_blocks() {
        let up = make_upstairs();

        for i in 0..99 {
            let exv = vec![(0, Block::new_512(i), 2)];
            assert_eq!(up_efo(&up, Block::new_512(i), 2).unwrap(), exv);
        }

        let exv = vec![(0, Block::new_512(99), 1), (1, Block::new_512(0), 1)];
        assert_eq!(up_efo(&up, Block::new_512(99), 2).unwrap(), exv);

        for i in 0..99 {
            let exv = vec![(1, Block::new_512(i), 1)];
            assert_eq!(up_efo(&up, Block::new_512(100 + i), 1).unwrap(), exv);
        }

        let exv = vec![(1, Block::new_512(99), 1), (2, Block::new_512(0), 1)];
        assert_eq!(up_efo(&up, Block::new_512(199), 2).unwrap(), exv);

        let exv = vec![(2, Block::new_512(0), 2)];
        assert_eq!(up_efo(&up, Block::new_512(200), 2).unwrap(), exv);

        let exv = vec![(9, Block::new_512(98), 2)];
        assert_eq!(up_efo(&up, Block::new_512(998), 2).unwrap(), exv);
    }

    #[test]
    fn off_to_extent_bridge() {
        /*
         * Testing when our buffer crosses extents.
         */
        let up = make_upstairs();

        /*
         * 1024 buffer
         */
        let exv = vec![(0, Block::new_512(99), 1), (1, Block::new_512(0), 1)];
        assert_eq!(up_efo(&up, Block::new_512(99), 2).unwrap(), exv);
        let exv = vec![(0, Block::new_512(98), 2), (1, Block::new_512(0), 2)];
        assert_eq!(up_efo(&up, Block::new_512(98), 4).unwrap(), exv);

        /*
         * Largest buffer
         */
        let exv = vec![(0, Block::new_512(1), 99), (1, Block::new_512(0), 1)];
        assert_eq!(up_efo(&up, Block::new_512(1), 100).unwrap(), exv);
        let exv = vec![(0, Block::new_512(2), 98), (1, Block::new_512(0), 2)];
        assert_eq!(up_efo(&up, Block::new_512(2), 100).unwrap(), exv);
        let exv = vec![(0, Block::new_512(4), 96), (1, Block::new_512(0), 4)];
        assert_eq!(up_efo(&up, Block::new_512(4), 100).unwrap(), exv);

        /*
         * Largest buffer, last block offset possible
         */
        let exv = vec![(0, Block::new_512(99), 1), (1, Block::new_512(0), 99)];
        assert_eq!(up_efo(&up, Block::new_512(99), 100).unwrap(), exv);
    }

    /*
     * Testing various invalid inputs
     */
    #[test]
    #[should_panic]
    fn off_to_extent_length_zero() {
        let up = make_upstairs();
        up_efo(&up, Block::new_512(0), 0).unwrap();
    }

    #[test]
    fn off_to_extent_length_almost_too_big() {
        let up = make_upstairs();
        up_efo(&up, Block::new_512(0), 1000).unwrap();
    }

    #[test]
    #[should_panic]
    fn off_to_extent_length_too_big() {
        let up = make_upstairs();
        up_efo(&up, Block::new_512(0), 1001).unwrap();
    }

    #[test]
    fn off_to_extent_length_and_offset_almost_too_big() {
        let up = make_upstairs();
        up_efo(&up, Block::new_512(900), 100).unwrap();
    }

    #[test]
    #[should_panic]
    fn off_to_extent_length_and_offset_too_big() {
        let up = make_upstairs();
        up_efo(&up, Block::new_512(900), 101).unwrap();
    }

    #[test]
    #[should_panic]
    fn not_right_block_size() {
        let up = make_upstairs();
        up_efo(&up, Block::new(900 * 4096, 4096), 101).unwrap();
    }

    // key material made with `openssl rand -base64 32`
    #[test]
    pub fn test_upstairs_encryption_context_ok() {
        use rand::{thread_rng, Rng};

        let key_bytes =
            base64::decode("ClENKTXD2bCyXSHnKXY7GGnk+NvQKbwpatjWP2fJzk0=")
                .unwrap();
        let context = EncryptionContext::new(Vec::<u8>::from(key_bytes), 512);

        let mut block = [0u8; 512];
        thread_rng().fill(&mut block[..]);

        let orig_block = block.clone();

        context.encrypt_in_place(&mut block[..], 0);
        assert_ne!(block, orig_block);

        context.decrypt_in_place(&mut block[..], 0);
        assert_eq!(block, orig_block);
    }

    #[test]
    pub fn test_upstairs_encryption_context_bad_index() {
        use rand::{thread_rng, Rng};

        let key_bytes =
            base64::decode("EVrH+ABhMP0MLfxynCalDq1vWCCWCWFfsSsJoJeDCx8=")
                .unwrap();
        let context = EncryptionContext::new(Vec::<u8>::from(key_bytes), 512);

        let mut block = [0u8; 512];
        thread_rng().fill(&mut block[..]);

        let orig_block = block.clone();

        // The wrong block index shouldn't work.
        context.encrypt_in_place(&mut block[..], 0);
        assert_ne!(block, orig_block);

        context.decrypt_in_place(&mut block[..], 1);
        assert_ne!(block, orig_block);
    }

    #[test]
    fn work_flush_three_ok() {
        let mut work = Work {
            active: HashMap::new(),
            completed: AllocRingBuffer::with_capacity(2),
            next_id: 1000,
        };

        let next_id = work.next_id();

        let op = create_flush(next_id, vec![], 10, 0);

        work.enqueue(op);

        work.in_progress(next_id, 0);
        work.in_progress(next_id, 1);
        work.in_progress(next_id, 2);

        assert_eq!(work.complete(next_id, 0, None, Ok(())).unwrap(), false);
        assert_eq!(work.ackable_work().len(), 0);
        assert_eq!(work.completed.len(), 0);

        assert_eq!(work.complete(next_id, 1, None, Ok(())).unwrap(), true);
        assert_eq!(work.ackable_work().len(), 1);
        assert_eq!(work.completed.len(), 0);

        let state = work.active.get_mut(&next_id).unwrap().state.get(&1);
        assert_eq!(*state.unwrap(), IOState::AckReady);
        work.active
            .get_mut(&next_id)
            .unwrap()
            .state
            .insert(1, IOState::Acked);

        assert_eq!(work.complete(next_id, 2, None, Ok(())).unwrap(), false);
        assert_eq!(work.ackable_work().len(), 0);
        assert_eq!(work.completed.len(), 1);
    }

    #[test]
    fn work_flush_one_error_then_ok() {
        let mut work = Work {
            active: HashMap::new(),
            completed: AllocRingBuffer::with_capacity(2),
            next_id: 1000,
        };

        let next_id = work.next_id();

        let op = create_flush(next_id, vec![], 10, 0);

        work.enqueue(op);

        work.in_progress(next_id, 0);
        work.in_progress(next_id, 1);
        work.in_progress(next_id, 2);

        assert_eq!(
            work.complete(
                next_id,
                0,
                None,
                Err(CrucibleError::GenericError(format!("bad")))
            )
            .unwrap(),
            false
        );
        assert_eq!(work.ackable_work().len(), 0);
        assert_eq!(work.completed.len(), 0);

        assert_eq!(work.complete(next_id, 1, None, Ok(())).unwrap(), false);
        assert_eq!(work.ackable_work().len(), 0);
        assert_eq!(work.completed.len(), 0);

        assert_eq!(work.complete(next_id, 2, None, Ok(())).unwrap(), true);
        assert_eq!(work.ackable_work().len(), 1);

        let state = work.active.get_mut(&next_id).unwrap().state.get(&2);
        assert_eq!(*state.unwrap(), IOState::AckReady);
        work.active
            .get_mut(&next_id)
            .unwrap()
            .state
            .insert(2, IOState::Acked);

        assert_eq!(work.ackable_work().len(), 0);
        assert_eq!(work.completed.len(), 0);

        work.retire_check(next_id);

        assert_eq!(work.completed.len(), 1);
    }

    #[test]
    fn work_flush_two_errors_equals_fail() {
        let mut work = Work {
            active: HashMap::new(),
            completed: AllocRingBuffer::with_capacity(2),
            next_id: 1000,
        };

        let next_id = work.next_id();

        let op = create_flush(next_id, vec![], 10, 0);

        work.enqueue(op);

        work.in_progress(next_id, 0);
        work.in_progress(next_id, 1);
        work.in_progress(next_id, 2);

        assert_eq!(
            work.complete(
                next_id,
                0,
                None,
                Err(CrucibleError::GenericError(format!("bad")))
            )
            .unwrap(),
            false
        );
        assert_eq!(work.ackable_work().len(), 0);
        assert_eq!(work.completed.len(), 0);

        assert_eq!(work.complete(next_id, 1, None, Ok(())).unwrap(), false);
        assert_eq!(work.ackable_work().len(), 0);
        assert_eq!(work.completed.len(), 0);

        // XXX should notify guest with error
        assert_eq!(
            work.complete(
                next_id,
                2,
                None,
                Err(CrucibleError::GenericError(format!("bad")))
            )
            .unwrap(),
            false
        );
        assert_eq!(work.ackable_work().len(), 0);

        assert_eq!(work.completed.len(), 1);
    }

    #[test]
    fn work_read_one_ok() {
        let mut work = Work {
            active: HashMap::new(),
            completed: AllocRingBuffer::with_capacity(2),
            next_id: 1000,
        };

        let next_id = work.next_id();

        let op = create_read_eob(next_id, vec![], 10, 0, Block::new_512(7), 2);

        work.enqueue(op);

        work.in_progress(next_id, 0);
        work.in_progress(next_id, 1);
        work.in_progress(next_id, 2);

        let bytes = Some(Bytes::from(vec![]));

        assert_eq!(work.complete(next_id, 0, bytes, Ok(())).unwrap(), true);
        assert_eq!(work.ackable_work().len(), 1);
        assert_eq!(work.completed.len(), 0);

        let state = work.active.get_mut(&next_id).unwrap().state.get(&0);
        assert_eq!(*state.unwrap(), IOState::AckReady);
        work.active
            .get_mut(&next_id)
            .unwrap()
            .state
            .insert(0, IOState::Acked);

        let bytes = Some(Bytes::from(vec![]));

        assert_eq!(work.complete(next_id, 1, bytes, Ok(())).unwrap(), false);
        assert_eq!(work.ackable_work().len(), 0);
        assert_eq!(work.completed.len(), 0);

        let bytes = Some(Bytes::from(vec![]));

        assert_eq!(work.complete(next_id, 2, bytes, Ok(())).unwrap(), false);
        assert_eq!(work.ackable_work().len(), 0);
        assert_eq!(work.completed.len(), 1);
    }

    #[test]
    fn work_read_one_bad_two_ok() {
        let mut work = Work {
            active: HashMap::new(),
            completed: AllocRingBuffer::with_capacity(2),
            next_id: 1000,
        };

        let next_id = work.next_id();

        let op = create_read_eob(next_id, vec![], 10, 0, Block::new_512(7), 2);

        work.enqueue(op);

        work.in_progress(next_id, 0);
        work.in_progress(next_id, 1);
        work.in_progress(next_id, 2);

        let bytes = Some(Bytes::from(vec![]));

        assert_eq!(
            work.complete(
                next_id,
                0,
                bytes,
                Err(CrucibleError::GenericError(format!("bad")))
            )
            .unwrap(),
            false
        );
        assert_eq!(work.ackable_work().len(), 0);
        assert_eq!(work.completed.len(), 0);

        let bytes = Some(Bytes::from(vec![]));

        assert_eq!(work.complete(next_id, 1, bytes, Ok(())).unwrap(), true);
        assert_eq!(work.ackable_work().len(), 1);
        assert_eq!(work.completed.len(), 0);

        let state = work.active.get_mut(&next_id).unwrap().state.get(&1);
        assert_eq!(*state.unwrap(), IOState::AckReady);
        work.active
            .get_mut(&next_id)
            .unwrap()
            .state
            .insert(1, IOState::Acked);

        let bytes = Some(Bytes::from(vec![]));

        assert_eq!(work.complete(next_id, 2, bytes, Ok(())).unwrap(), false);
        assert_eq!(work.ackable_work().len(), 0);
        assert_eq!(work.completed.len(), 1);
    }

    #[test]
    fn work_read_two_bad_one_ok() {
        let mut work = Work {
            active: HashMap::new(),
            completed: AllocRingBuffer::with_capacity(2),
            next_id: 1000,
        };

        let next_id = work.next_id();

        let op = create_read_eob(next_id, vec![], 10, 0, Block::new_512(7), 2);

        work.enqueue(op);

        work.in_progress(next_id, 0);
        work.in_progress(next_id, 1);
        work.in_progress(next_id, 2);

        let bytes = Some(Bytes::from(vec![]));

        assert_eq!(
            work.complete(
                next_id,
                0,
                bytes,
                Err(CrucibleError::GenericError(format!("bad")))
            )
            .unwrap(),
            false
        );
        assert_eq!(work.ackable_work().len(), 0);
        assert_eq!(work.completed.len(), 0);

        let bytes = Some(Bytes::from(vec![]));

        assert_eq!(
            work.complete(
                next_id,
                1,
                bytes,
                Err(CrucibleError::GenericError(format!("bad")))
            )
            .unwrap(),
            false
        );
        assert_eq!(work.ackable_work().len(), 0);
        assert_eq!(work.completed.len(), 0);

        let bytes = Some(Bytes::from(vec![]));

        assert_eq!(work.complete(next_id, 2, bytes, Ok(())).unwrap(), true);
        assert_eq!(work.ackable_work().len(), 1);

        let state = work.active.get_mut(&next_id).unwrap().state.get(&2);
        assert_eq!(*state.unwrap(), IOState::AckReady);
        work.active
            .get_mut(&next_id)
            .unwrap()
            .state
            .insert(2, IOState::Acked);

        assert_eq!(work.ackable_work().len(), 0);
        assert_eq!(work.completed.len(), 0);

        work.retire_check(next_id);

        assert_eq!(work.completed.len(), 1);
    }

    #[test]
    fn todo() {
        // Verify we did find an AckReady downstairs IO
    }
}
